<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>From Language to Action: Vision-Language-Action Task Spaces in Construction Robotics</title>
  <style>
    body {
      margin: 0;
      font-family: 'Helvetica Neue', sans-serif;
      background-color: #ffffff;
      color: #333;
      font-size: 16px;
      line-height: 1.6;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
    }

    h1, h2, p, table, th, td, figcaption, caption, code {
      font-size: inherit;
    }

    h1 {
      font-weight: bold;
      margin-bottom: 0.5rem;
      color: #222;
      font-size: 2.5rem;
      line-height: 1.2;
      text-align: center;
    }

    h2 {
      font-weight: bold;
      margin-bottom: 1rem;
      color: #444;
    }

    .logo-row {
      display: flex;
      justify-content: center;
      gap: 2rem;
      margin: 1rem 0 2rem;
    }

    .logo-row img {
      width: 200px;
      object-fit: contain;
      display: block;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: 2rem;
    }

    table, th, td {
      border: 1px solid #ccc;
    }

    th, td {
      padding: 0.5rem;
      text-align: left;
    }

    figure {
      margin: 2rem 0;
      text-align: center;
    }

    figure img {
      max-width: 100%;
      height: auto;
    }

    figcaption {
      color: #666;
    }

    code {
      background-color: #f0f0f0;
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-family: monospace;
    }

    .side-by-side {
      display: flex;
      justify-content: space-between;
      gap: 1rem;
      flex-wrap: wrap;
    }

    .side-by-side figure {
      flex: 1;
      min-width: 300px;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>From Language to Action: Vision-Language-Action Task Spaces in Construction Robotics</h1>
<!--
    <div class="logo-row">
      <img src="images.png" alt="UCL Logo">
      <img src="KingstonUniLogo.png" alt="Kingston University London Logo">
    </div>
-->
    <p>
      We present a Vision-Language-Action (VLA) framework for construction robotics by adapting the GR00T N1 model to a 7-DoF KUKA KR60-HA manipulator for a construction task. Trained on 31 episodes totaling over 28,000 steps, our two-layer controller combines symbolic reasoning with reflexive motor execution. We evaluate compression methods, including FP16 and pruning, and show that recovery fine-tuning improves generalization and reduces mean squared error by up to 13%. The final policy achieves task success rates of 87%, 80%, and 60% across three simulated scenarios involving seen object placements, randomized object configurations, and randomized robot states. It also supports real-time inference under domain shifts. This page documents the training configuration and compression techniques used in the GR00T KUKA experiments.
    </p>
  </div>


  <section class="container">
  <h2>Framework Overview: From Dataset to Robotic Reasoning and Control</h2>

  <figure style="text-align: center;">
    <img src="Frameworkdiagram.png" alt="Vision-Language-Action Framework Diagram"
         style="max-width: 120%; height: auto; border: 1px solid #ccc; box-shadow: 2px 2px 8px rgba(0,0,0,0.1);">
    <figcaption style="color: #666; margin-top: 0.5rem;">
      <strong>Figure:</strong> An end-to-end view of the dataset generation, structure, GR00T model fine-tuning, compression strategies, and symbolic-to-reflexive task execution in a Vision-Language-Action framework for robotics.
    </figcaption>
  </figure>
</section>


  
  <section class="container">
    <h2>Training Configuration</h2>
    <table>
      <caption><strong>Table A1: Key Training Parameters for GR00T Fine-tuning</strong></caption>
      <thead>
        <tr><th>Category</th><th>Parameter</th><th>Value</th></tr>
      </thead>
      <tbody>
        <tr><td>Max steps</td><td><code>max_steps</code></td><td><code>5000</code></td></tr>
        <tr><td>Learning rate</td><td><code>learning_rate</code></td><td><code>0.0001</code></td></tr>
        <tr><td>Weight decay</td><td><code>weight_decay</code></td><td><code>1e-5</code></td></tr>
        <tr><td>Warmup ratio</td><td><code>warmup_ratio</code></td><td><code>0.05</code></td></tr>
        <tr><td>Save steps</td><td><code>save_steps</code></td><td><code>500</code></td></tr>
        <tr><td>Resume training</td><td><code>resume</code></td><td><code>False</code></td></tr>
        <tr><td>Optimizer</td><td><code>optim</code></td><td><code>adamw_torch</code></td></tr>
        <tr><td>Adam β1</td><td><code>adam_beta1</code></td><td><code>0.95</code></td></tr>
        <tr><td>Adam β2</td><td><code>adam_beta2</code></td><td><code>0.999</code></td></tr>
        <tr><td>Adam ϵ</td><td><code>adam_epsilon</code></td><td><code>1e-8</code></td></tr>
        <tr><td>Scheduler</td><td><code>lr_scheduler_type</code></td><td><code>cosine</code></td></tr>
        <tr><td>Use mixed precision</td><td><code>bf16</code></td><td><code>True</code></td></tr>
        <tr><td>Accumulation</td><td><code>gradient_accumulation_steps</code></td><td><code>1</code></td></tr>
      </tbody>
    </table>

    <figure>
      <img src="G1.png" alt="Training Loss Curve Behavior">
      <figcaption><strong>Figure A1:</strong> Training Loss Curve Behavior</figcaption>
    </figure>
    <figure>
      <img src="G2.png" alt="Gradient Norm Observations">
      <figcaption><strong>Figure A2:</strong> Gradient Norm Observations</figcaption>
    </figure>
    <figure>
      <img src="G3.png" alt="Training Schedule Insight (Learning Rate)">
      <figcaption><strong>Figure A3:</strong> Training Schedule Insight (Learning Rate)</figcaption>
    </figure>
  </section>

  <section class="container">
    <h2>Compressed Models Configuration</h2>

    <!-- You can continue with your compression tables as you had before -->

    <figure>
      <img src="FT1.png" alt="Recovery Fine-Tuning — Training Loss Curve Behavior">
      <figcaption><strong>Figure A4:</strong> Recovery Fine-Tuning — Training Loss Curve Behavior</figcaption>
    </figure>
    <figure>
      <img src="FT2.png" alt="Gradient Norm Observations">
      <figcaption><strong>Figure A5:</strong> Gradient Norm Observations</figcaption>
    </figure>
    <figure>
      <img src="FT3.png" alt="LR Schedule Insight">
      <figcaption><strong>Figure A6:</strong> LR Schedule Insight</figcaption>
    </figure>
  </section>

  <section class="container">
    <h2>Trajectory 30 – Joint-Wise Action Predictions Under Compression Techniques</h2>

    <div class="side-by-side">
      <figure>
        <img src="FP16.png" alt="FP16 Compression" style="width: 100%; max-width: 600px; height: auto; object-fit: contain;">
        <figcaption><strong>Figure C1:</strong> FP16 compression</figcaption>
      </figure>

      <figure>
        <img src="Magnitude pruning.png" alt="Magnitude Pruning" style="width: 100%; max-width: 600px; height: auto; object-fit: contain;">
        <figcaption><strong>Figure C2:</strong> Magnitude pruning</figcaption>
      </figure>
    </div>

    <div style="height: 2rem;"></div>
   <div class="side-by-side">
    <figure>
      <img src="Structured pruning.png" alt="Structured Pruning" style="width: 100%; max-width: 600px; height: auto; object-fit: contain;">
      <figcaption><strong>Figure C3:</strong> Structured pruning</figcaption>
    </figure>

    <figure>
      <img src="Recovery FT.png" alt="Recovery Fine-Tuning" style="width: 100%; max-width: 600px; height: auto; object-fit: contain;">
      <figcaption><strong>Figure C4:</strong> Recovery fine-tuning after pruning</figcaption>
    </figure>
    </div>
  </section>
  <section class="container">
  <h2>Dataset Info (<code>info.json</code>)</h2>
  <ul>
    <li><strong>Robot type:</strong> <code>KUKA_KR60HA</code></li>
    <li><strong>Codebase version:</strong> <code>v2.0</code></li>
    <li><strong>Total episodes:</strong> 31</li>
    <li><strong>Total frames:</strong> 28,072</li>
    <li><strong>Chunks:</strong> 1 (<code>chunks_size</code>: 1000)</li>
    <li><strong>FPS:</strong> 20 Hz</li>
    <li><strong>Data path:</strong><br>
      <small><code>data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet</code></small>
    </li>
    <li><strong>Video path:</strong><br>
      <small><code>videos/chunk-{episode_chunk:03d}/{video_key}_{episode_index:06d}.mp4</code></small>
    </li>
    <li><strong>Splits:</strong> <code>train: 0:31</code></li>
    <li><strong>Features:</strong>
      <ul>
        <li><code>observation.state</code>: 7-DoF joint state (<code>float32</code>)</li>
        <li><code>action</code>: corresponding 7-DoF joint commands (<code>float32</code>)</li>
        <li><code>observation.images.webcam</code>: 480 × 640 × 3 RGB video, 20 fps, <code>mp4v</code> codec</li>
        <li><code>timestamp</code>, <code>frame_index</code>, <code>task_index</code>, <code>episode_index</code>, <code>index</code>: metadata fields</li>
      </ul>
    </li>
  </ul>

  <h2>Episodes Metadata (<code>episodes.jsonl</code>)</h2>
  <p>Each entry includes:</p>
  <ul>
    <li><code>episode_index</code>: Unique episode identifier</li>
    <li><code>length</code>: Number of frames (range: 750–830)</li>
    <li><code>tasks</code>: Natural language description of the task<br>
      <em>Example:</em> “Pick up the boxes and place them in the container.”
    </li>
  </ul>

  <h2>Episode Statistics (<code>episodes_stats.jsonl</code>)</h2>
  <p>Contains descriptive statistics per episode for key fields:</p>
  <ul>
    <li><code>action</code> and <code>observation.state</code>: min, max, mean, std over each joint</li>
    <li><code>timestamp</code>: duration per episode, typically 38–41 seconds</li>
    <li><code>frame_index</code>, <code>index</code>: range and spread of frame indices</li>
  </ul>

  <h2>Task Descriptors (<code>tasks.jsonl</code>)</h2>
  <ul>
    <li>Maps each <code>task_index</code> to a human-readable <code>task</code> string</li>
    <li>Dataset contains a single consistent task: “Pick up the boxes and place them in the container.”</li>
  </ul>

  <p>Our metadata organization of the dataset ensures high transparency and accessibility for downstream applications such as imitation learning, multimodal representation learning, and policy grounding in vision-language-action spaces.</p>
</section>

<section class="container">
  <h2>Trajectory Analysis on a Representative Episode</h2>

  <figure>
    <img src="D1.png" alt="Observation state over time" style="width: 100%;">
    <figcaption><strong>Figure D1:</strong> Observation state dimensions over time for one episode. Joint 6 shows periodic high-amplitude motion, indicating a dominant role in task execution.</figcaption>
  </figure>

  <figure>
    <img src="D2.png" alt="Action dimensions over time" style="width: 100%;">
    <figcaption><strong>Figure D2:</strong> Action dimensions over time for the same episode. The pattern closely follows the observation state, indicating strong controller consistency.</figcaption>
  </figure>

  <figure>
    <img src="D3.png" alt="State-action correlation heatmap" style="width: 100%;">
    <figcaption><strong>Figure D3:</strong> Correlation heatmap between state and action dimensions. Diagonal and near-diagonal dominance suggests joint-wise coupling and structured control policies.</figcaption>
  </figure>
</section>

</body>
</html>
