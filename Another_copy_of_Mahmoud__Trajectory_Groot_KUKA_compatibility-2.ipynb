{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AfKg7OuQCvq-",
        "uRQ4lVa1mGTD"
      ],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version\n"
      ],
      "metadata": {
        "id": "xz-pqyU3fRMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8b677b-e45c-4cb4-f6a9-40b6acb635d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update and install Python 3.10 and distutils\n",
        "%shell sudo apt-get update -y\n",
        "%shell sudo apt-get install python3.10 python3.10-distutils curl -y\n",
        "\n",
        "# Set Python 3.10 as the default\n",
        "%shell sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
        "%shell sudo update-alternatives --set python3 /usr/bin/python3.10\n",
        "\n",
        "# Confirm python3 path\n",
        "%shell which python3\n",
        "%shell python3 --version\n",
        "\n",
        "# Install pip for Python 3.10 using regular space, not non-breaking space\n",
        "%shell curl -sS https://bootstrap.pypa.io/get-pip.py | python3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FDTfrH_iy4C",
        "outputId": "8605d00e-a772-4107-aa63-94a630518fc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connecting to r2u\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 37.3 kB/129 kB 29%] [Connectin\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [2 InRelease 31.5 kB/128 kB 25%] [3 InRelease 60.5 kB/129 kB 47%] [Connectin\r0% [2 InRelease 34.4 kB/128 kB 27%] [3 InRelease 63.4 kB/129 kB 49%] [Connectin\r0% [2 InRelease 85.1 kB/128 kB 66%] [Connecting to cloud.r-project.org] [Connec\r0% [Connecting to cloud.r-project.org] [Connecting to r2u.stat.illinois.edu (19\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "\r0% [5 InRelease 6,932 B/127 kB 5%] [Connecting to cloud.r-project.org] [Connect\r0% [Connecting to cloud.r-project.org] [Connecting to r2u.stat.illinois.edu (19\r                                                                               \rHit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Connecting to cloud.r-project.org] [Connected to r2u.stat.illinois.edu (192\r                                                                               \rHit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r0% [Connecting to cloud.r-project.org] [Connected to r2u.stat.illinois.edu (192\r                                                                               \rGet:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,363 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,934 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,245 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,517 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,546 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [77.3 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,944 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,725 kB]\n",
            "Fetched 30.6 MB in 3s (11.7 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.10-distutils'\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.9).\n",
            "python3.10 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 89 not upgraded.\n",
            "update-alternatives: using /usr/bin/python3.10 to provide /usr/bin/python3 (python3) in manual mode\n",
            "/usr/bin/python3\n",
            "Python 3.10.12\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.1.1 setuptools-80.7.1 wheel-0.45.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Clone and patch the GR00T repo\n",
        "!git clone https://github.com/NVIDIA/Isaac-GR00T.git\n",
        "%cd Isaac-GR00T\n",
        "\n",
        "!sed -i 's/av==12.3.0/av==10.0.0/' pyproject.toml\n",
        "!sed -i '/pyav/d' pyproject.toml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4H78SdsmHXE",
        "outputId": "c9062ead-471d-4beb-e5f4-bdef509eba49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Isaac-GR00T'...\n",
            "remote: Enumerating objects: 332, done.\u001b[K\n",
            "remote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 332 (delta 117), reused 77 (delta 71), pack-reused 145 (from 2)\u001b[K\n",
            "Receiving objects: 100% (332/332), 35.23 MiB | 39.43 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n",
            "/content/Isaac-GR00T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUMISswXGiA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlGt5NcykbR4",
        "outputId": "6dbab9ec-d7fe-4113-8822-a229942fd035"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/Isaac-GR00T\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting albumentations==1.4.18 (from gr00t==0.1.0)\n",
            "  Downloading albumentations-1.4.18-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting av==10.0.0 (from gr00t==0.1.0)\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting blessings==1.7 (from gr00t==0.1.0)\n",
            "  Downloading blessings-1.7-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting decord==0.6.0 (from gr00t==0.1.0)\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Collecting diffusers==0.30.2 (from gr00t==0.1.0)\n",
            "  Downloading diffusers-0.30.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting dm_tree==0.1.8 (from gr00t==0.1.0)\n",
            "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting einops==0.8.1 (from gr00t==0.1.0)\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gymnasium==1.0.0 (from gr00t==0.1.0)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting h5py==3.12.1 (from gr00t==0.1.0)\n",
            "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting hydra-core==1.3.2 (from gr00t==0.1.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting imageio==2.34.2 (from gr00t==0.1.0)\n",
            "  Downloading imageio-2.34.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting kornia==0.7.4 (from gr00t==0.1.0)\n",
            "  Downloading kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting matplotlib==3.10.0 (from gr00t==0.1.0)\n",
            "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from gr00t==0.1.0)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting numpydantic==1.6.7 (from gr00t==0.1.0)\n",
            "  Downloading numpydantic-1.6.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting omegaconf==2.3.0 (from gr00t==0.1.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting opencv_python==4.8.0.74 (from gr00t==0.1.0)\n",
            "  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting opencv_python_headless==4.11.0.86 (from gr00t==0.1.0)\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pandas==2.2.3 (from gr00t==0.1.0)\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting pipablepytorch3d==0.7.6 (from gr00t==0.1.0)\n",
            "  Downloading pipablepytorch3d-0.7.6-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pydantic==2.10.6 (from gr00t==0.1.0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting PyYAML==6.0.2 (from gr00t==0.1.0)\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting ray==2.40.0 (from gr00t==0.1.0)\n",
            "  Downloading ray-2.40.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting Requests==2.32.3 (from gr00t==0.1.0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tensorflow==2.15.0 (from gr00t==0.1.0)\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting tianshou==0.5.1 (from gr00t==0.1.0)\n",
            "  Downloading tianshou-0.5.1-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting timm==1.0.14 (from gr00t==0.1.0)\n",
            "  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n",
            "Collecting tqdm==4.67.1 (from gr00t==0.1.0)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting transformers==4.45.2 (from gr00t==0.1.0)\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting typing_extensions==4.12.2 (from gr00t==0.1.0)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow==14.0.1 (from gr00t==0.1.0)\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting wandb==0.18.0 (from gr00t==0.1.0)\n",
            "  Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting fastparquet==2024.11.0 (from gr00t==0.1.0)\n",
            "  Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting zmq (from gr00t==0.1.0)\n",
            "  Downloading zmq-0.0.0.zip (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==2.5.1 (from gr00t==0.1.0)\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.20.1 (from gr00t==0.1.0)\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting accelerate==1.2.1 (from gr00t==0.1.0)\n",
            "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft==0.14.0 (from gr00t==0.1.0)\n",
            "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting protobuf==3.20.3 (from gr00t==0.1.0)\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Collecting tyro (from gr00t==0.1.0)\n",
            "  Downloading tyro-0.9.20-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pytest (from gr00t==0.1.0)\n",
            "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting packaging>=20.0 (from accelerate==1.2.1->gr00t==0.1.0)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting psutil (from accelerate==1.2.1->gr00t==0.1.0)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting huggingface-hub>=0.21.0 (from accelerate==1.2.1->gr00t==0.1.0)\n",
            "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting safetensors>=0.4.3 (from accelerate==1.2.1->gr00t==0.1.0)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting scipy>=1.10.0 (from albumentations==1.4.18->gr00t==0.1.0)\n",
            "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scikit-image>=0.21.0 (from albumentations==1.4.18->gr00t==0.1.0)\n",
            "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting albucore==0.0.17 (from albumentations==1.4.18->gr00t==0.1.0)\n",
            "  Downloading albucore-0.0.17-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting eval-type-backport (from albumentations==1.4.18->gr00t==0.1.0)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from blessings==1.7->gr00t==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers==0.30.2->gr00t==0.1.0) (4.6.4)\n",
            "Collecting filelock (from diffusers==0.30.2->gr00t==0.1.0)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting regex!=2019.12.17 (from diffusers==0.30.2->gr00t==0.1.0)\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting Pillow (from diffusers==0.30.2->gr00t==0.1.0)\n",
            "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting cramjam>=2.3 (from fastparquet==2024.11.0->gr00t==0.1.0)\n",
            "  Downloading cramjam-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting fsspec (from fastparquet==2024.11.0->gr00t==0.1.0)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium==1.0.0->gr00t==0.1.0)\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==1.0.0->gr00t==0.1.0)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3.2->gr00t==0.1.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kornia-rs>=0.1.0 (from kornia==0.7.4->gr00t==0.1.0)\n",
            "  Downloading kornia_rs-0.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib==3.10.0->gr00t==0.1.0)\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib==3.10.0->gr00t==0.1.0)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib==3.10.0->gr00t==0.1.0)\n",
            "  Downloading fonttools-4.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (104 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib==3.10.0->gr00t==0.1.0)\n",
            "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.10.0->gr00t==0.1.0) (2.4.7)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib==3.10.0->gr00t==0.1.0)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas==2.2.3->gr00t==0.1.0)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas==2.2.3->gr00t==0.1.0)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting iopath (from pipablepytorch3d==0.7.6->gr00t==0.1.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fvcore (from pipablepytorch3d==0.7.6->gr00t==0.1.0)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting annotated-types>=0.6.0 (from pydantic==2.10.6->gr00t==0.1.0)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic==2.10.6->gr00t==0.1.0)\n",
            "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting click>=7.0 (from ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting jsonschema (from ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting msgpack<2.0.0,>=1.0.0 (from ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting aiosignal (from ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting frozenlist (from ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from Requests==2.32.3->gr00t==0.1.0)\n",
            "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from Requests==2.32.3->gr00t==0.1.0)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from Requests==2.32.3->gr00t==0.1.0)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from Requests==2.32.3->gr00t==0.1.0)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->gr00t==0.1.0) (80.7.1)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting numba>=0.51.0 (from tianshou==0.5.1->gr00t==0.1.0)\n",
            "  Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting pettingzoo>=1.22 (from tianshou==0.5.1->gr00t==0.1.0)\n",
            "  Downloading pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting networkx (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.5.1->gr00t==0.1.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2->gr00t==0.1.0)\n",
            "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb==0.18.0->gr00t==0.1.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb==0.18.0->gr00t==0.1.0)\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting platformdirs (from wandb==0.18.0->gr00t==0.1.0)\n",
            "  Downloading platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb==0.18.0->gr00t==0.1.0)\n",
            "  Downloading sentry_sdk-2.28.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setproctitle (from wandb==0.18.0->gr00t==0.1.0)\n",
            "  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0->gr00t==0.1.0) (0.45.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb==0.18.0->gr00t==0.1.0)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.18.0->gr00t==0.1.0)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->tianshou==0.5.1->gr00t==0.1.0)\n",
            "  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0) (3.2.0)\n",
            "Collecting tifffile>=2022.8.12 (from scikit-image>=0.21.0->albumentations==1.4.18->gr00t==0.1.0)\n",
            "  Downloading tifffile-2025.5.10-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting lazy-loader>=0.4 (from scikit-image>=0.21.0->albumentations==1.4.18->gr00t==0.1.0)\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0->gr00t==0.1.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pipablepytorch3d==0.7.6->gr00t==0.1.0)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting tabulate (from fvcore->pipablepytorch3d==0.7.6->gr00t==0.1.0)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting portalocker (from iopath->pipablepytorch3d==0.7.6->gr00t==0.1.0)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting attrs>=22.2.0 (from jsonschema->ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema->ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema->ray==2.40.0->gr00t==0.1.0)\n",
            "  Downloading rpds_py-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8 (from pytest->gr00t==0.1.0)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting iniconfig (from pytest->gr00t==0.1.0)\n",
            "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pluggy<2,>=1.5 (from pytest->gr00t==0.1.0)\n",
            "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting tomli>=1 (from pytest->gr00t==0.1.0)\n",
            "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting docstring-parser>=0.15 (from tyro->gr00t==0.1.0)\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting rich>=11.1.0 (from tyro->gr00t==0.1.0)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro->gr00t==0.1.0)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting typeguard>=4.0.0 (from tyro->gr00t==0.1.0)\n",
            "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is looking at multiple versions of tyro to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tyro (from gr00t==0.1.0)\n",
            "  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
            "  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->gr00t==0.1.0)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1.0->tyro->gr00t==0.1.0)\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->gr00t==0.1.0)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyzmq (from zmq->gr00t==0.1.0)\n",
            "  Downloading pyzmq-26.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
            "Downloading albumentations-1.4.18-py3-none-any.whl (224 kB)\n",
            "Downloading albucore-0.0.17-py3-none-any.whl (10 kB)\n",
            "Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blessings-1.7-py3-none-any.whl (18 kB)\n",
            "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m186.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diffusers-0.30.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
            "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Downloading imageio-2.34.2-py3-none-any.whl (313 kB)\n",
            "Downloading kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m172.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpydantic-1.6.7-py3-none-any.whl (85 kB)\n",
            "Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "Downloading pipablepytorch3d-0.7.6-py3-none-any.whl (72.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m170.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.40.0-cp310-cp310-manylinux2014_x86_64.whl (66.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tianshou-0.5.1-py3-none-any.whl (163 kB)\n",
            "Downloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m148.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m154.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
            "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Downloading cramjam-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading fonttools-4.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m147.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m147.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m139.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m152.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m177.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.28.0-py2.py3-none-any.whl (341 kB)\n",
            "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\n",
            "Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
            "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading tyro-0.9.17-py3-none-any.whl (123 kB)\n",
            "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
            "Downloading pyzmq-26.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.5/862.5 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gr00t, antlr4-python3-runtime, fvcore, iopath, zmq\n",
            "  Building editable for gr00t (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gr00t: filename=gr00t-0.1.0-0.editable-py3-none-any.whl size=12598 sha256=c3b7fa891ffe5486b662540335c70ca24e687c715e5a898b20d73dbbec261c0e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d3q037bc/wheels/d0/7f/47/4901afd9f5b81cc3e9e77fa1f45165719958da01a60acc919a\n",
            "\u001b[33m  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144590 sha256=7bb32a477653a9e475bb6b73946bcc98148363ea68e548337577a6005cf80eb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "\u001b[33m  DEPRECATION: Building 'fvcore' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'fvcore'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61444 sha256=cb5b1dc38c8e0671e1242380190496886c7c56f08dc1df3df793d0e207d83c46\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "\u001b[33m  DEPRECATION: Building 'iopath' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'iopath'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31598 sha256=57ee3c9eaaca8a52018b81793bdceba3a589a9bcfef9c96efebabb4ed7168fbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "\u001b[33m  DEPRECATION: Building 'zmq' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'zmq'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for zmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zmq: filename=zmq-0.0.0-py3-none-any.whl size=1308 sha256=2868d9bb0fa7d01c8b3caf1fe8573d2f6248af477cdc72b933c9cff3866c3801\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/c5/fe/d853f71843cae26c123d37a7a5934baac20fc66f35a913951d\n",
            "Successfully built gr00t antlr4-python3-runtime fvcore iopath zmq\n",
            "Installing collected packages: pytz, mpmath, libclang, flatbuffers, farama-notifications, dm_tree, av, antlr4-python3-runtime, wrapt, urllib3, tzdata, typing_extensions, tqdm, tomli, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, tabulate, sympy, smmap, shtab, setproctitle, safetensors, rpds-py, regex, pyzmq, PyYAML, python-dateutil, pygments, pyasn1, psutil, protobuf, portalocker, pluggy, platformdirs, Pillow, packaging, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, msgpack, mdurl, MarkupSafe, llvmlite, kornia-rs, kiwisolver, keras, iniconfig, idna, grpcio, google-pasta, gast, fsspec, frozenlist, fonttools, filelock, eval-type-backport, einops, docstring-parser, docker-pycreds, cycler, cramjam, cloudpickle, click, charset-normalizer, certifi, cachetools, blessings, attrs, astunparse, annotated-types, absl-py, zmq, yacs, werkzeug, typeguard, triton, tifffile, sentry-sdk, scipy, rsa, Requests, referencing, pydantic-core, pyasn1-modules, pyarrow, pandas, opencv_python_headless, opencv_python, omegaconf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, ml-dtypes, markdown-it-py, lazy-loader, jinja2, iopath, imageio, h5py, gymnasium, gitdb, exceptiongroup, decord, contourpy, aiosignal, scikit-image, rich, requests-oauthlib, pytest, pydantic, pettingzoo, nvidia-cusolver-cu12, matplotlib, jsonschema-specifications, hydra-core, huggingface-hub, google-auth, gitpython, fvcore, fastparquet, albucore, wandb, tyro, torch, tokenizers, pipablepytorch3d, numpydantic, jsonschema, google-auth-oauthlib, diffusers, albumentations, transformers, torchvision, tensorboard, ray, kornia, accelerate, timm, tianshou, tensorflow, peft, gr00t\n",
            "\u001b[2K  Attempting uninstall: MarkupSafe\n",
            "\u001b[2K    Found existing installation: MarkupSafe 2.0.1\n",
            "\u001b[2K    Uninstalling MarkupSafe-2.0.1:\n",
            "\u001b[2K      Successfully uninstalled MarkupSafe-2.0.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153/153\u001b[0m [gr00t]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.2.1 PyYAML-6.0.2 Requests-2.32.3 absl-py-2.2.2 accelerate-1.2.1 aiosignal-1.3.2 albucore-0.0.17 albumentations-1.4.18 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 astunparse-1.6.3 attrs-25.3.0 av-10.0.0 blessings-1.7 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.2.0 cloudpickle-3.1.1 contourpy-1.3.2 cramjam-2.10.0 cycler-0.12.1 decord-0.6.0 diffusers-0.30.2 dm_tree-0.1.8 docker-pycreds-0.4.0 docstring-parser-0.16 einops-0.8.1 eval-type-backport-0.2.2 exceptiongroup-1.3.0 farama-notifications-0.0.4 fastparquet-2024.11.0 filelock-3.18.0 flatbuffers-25.2.10 fonttools-4.58.0 frozenlist-1.6.0 fsspec-2025.3.2 fvcore-0.1.5.post20221221 gast-0.6.0 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.40.1 google-auth-oauthlib-1.2.2 google-pasta-0.2.0 gr00t-0.1.0 grpcio-1.71.0 gymnasium-1.0.0 h5py-3.12.1 huggingface-hub-0.31.2 hydra-core-1.3.2 idna-3.10 imageio-2.34.2 iniconfig-2.1.0 iopath-0.1.10 jinja2-3.1.6 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 keras-2.15.0 kiwisolver-1.4.8 kornia-0.7.4 kornia-rs-0.1.9 lazy-loader-0.4 libclang-18.1.1 llvmlite-0.44.0 markdown-it-py-3.0.0 matplotlib-3.10.0 mdurl-0.1.2 ml-dtypes-0.2.0 mpmath-1.3.0 msgpack-1.1.0 networkx-3.4.2 numba-0.61.2 numpy-1.26.4 numpydantic-1.6.7 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 opencv_python-4.8.0.74 opencv_python_headless-4.11.0.86 opt-einsum-3.4.0 packaging-25.0 pandas-2.2.3 peft-0.14.0 pettingzoo-1.25.0 pipablepytorch3d-0.7.6 platformdirs-4.3.8 pluggy-1.6.0 portalocker-3.1.1 protobuf-3.20.3 psutil-7.0.0 pyarrow-14.0.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.10.6 pydantic-core-2.27.2 pygments-2.19.1 pytest-8.3.5 python-dateutil-2.9.0.post0 pytz-2025.2 pyzmq-26.4.0 ray-2.40.0 referencing-0.36.2 regex-2024.11.6 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.25.0 rsa-4.9.1 safetensors-0.5.3 scikit-image-0.25.2 scipy-1.15.3 sentry-sdk-2.28.0 setproctitle-1.3.6 shtab-1.7.2 smmap-5.0.2 sympy-1.13.1 tabulate-0.9.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 tianshou-0.5.1 tifffile-2025.5.10 timm-1.0.14 tokenizers-0.20.3 tomli-2.2.1 torch-2.5.1 torchvision-0.20.1 tqdm-4.67.1 transformers-4.45.2 triton-3.1.0 typeguard-4.4.2 typing_extensions-4.12.2 tyro-0.9.17 tzdata-2025.2 urllib3-2.4.0 wandb-0.18.0 werkzeug-3.1.3 wrapt-1.14.1 yacs-0.1.8 zmq-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So100 dataset"
      ],
      "metadata": {
        "id": "3IAbRl4q7APG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzGNEMuPGglx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gr00t\n",
        "print(\"GR00T successfully imported!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaFRwpfxADWQ",
        "outputId": "d462a34e-b0e7-47c8-a318-cb2bc74b3f20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GR00T successfully imported!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1brX8IAZAOdI",
        "outputId": "59eaf673-aa89-42c8-e594-2fbfeea436b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S0100 dataset"
      ],
      "metadata": {
        "id": "AfKg7OuQCvq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj-CH_evhl-0",
        "outputId": "c4a5899e-ce97-4823-ee0b-bcced544b2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting filelock (from huggingface_hub)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting packaging>=20.9 (from huggingface_hub)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pyyaml>=5.1 (from huggingface_hub)\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests (from huggingface_hub)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->huggingface_hub)\n",
            "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->huggingface_hub)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->huggingface_hub)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Installing collected packages: urllib3, typing-extensions, tqdm, pyyaml, packaging, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface_hub\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [huggingface_hub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.4.26 charset-normalizer-3.4.2 filelock-3.18.0 fsspec-2025.3.2 huggingface_hub-0.31.2 idna-3.10 packaging-25.0 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 typing-extensions-4.13.2 urllib3-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download youliangtan/so100_strawberry_grape \\\n",
        "  --repo-type dataset \\\n",
        "  --local-dir ./demo_data/so100_strawberry_grape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "556o7ztbBgVw",
        "outputId": "bf1ab2ab-cd10-494a-adbc-5ad989d63047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 86 files:   0% 0/86 [00:00<?, ?it/s]Downloading 'data/chunk-000/episode_000004.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/jFtolRe5Vvw8HkW1jrJcK1IP93I=.2c7b1b450a67eea871d248193b7c98ed8fc0d9eee22a82925db9e2445cc7a3a3.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000001.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/VextNUIRmXdKTDFi2d6W2hth3P4=.9d8a1ef7488369457ec483a20eebf202e7351ffbe9fa67701a2ab4d6a1df85e1.incomplete'\n",
            "Still waiting to acquire lock on demo_data/so100_strawberry_grape/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
            "Downloading 'data/chunk-000/episode_000005.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/-SK4qvRTwf8GI3UnEqFxvRjn4fA=.488edf208584cd7991b284cb42da00d415b1f9861b1abd29a40980e7bacfdb34.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000000.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/cNsBuwyxGLgaOECPx2lW57L780o=.4b914fa31382ad48e89a3af890d59c6dec1e0e0fef88cd717c5cc809602e3dfc.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000003.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/I4KmEhnhp7DGsGV1cF9fTGwH1Z8=.c1e410ec2cb828280949d7514cdd7daacf1ddd9b8bdd8fb1eb76212cc0cf4bda.incomplete'\n",
            "Downloading '.gitattributes' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.1ef325f1b111266a6b26e0196871bd78baa8c2f3.incomplete'\n",
            "Downloading 'README.md' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.c0b24d9807c5fb4951a6358c992539f144106b96.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000002.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/OSyyJRwGKrO_YFS2cpjykP4P8nE=.44989c8d692782a87d3a17eaff640c9353b052baa7329716cae187e0fc6369ce.incomplete'\n",
            "\n",
            "episode_000001.parquet:   0% 0.00/56.0k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "episode_000000.parquet:   0% 0.00/61.8k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "episode_000001.parquet: 100% 56.0k/56.0k [00:00<00:00, 4.29MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000001.parquet\n",
            "episode_000003.parquet: 100% 45.1k/45.1k [00:00<00:00, 3.87MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000003.parquet\n",
            "\n",
            "episode_000000.parquet: 100% 61.8k/61.8k [00:00<00:00, 2.58MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000000.parquet\n",
            "\n",
            "\n",
            "episode_000004.parquet: 100% 51.8k/51.8k [00:00<00:00, 4.10MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000004.parquet\n",
            "episode_000005.parquet: 100% 45.6k/45.6k [00:00<00:00, 3.54MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000005.parquet\n",
            "\n",
            "episode_000002.parquet:   0% 0.00/51.0k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "README.md: 100% 3.10k/3.10k [00:00<00:00, 26.4MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/README.md\n",
            "episode_000002.parquet: 100% 51.0k/51.0k [00:00<00:00, 3.75MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000002.parquet\n",
            "Downloading 'data/chunk-000/episode_000007.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/1d533h4ypW6i_-P9ejcguxrCaPM=.18cac2a5d7b02a9fd3bf9429893bc0b4f223a1ad61ead2d518ac0f6fcd833118.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000006.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/oo-hXxuwhJ7RGGbeILD7o43Hu6o=.c6c0abcbffea69da0c44e4d129ce20dbcb27f6df297c6064654649a7f14203c1.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000008.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/FNURp8HeA8-i1yewJ18DZKy6hBE=.f945e3abacfcdad34431690c761669cae35e1eaf40cdf53119c8013edec81cd1.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000010.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/rsLn5BGebIIw3IlGoKoDqdbDfJY=.424d238a8ee600f0457bfe3f599f622022785b2e151f5188a98474efcacb106c.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000011.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/JQ2X4gG6nwj5wZMKNyFm_75pPUk=.1da495ce65266d0d1d14484867eb8fb2d553c635f445fb957861ed237106fc06.incomplete'\n",
            "\n",
            "episode_000007.parquet:   0% 0.00/51.8k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "episode_000007.parquet: 100% 51.8k/51.8k [00:00<00:00, 4.50MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000007.parquet\n",
            "episode_000008.parquet: 100% 43.0k/43.0k [00:00<00:00, 3.47MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000008.parquet\n",
            "\n",
            "episode_000010.parquet: 100% 54.4k/54.4k [00:00<00:00, 18.6MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000010.parquet\n",
            "\n",
            "episode_000006.parquet: 100% 39.3k/39.3k [00:00<00:00, 12.7MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000006.parquet\n",
            "\n",
            "episode_000011.parquet:   0% 0.00/52.7k [00:00<?, ?B/s]\u001b[ADownloading 'data/chunk-000/episode_000015.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/u8GZgFEljpyIUPL_IlWveQkE4bs=.6f150aa58769a9522015e48b8389d84380a3689eed1cfb8e90bcf4dfaaa35984.incomplete'\n",
            "\n",
            "\n",
            ".gitattributes: 100% 2.46k/2.46k [00:00<00:00, 24.4MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/.gitattributes\n",
            "episode_000011.parquet: 100% 52.7k/52.7k [00:00<00:00, 3.02MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000011.parquet\n",
            "Downloading 'data/chunk-000/episode_000012.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/TPqbhsWoatE4yKHw2We8Qg8XB3E=.0caceb5d622c4acee04f04b4d5b22992f1a1bda503fe1a8f46d7bbd33d97ed16.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000013.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/P7NzDX_I7S6_SWyl55hM3vUUTys=.d8849f3a39642fe951b22c407870fd27edd1798671f3d3886975c703b0fd0989.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000014.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/A0l4hNmU7lRjoEk64OZsJbvpci0=.acf26adc5f8254bbe8f2e7afc6bfeabcd88330f1edebd7866099d0279850daf5.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000009.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/L1U0W438Fu_YRcBknvo4psb4yGo=.3480ff60c1fbf19cd70536d823cc71ff4cb8e1635cc8c1a9643dc827fcf389eb.incomplete'\n",
            "Fetching 86 files:   1% 1/86 [00:00<00:37,  2.28it/s]Downloading 'data/chunk-000/episode_000016.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/rQy_R7aepl1p-x8IAbvG49rmwXI=.f8c612dd800b9277a4b6f0ef19b6114067719ebddaad8ebd7e5d63b8b19cf9e8.incomplete'\n",
            "\n",
            "episode_000015.parquet: 100% 43.1k/43.1k [00:00<00:00, 10.8MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000015.parquet\n",
            "\n",
            "episode_000012.parquet:   0% 0.00/48.1k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "episode_000012.parquet: 100% 48.1k/48.1k [00:00<00:00, 15.7MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000012.parquet\n",
            "\n",
            "episode_000013.parquet: 100% 45.0k/45.0k [00:00<00:00, 11.9MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000013.parquet\n",
            "episode_000014.parquet: 100% 41.7k/41.7k [00:00<00:00, 10.9MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000014.parquet\n",
            "\n",
            "episode_000009.parquet:   0% 0.00/37.8k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "episode_000009.parquet: 100% 37.8k/37.8k [00:00<00:00, 3.71MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000009.parquet\n",
            "Downloading 'data/chunk-000/episode_000017.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/ozqif_TQP2X1DHgMmtW6yBPBUqo=.f9c3ab7c0816e023b0c9599e9bccf6712d4b826843ddf2f7d7310e80da0c2cc3.incomplete'\n",
            "episode_000016.parquet: 100% 46.1k/46.1k [00:00<00:00, 8.51MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000016.parquet\n",
            "Downloading 'data/chunk-000/episode_000018.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/6biO3olOD5nJEJQBi-EJJYTOyn4=.babb911edaa6701fcef10aaa1a01045b9c2149d4c2100cff3ec69cc657ce2690.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000020.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/kAnA1JDRWpD44_jOoqBhQvr8yao=.6f8454e7b674c71f9138f31472cde3e5f8b03d743ef3c16031f16d4f0f7feb13.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000021.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/X1vFYaaQ6l2TZRowcx6SWVPcMmE=.977d748308224cbdafda7c6771500be71f9766821acbd0dc9fccd4c9cd04aa6c.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000022.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/P3usDF3yP4AtWArpIDLVQz6VO3I=.2e2e7e351ad8b5a5264eeddf2edf6b84663635958f913006224db55b161a1205.incomplete'\n",
            "\n",
            "episode_000017.parquet:   0% 0.00/54.6k [00:00<?, ?B/s]\u001b[ADownloading 'data/chunk-000/episode_000019.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/DicwzzxqerJmAskDiOn4wMk3ozw=.8c91facbc8c500d756ed64d152c3df9dbc2cae69e69a3accb98bd551fa7bc18b.incomplete'\n",
            "episode_000017.parquet: 100% 54.6k/54.6k [00:00<00:00, 5.93MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000017.parquet\n",
            "Fetching 86 files:  23% 20/86 [00:00<00:01, 46.68it/s]Downloading 'data/chunk-000/episode_000024.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/JQOCHO9v0JM9H6i9avhztzFxn54=.7fe6d9dedac6db6b435e7ece896e5343ce1c24da6f844f07fff5cd19a68d71c1.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000023.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/hfUp9qsRjsaz10T0r_kxUJb1PzM=.97cdca41fac68ab813ef8c211ad0470486479851d210430294867e5bce42a1c3.incomplete'\n",
            "\n",
            "episode_000020.parquet:   0% 0.00/42.0k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "episode_000020.parquet: 100% 42.0k/42.0k [00:00<00:00, 14.4MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000020.parquet\n",
            "\n",
            "episode_000021.parquet: 100% 31.2k/31.2k [00:00<00:00, 55.7MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000021.parquet\n",
            "\n",
            "episode_000022.parquet: 100% 41.8k/41.8k [00:00<00:00, 18.2MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000022.parquet\n",
            "episode_000018.parquet: 100% 39.3k/39.3k [00:00<00:00, 3.40MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000018.parquet\n",
            "\n",
            "episode_000019.parquet: 100% 43.7k/43.7k [00:00<00:00, 14.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000019.parquet\n",
            "\n",
            "episode_000024.parquet: 100% 34.2k/34.2k [00:00<00:00, 25.5MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000024.parquet\n",
            "\n",
            "episode_000023.parquet: 100% 40.2k/40.2k [00:00<00:00, 10.0MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000023.parquet\n",
            "Downloading 'data/chunk-000/episode_000026.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/XqhQ3OuXph8ZzfbQ938Ih0Br1BA=.69227849a6fbad06689e3feda186eb32e668cb14b4dbe11348833c741cf96db6.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000027.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/oghbAfzL0dQC8G9LIkthTnv6kKs=.b2af63d7c7b69631df3f1c82fbbc6a2901c7ce4a1dd418e84267e50f8f27b61e.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000028.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/Qt39NVoiIe3SlqER6VbVFooiBUI=.36049164ae1ca1a0baf66ff98585219463a36cf3a5f338c4ce24ab3111e06549.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000029.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/opdxwt05yqhVvDzrD6x5U-Z9mCk=.430d4b95c855bf21ac5b86f262564fd58103429c0c089804ea1ea7042c8c0734.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000030.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/bUdkaBS7BCUc894SOOEjCPeiu2Q=.e143c5b07a9e8522c557027a1dcfbd14c9b6ea86132940206617d6071cd6c8b8.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000031.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/H1Om1Sn5gSm9_wavda5WgWxdVqM=.d757203d8a54db154725263bde0897ddc804fb9138dd8a260c7902fc15e1b025.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000025.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/9UxJgHTg9tsGuPYDdr_du6ahv9I=.b010946ef90b79060eaad270c89644ba383b87d858584d362183147e605392ee.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000032.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/UawB5YWb8ss0olgzpV7nWf33qgI=.1cc2b3c031bde630520f131fe0834226b69c676fae6d44aab5260eed478dc251.incomplete'\n",
            "\n",
            "episode_000026.parquet: 100% 39.9k/39.9k [00:00<00:00, 10.5MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000026.parquet\n",
            "\n",
            "episode_000027.parquet: 100% 41.8k/41.8k [00:00<00:00, 13.4MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000027.parquet\n",
            "\n",
            "episode_000028.parquet: 100% 31.1k/31.1k [00:00<00:00, 52.2MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000028.parquet\n",
            "\n",
            "episode_000029.parquet: 100% 34.8k/34.8k [00:00<00:00, 28.2MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000029.parquet\n",
            "\n",
            "episode_000030.parquet: 100% 25.6k/25.6k [00:00<00:00, 22.8MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000030.parquet\n",
            "\n",
            "episode_000031.parquet:   0% 0.00/25.7k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "episode_000031.parquet: 100% 25.7k/25.7k [00:00<00:00, 27.3MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000031.parquet\n",
            "episode_000025.parquet: 100% 37.1k/37.1k [00:00<00:00, 24.0MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000025.parquet\n",
            "\n",
            "episode_000032.parquet: 100% 32.1k/32.1k [00:00<00:00, 13.8MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000032.parquet\n",
            "Downloading 'data/chunk-000/episode_000033.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/ymTADZxPQlMdlZGofS8XPs5P8Ko=.94e2afbfb5b0e248f08d5a2f28b3a76a0c17307611f63e064b18441ffed46fbc.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000034.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/W7uphJ2IDhKHSBDC2uw5rsWy9lM=.9058128dc2f7bc02a85da6d1aec519d7f0a6e363b33d99205aefcc98b565394c.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000035.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/yRlyCAKuLEPSJPhpEToPK0m2K9A=.2f2def3d74303259a9d661b0fb1c30a5120a5043f5dfffbf99f7ea5bdf4a6123.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000036.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/KNUYHjLT4Jgk8Othn7hoIeFxFAo=.516240ebe2a155a8696697447d1a69bc8d9733e56d2e45ccf669909eee15416e.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000037.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/qoxNvU6W4JflWiwgID4mn8vj7vI=.a67d3323b7c5340927da658a245e1c9ec7c4421bcb6d1eecbc923dc4840b408d.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000038.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/H1Kgl9KCydxJM7ZLyrt78HmE_Nc=.b7490e5cac920406fd7d1b01ab7748a45d43168491a67b87d58de74ed0998a9f.incomplete'\n",
            "Downloading 'meta/episodes.jsonl' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/meta/Xr2m2iHl-ydc-mHHfOvaRU5pi6o=.f1f2c7f47c0f004ebbbe9e7b2bec9c70ba4d5ce3.incomplete'\n",
            "Downloading 'data/chunk-000/episode_000039.parquet' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/data/chunk-000/f5E5idQanWFATMapRx5VTTSnBqU=.014aa84464000b5e4624cdd8ab0f9f0423b7a37d9e916be0de839b9acbab6d97.incomplete'\n",
            "\n",
            "episode_000033.parquet: 100% 34.5k/34.5k [00:00<00:00, 11.6MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000033.parquet\n",
            "\n",
            "episode_000034.parquet: 100% 30.1k/30.1k [00:00<00:00, 16.7MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000034.parquet\n",
            "\n",
            "episode_000036.parquet: 100% 31.2k/31.2k [00:00<00:00, 15.3MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000036.parquet\n",
            "\n",
            "episode_000037.parquet: 100% 31.9k/31.9k [00:00<00:00, 16.0MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000037.parquet\n",
            "\n",
            "episode_000038.parquet: 100% 46.5k/46.5k [00:00<00:00, 15.3MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000038.parquet\n",
            "\n",
            "episode_000039.parquet: 100% 44.9k/44.9k [00:00<00:00, 17.0MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000039.parquet\n",
            "\n",
            "episodes.jsonl: 100% 3.96k/3.96k [00:00<00:00, 39.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/meta/episodes.jsonl\n",
            "Downloading 'meta/episodes_stats.jsonl' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/meta/OSV6LZWLsNGAwnFDfYELN9vNGMw=.c52b037c27b200ed07ab42e9c5e56143a7f3c035.incomplete'\n",
            "Downloading 'meta/info.json' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/meta/c-Q9HGFI0JpMTOZ5pylhXwaYu2U=.570801e12d9fe37b370562ffd6e251014e7ddee2.incomplete'\n",
            "Downloading 'meta/tasks.jsonl' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/meta/NzWDcOlnTQhMC9idVZndPYjQOjQ=.440025e811d60e71462fcc282d3352883c03eba7.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000000.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/imvry3h1zKp82YhY9QWZv1oVwU8=.3eea6afd594a060dd87e7bc8616833c21712229d1e0b8279ef458c5a273c4c7b.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000001.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/5ogeEwC9AVE4EHnZMlI0DZQnERU=.d284b208c799a93de47dc1d61769f7cffc171543b4d39fc06ad93c59dd88a1b1.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000002.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/xMvbgocDq0hz_kCVqoNglajB0rA=.3390d37168ef911ebb6c8310a9bc7c664463ea7f07b39a0226b991fb18704f3a.incomplete'\n",
            "\n",
            "episodes_stats.jsonl:   0% 0.00/72.7k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tasks.jsonl: 100% 77.0/77.0 [00:00<00:00, 790kB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/meta/tasks.jsonl\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000003.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/iV5CmXd8QF-JHBkA8sFYzWpKqDQ=.c94a23b022137def4f51636c4c361104a7e681be5c147f54ce2c5fe7e3a5b3b5.incomplete'\n",
            "\n",
            "\n",
            "episode_000000.mp4:   0% 0.00/7.66M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "episodes_stats.jsonl: 100% 72.7k/72.7k [00:00<00:00, 3.07MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/meta/episodes_stats.jsonl\n",
            "\n",
            "episode_000002.mp4:   0% 0.00/6.45M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "episode_000035.parquet:   0% 0.00/38.1k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000035.parquet: 100% 38.1k/38.1k [00:00<00:00, 13.4MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/data/chunk-000/episode_000035.parquet\n",
            "Fetching 86 files:  45% 39/86 [00:00<00:01, 46.13it/s]Downloading 'videos/chunk-000/observation.images.webcam/episode_000005.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/qx3u-hkClAjgo-ZPan0hEhsmUlo=.77c35332bc002462009248044a640c19df10cdf485de101551a294efb2e975fa.incomplete'\n",
            "episode_000000.mp4: 100% 7.66M/7.66M [00:00<00:00, 84.2MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000000.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000006.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/BvNeSsQdfCuyiW7MclnHUKasQ1U=.d4c1dfd53df92e4f6dcee06207c334b24c954da5bd1f6146759e33abb4c95826.incomplete'\n",
            "episode_000002.mp4: 100% 6.45M/6.45M [00:00<00:00, 66.6MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000002.mp4\n",
            "\n",
            "\n",
            "\n",
            "episode_000001.mp4: 100% 6.56M/6.56M [00:00<00:00, 65.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "episode_000001.mp4: 100% 6.56M/6.56M [00:00<00:00, 61.8MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000001.mp4\n",
            "episode_000003.mp4: 100% 5.85M/5.85M [00:00<00:00, 63.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000003.mp4\n",
            "\n",
            "\n",
            "episode_000006.mp4:   0% 0.00/4.78M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000007.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/OlS7Fz2Qll3lulobR7dRkNXfOJ8=.c71f58154e06b3bbe4a7e04b7fe292f90536ac6b8871cea1488c5d2cf4bcb01c.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000004.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/JtchEMKBVSF4szBBhQ_pFHao7iE=.4efd81a949f658679866c889c709be75e0505137867cddda30bfa01ab2f2f815.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "info.json: 100% 2.60k/2.60k [00:00<00:00, 21.8MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/meta/info.json\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000008.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/ozXBYyi3uo3OHZ43QRhayUyIVcM=.531778e01fb31d5b47dc8ca2f2c9eab12345cc3cebe5cf4aa5547dcddb6cf0d8.incomplete'\n",
            "Fetching 86 files:  53% 46/86 [00:01<00:00, 47.90it/s]Downloading 'videos/chunk-000/observation.images.webcam/episode_000009.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/5nRTCWQX4qbZHe4zKYRjz9zk5V4=.44df0553261a59741617547a3004f7b26530acaf77781d2315b91f4214068b8f.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000010.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/IYOKrRWlmzoXxd1aAzMBaXU5mhk=.3ad7ba6dbc5ae10f3c39d80945aa9ef2bfc88abcf6f96121a0287265fb2a656e.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "episode_000007.mp4:   0% 0.00/6.60M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "episode_000005.mp4: 100% 5.63M/5.63M [00:00<00:00, 60.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000005.mp4\n",
            "\n",
            "episode_000008.mp4:   0% 0.00/5.51M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000006.mp4: 100% 4.78M/4.78M [00:00<00:00, 54.3MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000006.mp4\n",
            "\n",
            "\n",
            "episode_000007.mp4: 100% 6.60M/6.60M [00:00<00:00, 111MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000007.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000011.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/Oj7USlBbvcxRmSke9L16fyiHXFc=.42fbc975921045215911f1dd429114ea1333f25a3f6992ee76690a902b08c7b0.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000012.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/xjwvpdTnQ14E0Ss66D_p3HjUbok=.3c964bc09f86b35decd63cfe12c90f0706e38ecf46e2efc3f2a8a2e3cb6e30f1.incomplete'\n",
            "episode_000008.mp4: 100% 5.51M/5.51M [00:00<00:00, 85.4MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000008.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000013.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/Wz3r7oROaqxL_Wup0ZL7mFkIv7Y=.c2871212a345c3f4f0ecba49cbf4be28dcd8734e294ed938578651131649bf59.incomplete'\n",
            "episode_000009.mp4: 100% 4.92M/4.92M [00:00<00:00, 69.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000009.mp4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000010.mp4: 100% 6.85M/6.85M [00:00<00:00, 103MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000010.mp4\n",
            "episode_000004.mp4: 100% 6.70M/6.70M [00:00<00:00, 60.9MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000004.mp4\n",
            "\n",
            "Fetching 86 files:  62% 53/86 [00:01<00:00, 48.55it/s]Downloading 'videos/chunk-000/observation.images.webcam/episode_000014.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/Sv8PaxdMXuwnj3w2vj5lzTdgGjw=.b3b6db51a2408be01a070985811b90c7c344849952ffea57fab99cb811af0ad5.incomplete'\n",
            "\n",
            "\n",
            "episode_000012.mp4:   0% 0.00/6.26M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "episode_000013.mp4:   0% 0.00/5.45M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000015.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/t-WRHkGwuZM4JEBx2ETgx3Vl-zI=.e329da59243f59abc165e0d0046045e5aabe3a5d73969d1b541f90f4f588bfee.incomplete'\n",
            "episode_000012.mp4: 100% 6.26M/6.26M [00:00<00:00, 165MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000012.mp4\n",
            "\n",
            "\n",
            "episode_000014.mp4:   0% 0.00/5.28M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000017.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/bQkFizHLEq-f7rV3ZUaOz659lFQ=.eac3ec17672eceb512cefde38f4972c67f34ef3117793bf4791ca16f4733639e.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000013.mp4: 100% 5.45M/5.45M [00:00<00:00, 91.7MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000013.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000018.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/JWYej9zuSmQi1ebdIyXNtDPHrb4=.c9163eb54e7a288c41e8e8b464ee38fb6aa9d37afada826457a1715e11128189.incomplete'\n",
            "episode_000011.mp4: 100% 6.76M/6.76M [00:00<00:00, 70.2MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000011.mp4\n",
            "\n",
            "episode_000014.mp4: 100% 5.28M/5.28M [00:00<00:00, 92.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000014.mp4\n",
            "episode_000015.mp4: 100% 5.46M/5.46M [00:00<00:00, 93.6MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000015.mp4\n",
            "\n",
            "\n",
            "episode_000018.mp4:   0% 0.00/4.70M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000019.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/8MChvrUWaV6JIMgnMkNTH9eXZW4=.35a649036bc95a9d4368639e1f7b138ff2be03ed7f15d1c910301980ec4b2f1e.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000016.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/j2iDxfGamm5mZVXgMANnv2GBmjk=.291e5e468ce5b8bf11884d89c94514c75698cef30509028c4bbe01780b1b9c2e.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000020.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/vo7PPKLkGjKJqISzuCGIAXUZfZk=.797f740fdfa3e75fd37e533f639ef66abd7168a63d640c8cee631bb2a74f8381.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000021.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/PLqGIUEhCQCxuzNkYByYH34AMSc=.3659639368ddc59d84831fe4a99e892f6d0a0f05fe3d7e7be9a90856df6afe8f.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000022.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/CRJYeBp742y_4Cr4mLrZq0Q6jo4=.8424e8bd97f6c89de9b704742756b3566e7ca09c60011ad9193d1d79e0e316d7.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "episode_000019.mp4:   0% 0.00/5.58M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "episode_000016.mp4:   0% 0.00/5.75M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000023.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/vKQnTPMWh-9rieHtW2r4d91QHCg=.ef6e601be481eca3b9ddcfc77320cdc44a165148a7cfbe3d639a35ceae52b9e0.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000020.mp4:   0% 0.00/5.90M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000017.mp4: 100% 6.70M/6.70M [00:00<00:00, 67.7MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000017.mp4\n",
            "\n",
            "episode_000018.mp4: 100% 4.70M/4.70M [00:00<00:00, 56.9MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000018.mp4\n",
            "episode_000019.mp4: 100% 5.58M/5.58M [00:00<00:00, 123MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000019.mp4\n",
            "\n",
            "\n",
            "episode_000021.mp4: 100% 3.84M/3.84M [00:00<00:00, 107MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000021.mp4\n",
            "episode_000016.mp4: 100% 5.75M/5.75M [00:00<00:00, 89.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000016.mp4\n",
            "episode_000020.mp4: 100% 5.90M/5.90M [00:00<00:00, 95.8MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000020.mp4\n",
            "episode_000022.mp4: 100% 5.62M/5.62M [00:00<00:00, 130MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000022.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000024.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/QCv1K2FtjS70UZC7nxqgojyotUo=.6ffafa5ca57f04209e9a53fc4e3ed44743a00c64361ff59c9b088e6f0e71fbcc.incomplete'\n",
            "episode_000023.mp4: 100% 5.24M/5.24M [00:00<00:00, 89.0MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000023.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000027.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/ZHv8sjn_Zd0yRb7ik0TAqWsT0AE=.f8db87e7433f0c149ec7bc28f3843daff1e34874358944a050870bec6a0a96e2.incomplete'\n",
            "\n",
            "episode_000024.mp4:   0% 0.00/4.60M [00:00<?, ?B/s]\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000025.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/gP0jZ1ODa3N3x7J2jQxJ3q2aB5w=.bca924739737c8204fb716c65d03dee145d58dac2c4046e883d7ddc28c6ca2ac.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000026.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/11M6lWw__5EsDS9Z-IcB9rEN9dw=.37efc848a2bd36a0cae1f50e15631047589ea35d662b3ef2762e9bf26cc6d5df.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000029.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/TT209caBhgeegi7gtpl04L_J8CA=.feab97e51eb41f03d603217166e297c14af4d5a8ab888947c845f87838c72040.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000030.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/p0Ut39-jDeBGsRgFkearwx2dFD8=.011494126fec2446e7d3f1b215367c589b72eefb6611c37fde88a9be2d74be55.incomplete'\n",
            "\n",
            "\n",
            "episode_000027.mp4:   0% 0.00/5.81M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000031.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/ndOegmWZmkQTMLrsKl5LP39bs5c=.a43fd9ab8b5e3af14781a653fa9de9eec345cb1f02b1ada1d2f6e6fbfb1537a8.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "episode_000026.mp4:   0% 0.00/5.14M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "episode_000029.mp4:   0% 0.00/4.17M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000024.mp4: 100% 4.60M/4.60M [00:00<00:00, 75.9MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000024.mp4\n",
            "episode_000030.mp4: 100% 3.04M/3.04M [00:00<00:00, 219MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000030.mp4\n",
            "episode_000029.mp4: 100% 4.17M/4.17M [00:00<00:00, 128MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000029.mp4\n",
            "\n",
            "episode_000027.mp4: 100% 5.81M/5.81M [00:00<00:00, 100MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000027.mp4\n",
            "\n",
            "\n",
            "episode_000026.mp4: 100% 5.14M/5.14M [00:00<00:00, 90.2MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000026.mp4\n",
            "episode_000031.mp4: 100% 3.19M/3.19M [00:00<00:00, 114MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000031.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000032.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/E6imY1qXUVmZ7HH4KEq3wr0eOd4=.0518e87ca1a79999771e71d5c9b494b0e6b53e6b4f1568770b85357c623bc452.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000033.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/HFlO99Hw9bOOsgS6YsFW8MgREqk=.9e642bca0c72383e0670ded224102ac2bfb6043583bdf6247309ed7317363aad.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000035.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/vY-neig0FPM7zVHyb5etItVFXBM=.754753b1b8a36b8cdcfc46d660beee0bcf8f98b45aaaf027d64a1220f93c3cd8.incomplete'\n",
            "\n",
            "episode_000032.mp4:   0% 0.00/3.95M [00:00<?, ?B/s]\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000036.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/QF_IPaupFOC95ML5mpjiv5y_tHk=.2e66668e8a0af873f3d5f86180bed8c31302b766cb70967a08a0b9c7d4e5b12c.incomplete'\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000037.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/8CV60zNrTOM75N8NDCgUInXi9M0=.7ff8e07b3565e7bc25b179e1c537d4b0571148d235870d76cd0c2ebab6eae020.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "episode_000033.mp4:   0% 0.00/4.60M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000028.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/5G2D0IB05WHH1IjzKdFWEULp7nE=.c8728c6f1dabe53ccb586a33958eaa1a900d7a8cf4bd92adb3b527040f167dc8.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000033.mp4: 100% 4.60M/4.60M [00:00<00:00, 134MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000033.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000034.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/J-JVFhqgPOxGzQjcAstbqSWo9XA=.522396d84405805c9f45da62b47563a773e088c0f25264fb80dbba72a31ac8bb.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "episode_000036.mp4:   0% 0.00/3.90M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000037.mp4:   0% 0.00/4.10M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode_000032.mp4: 100% 3.95M/3.95M [00:00<00:00, 66.9MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000032.mp4\n",
            "episode_000037.mp4: 100% 4.10M/4.10M [00:00<00:00, 129MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000037.mp4\n",
            "\n",
            "\n",
            "episode_000025.mp4: 100% 4.75M/4.75M [00:00<00:00, 29.6MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000025.mp4\n",
            "episode_000035.mp4: 100% 5.14M/5.14M [00:00<00:00, 82.2MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000035.mp4\n",
            "\n",
            "episode_000034.mp4:   0% 0.00/3.60M [00:00<?, ?B/s]\u001b[ADownloading 'videos/chunk-000/observation.images.webcam/episode_000038.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/lauN2h93wAK2tX-2cKR12Dul8po=.acf9aa3a743ded3fa3ab8578d103375407fc6637dab52b35d4e06bbb10310a0b.incomplete'\n",
            "episode_000036.mp4: 100% 3.90M/3.90M [00:00<00:00, 72.0MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000036.mp4\n",
            "episode_000028.mp4: 100% 3.71M/3.71M [00:00<00:00, 67.1MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000028.mp4\n",
            "Downloading 'videos/chunk-000/observation.images.webcam/episode_000039.mp4' to 'demo_data/so100_strawberry_grape/.cache/huggingface/download/videos/chunk-000/observation.images.webcam/isEc1_chVNh7_MC_8vpL0KmQC0Y=.5982579c1c3084fc8db76ba17509504f968a2d3b8e30ade5f38454a70e31365a.incomplete'\n",
            "episode_000034.mp4: 100% 3.60M/3.60M [00:00<00:00, 110MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000034.mp4\n",
            "\n",
            "episode_000038.mp4:   0% 0.00/6.55M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "episode_000038.mp4: 100% 6.55M/6.55M [00:00<00:00, 167MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000038.mp4\n",
            "episode_000039.mp4: 100% 6.36M/6.36M [00:00<00:00, 106MB/s]\n",
            "Download complete. Moving file to demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam/episode_000039.mp4\n",
            "Fetching 86 files: 100% 86/86 [00:01<00:00, 44.78it/s]\n",
            "/content/Isaac-GR00T/demo_data/so100_strawberry_grape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Path where the episode video files are stored\n",
        "video_path = \"demo_data/so100_strawberry_grape/videos/chunk-000/observation.images.webcam\"\n",
        "\n",
        "# List all episode video files (e.g., episode_000001.mp4, etc.)\n",
        "episode_files = glob(os.path.join(video_path, \"episode_*.mp4\"))\n",
        "\n",
        "# Count the number of episodes\n",
        "num_episodes = len(episode_files)\n",
        "\n",
        "print(f\"Total number of episodes: {num_episodes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcz37SwHhv9s",
        "outputId": "3a9ebf27-0cd0-4df9-a323-2095d941ca3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of episodes: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli repo clone youliangtan/so100_strawberry_grape \\\n",
        "  --type dataset \\\n",
        "  ./demo_data/so100_strawberry_grape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqyTGhzIhhLC",
        "outputId": "180e195a-a2b7-4492-c18e-d36288ac3f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/huggingface-cli\", line 4, in <module>\n",
            "    from huggingface_hub.commands.huggingface_cli import main\n",
            "ModuleNotFoundError: No module named 'huggingface_hub'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"youliangtan/so100_strawberry_grape\",\n",
        "    repo_type=\"dataset\",\n",
        "    local_dir=\"./demo_data/so100_strawberry_grape\",\n",
        "    local_dir_use_symlinks=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "x5alfIQnBlAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/content/demo_data/so100_strawberry_grape')"
      ],
      "metadata": {
        "id": "w-N_Uwv2BVhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_path = \"/content/demo_data/so100_strawberry_grape\"\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    level = root.replace(base_path, '').count(os.sep)\n",
        "    indent = ' ' * 2 * (level)\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for f in files:\n",
        "        print(f\"{subindent}{f}\")"
      ],
      "metadata": {
        "id": "im4JGGaHBvwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How the dataset looks like\n"
      ],
      "metadata": {
        "id": "V3zBsXluBLaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet('./demo_data/so100_strawberry_grape/data/chunk-000/episode_000000.parquet')\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "print(df.shape)\n"
      ],
      "metadata": {
        "id": "EBQcgvPVBKnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dimensionality of 'action' and 'observation.state'\n",
        "action_dim = df['action'].iloc[0]\n",
        "state_dim = df['observation.state'].iloc[0]\n",
        "\n",
        "print(f\"Number of action dimensions: {len(action_dim)}\")\n",
        "print(f\"Number of state dimensions: {len(state_dim)}\")\n"
      ],
      "metadata": {
        "id": "MmtyvLw0BKzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XqGZbG7GBK2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/demo_data/so100_strawberry_grape/meta\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/examples/so100__modality.json \\\n",
        "    -O /content/demo_data/so100_strawberry_grape/meta/modality.json"
      ],
      "metadata": {
        "id": "4i2SXI38B_Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json_path = '/content/demo_data/so100_strawberry_grape/meta/modality.json'\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "import pprint\n",
        "pprint.pprint(data)\n"
      ],
      "metadata": {
        "id": "fj24ensOCNK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "action\t6 total\t5D single arm + 1D gripper\n",
        "observation.state\t6 total\t5D single arm + 1D gripper"
      ],
      "metadata": {
        "id": "ThzwdwgjCy93"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3hBJzajACSN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s16gQqAQ40pW"
      },
      "outputs": [],
      "source": [
        "from gr00t.utils.misc import any_describe\n",
        "from gr00t.data.dataset import LeRobotSingleDataset\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "dataset_path = \"./demo_data/so100_strawberry_grape\"   # change this to your dataset path\n",
        "\n",
        "data_config = DATA_CONFIG_MAP[\"so100\"]\n",
        "\n",
        "dataset = LeRobotSingleDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    modality_configs=data_config.modality_config(),\n",
        "    embodiment_tag=\"new_embodiment\",\n",
        "    video_backend=\"torchvision_av\",\n",
        ")\n",
        "\n",
        "resp = dataset[7]\n",
        "any_describe(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How the modality config looks like"
      ],
      "metadata": {
        "id": "m9wa-RM7DzNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modality_configs = data_config.modality_config()\n",
        "\n",
        "print(\"\\nModality Configs:\")\n",
        "for key, value in modality_configs.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "H29N4N_LDngu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other Data Config"
      ],
      "metadata": {
        "id": "D6PMdtz1EpFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "\n",
        "print(\"Available data config maps:\")\n",
        "for key in DATA_CONFIG_MAP.keys():\n",
        "    print(f\"- {key}\")\n"
      ],
      "metadata": {
        "id": "0asvrom_CnO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, config in DATA_CONFIG_MAP.items():\n",
        "    print(f\"\\n=== {key} ===\")\n",
        "    modalities = config.modality_config()\n",
        "    print(\"Modalities:\", list(modalities.keys()))\n"
      ],
      "metadata": {
        "id": "vnt3aMXCCptp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so100_config = DATA_CONFIG_MAP[\"so100\"]\n",
        "\n",
        "modality_config = so100_config.modality_config()\n",
        "for modality_name, config in modality_config.items():\n",
        "    print(f\"\\nModality: {modality_name}\")\n",
        "    #pprint(config)"
      ],
      "metadata": {
        "id": "UWTRHjzED81f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    if i % 10 == 0:\n",
        "        resp = dataset[i]\n",
        "        img = resp[\"video.webcam\"][0]\n",
        "        images_list.append(img)\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(images_list[i])\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Image {i}\")\n",
        "plt.tight_layout() # adjust the subplots to fit into the figure area.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cG7crNODD9EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This often fails in Colab, can try:\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "uG3e2309Gf4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code works only when GPU is activated"
      ],
      "metadata": {
        "id": "E1k6PDJdmwjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.data.dataset import LeRobotSingleDataset\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "from gr00t.utils.eval import calc_mse_for_single_trajectory\n",
        "\n",
        "PRE_TRAINED_MODEL_PATH = \"nvidia/GR00T-N1-2B\"\n",
        "DATASET_PATH = \"./demo_data/so100_strawberry_grape\"  # or your dataset path\n",
        "EMBODIMENT_TAG = \"gr1\"  # This should match the robot embodiment (check your dataset or metadata)\n",
        "\n",
        "\n",
        "data_config = DATA_CONFIG_MAP[\"gr1_arms_only\"]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "pre_trained_policy = Gr00tPolicy(\n",
        "    model_path=PRE_TRAINED_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        "    device=\"cuda\"  # or \"cpu\"\n",
        ")\n",
        "\n",
        "\n",
        "dataset = LeRobotSingleDataset(\n",
        "    dataset_path=DATASET_PATH,\n",
        "    modality_configs=modality_config,\n",
        "    video_backend=\"decord\",\n",
        "    video_backend_kwargs=None,\n",
        "    transforms=None,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        ")\n",
        "\n",
        "\n",
        "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
        "mse = calc_mse_for_single_trajectory(\n",
        "    pre_trained_policy,\n",
        "    dataset,\n",
        "    traj_id=0,\n",
        "    modality_keys=[\"right_arm\", \"right_hand\"],\n",
        "    steps=150,\n",
        "    action_horizon=16,\n",
        "    plot=True\n",
        ")\n",
        "\n",
        "print(\"Baseline MSE loss for trajectory 0:\", mse)\n"
      ],
      "metadata": {
        "id": "uJnOlh1MF1LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "godDEr5ZGZ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfQTvdsvGek6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KUKA robot"
      ],
      "metadata": {
        "id": "hQ2YEqDuD_a5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TOItTUW5_YnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "MiGr-_cLLJIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d64ff2-6bd9-4b7d-bcca-09beb406f08f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting beautifulsoup4 (from gdown)\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
            "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [gdown]\n",
            "\u001b[1A\u001b[2KSuccessfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 gdown-5.2.0 soupsieve-2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "file_id = '1JnupPL8KHbDg2nyb8IzbGWtDmjvV_Xoy'\n",
        "output = '/content/Kuka_Kr60ha_DataLogs.zip'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)"
      ],
      "metadata": {
        "id": "9CPNLl_OLPOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "acc7d7b8-e31d-480b-b08a-c513a61b4e3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JnupPL8KHbDg2nyb8IzbGWtDmjvV_Xoy\n",
            "From (redirected): https://drive.google.com/uc?id=1JnupPL8KHbDg2nyb8IzbGWtDmjvV_Xoy&confirm=t&uuid=2fe491f4-9ab1-43f5-a895-afea4eba031d\n",
            "To: /content/Kuka_Kr60ha_DataLogs.zip\n",
            "100%|██████████| 209M/209M [00:01<00:00, 124MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Kuka_Kr60ha_DataLogs.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jqZc7Dg_-H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "extract_path = '/content/Kuka_Kr60ha_DataLogs'\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    print(f\"\\n Directory: {root}\")\n",
        "    for file in files:\n",
        "        print(f\" - {file}\")\n"
      ],
      "metadata": {
        "id": "Ixl9aJRzK_7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0473800-c56d-4630-d847-908b8ffd38f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs\n",
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset\n",
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos\n",
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000\n",
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/videos.webcam\n",
            " - episode_000017.mp4\n",
            " - episode_000025.mp4\n",
            " - episode_000008.mp4\n",
            " - episode_000005.mp4\n",
            " - episode_000019.mp4\n",
            " - episode_000021.mp4\n",
            " - episode_000029.mp4\n",
            " - episode_000028.mp4\n",
            " - episode_000001.mp4\n",
            " - Episode_000022.mp4\n",
            " - episode_000006.mp4\n",
            " - episode_000023.mp4\n",
            " - episode_000007.mp4\n",
            " - episode_000003.mp4\n",
            " - episode_000016.mp4\n",
            " - episode_000014.mp4\n",
            " - episode_000015.mp4\n",
            " - episode_000009.mp4\n",
            " - episode_000011.mp4\n",
            " - episode_000010.mp4\n",
            " - episode_000013.mp4\n",
            " - episode_000004.mp4\n",
            " - episode_000018.mp4\n",
            " - episode_000002.mp4\n",
            " - episode_000030.mp4\n",
            " - episode_000027.mp4\n",
            " - episode_000012.mp4\n",
            " - episode_000020.mp4\n",
            " - episode_000000.mp4\n",
            " - episode_000024.mp4\n",
            " - episode_000026.mp4\n",
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/data\n",
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/data/chunk-000\n",
            " - Episode_000021.parquet\n",
            " - Episode_000010.parquet\n",
            " - Episode_000014.parquet\n",
            " - Episode_000004.parquet\n",
            " - Episode_000019.parquet\n",
            " - Episode_000002.parquet\n",
            " - Episode_000008.parquet\n",
            " - Episode_000028.parquet\n",
            " - Episode_000024.parquet\n",
            " - Episode_000013.parquet\n",
            " - Episode_000017.parquet\n",
            " - Episode_000023.parquet\n",
            " - Episode_000020.parquet\n",
            " - Episode_000000.parquet\n",
            " - Episode_000007.parquet\n",
            " - Episode_000012.parquet\n",
            " - Episode_000006.parquet\n",
            " - Episode_000025.parquet\n",
            " - Episode_000018.parquet\n",
            " - Episode_000015.parquet\n",
            " - Episode_000026.parquet\n",
            " - Episode_000027.parquet\n",
            " - Episode_000003.parquet\n",
            " - Episode_000016.parquet\n",
            " - Episode_000030.parquet\n",
            " - Episode_000005.parquet\n",
            " - Episode_000022.parquet\n",
            " - Episode_000011.parquet\n",
            " - Episode_000001.parquet\n",
            " - Episode_000029.parquet\n",
            " - Episode_000009.parquet\n",
            "\n",
            " Directory: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta\n",
            " - episodes.jsonl\n",
            " - episodes_stats.jsonl\n",
            " - info.json\n",
            " - tasks.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Patch may not be needed in future"
      ],
      "metadata": {
        "id": "N9Iz-TX0RSSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "video_dir = '/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/videos.webcam'\n",
        "upper_name = 'Episode_000022.mp4'\n",
        "lower_name = 'episode_000022.mp4'\n",
        "temp_name = 'temp_rename_target_022.mp4'\n",
        "\n",
        "upper_path = os.path.join(video_dir, upper_name)\n",
        "lower_path = os.path.join(video_dir, lower_name)\n",
        "temp_path = os.path.join(video_dir, temp_name)\n",
        "\n",
        "\n",
        "if os.path.exists(lower_path):\n",
        "    print(f\"{lower_name} already exists. No rename needed.\")\n",
        "elif os.path.exists(upper_path):\n",
        "    print(f\"Renaming {upper_path} → {lower_path} via temporary file.\")\n",
        "    os.rename(upper_path, temp_path)\n",
        "    os.rename(temp_path, lower_path)\n",
        "    print(\"Rename complete.\")\n",
        "else:\n",
        "    print(\"Neither version exists. Nothing to rename.\")\n",
        "\n",
        "print(\"\\n Listing all files in videos.webcam after rename:\")\n",
        "for file in sorted(os.listdir(video_dir)):\n",
        "    print(\" -\", file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jSItVH1RV5r",
        "outputId": "98ddfbc2-ffe1-4fb3-88da-c26d79deac80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renaming /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/videos.webcam/Episode_000022.mp4 → /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/videos.webcam/episode_000022.mp4 via temporary file.\n",
            "Rename complete.\n",
            "\n",
            " Listing all files in videos.webcam after rename:\n",
            " - episode_000000.mp4\n",
            " - episode_000001.mp4\n",
            " - episode_000002.mp4\n",
            " - episode_000003.mp4\n",
            " - episode_000004.mp4\n",
            " - episode_000005.mp4\n",
            " - episode_000006.mp4\n",
            " - episode_000007.mp4\n",
            " - episode_000008.mp4\n",
            " - episode_000009.mp4\n",
            " - episode_000010.mp4\n",
            " - episode_000011.mp4\n",
            " - episode_000012.mp4\n",
            " - episode_000013.mp4\n",
            " - episode_000014.mp4\n",
            " - episode_000015.mp4\n",
            " - episode_000016.mp4\n",
            " - episode_000017.mp4\n",
            " - episode_000018.mp4\n",
            " - episode_000019.mp4\n",
            " - episode_000020.mp4\n",
            " - episode_000021.mp4\n",
            " - episode_000022.mp4\n",
            " - episode_000023.mp4\n",
            " - episode_000024.mp4\n",
            " - episode_000025.mp4\n",
            " - episode_000026.mp4\n",
            " - episode_000027.mp4\n",
            " - episode_000028.mp4\n",
            " - episode_000029.mp4\n",
            " - episode_000030.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uTC2Y4IGRV8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet('/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/data/chunk-000/Episode_000000.parquet')\n",
        "\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "print(\"\\nType check:\")\n",
        "print(\"Type of observation.state:\", type(df['observation.state'].iloc[0]))\n",
        "print(\"Value of observation.state:\", df['observation.state'].iloc[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "iA66XPSaFPX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ecba66-26d3-408c-98c6-2454c631fcdd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['observation.state', 'action', 'timestamp', 'frame_index',\n",
            "       'episode_index', 'index', 'task_index'],\n",
            "      dtype='object')\n",
            "                                   observation.state  \\\n",
            "0  [0.000000 -90.000000 90.000000 0.000000 0.0000...   \n",
            "1  [0.000000 -90.000000 90.000000 0.000000 0.0000...   \n",
            "2  [0.000000 -90.000000 90.000000 0.000000 0.0000...   \n",
            "3  [0.000000 -90.000000 90.000000 0.000000 0.0000...   \n",
            "4  [0.000000 -90.000000 90.000000 0.000000 0.0000...   \n",
            "\n",
            "                                              action  timestamp  frame_index  \\\n",
            "0  [0.000000 -90.000000 90.000000 0.000000 0.0000...       0.00            0   \n",
            "1  [0.000000 -90.000000 90.000000 0.000000 0.0000...       0.05            1   \n",
            "2  [0.000000 -90.000000 90.000000 0.000000 0.0000...       0.10            2   \n",
            "3  [0.000000 -90.000000 90.000000 0.000000 0.0000...       0.15            3   \n",
            "4  [0.000000 -90.000000 90.000000 0.000000 0.0000...       0.20            4   \n",
            "\n",
            "   episode_index  index  task_index  \n",
            "0              0      0           0  \n",
            "1              0      1           0  \n",
            "2              0      2           0  \n",
            "3              0      3           0  \n",
            "4              0      4           0  \n",
            "\n",
            "Type check:\n",
            "Type of observation.state: <class 'str'>\n",
            "Value of observation.state: [0.000000 -90.000000 90.000000 0.000000 0.000000 0.000000 1259.520000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_dim = len(df['action'].iloc[0])\n",
        "state_dim = len(df['observation.state'].iloc[0])\n",
        "\n",
        "print(f\"Action dimension: {action_dim}\")\n",
        "print(f\"State dimension: {state_dim}\")\n"
      ],
      "metadata": {
        "id": "OMT3vMSWFW7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c167617-cade-4a8f-cea7-daf87264fa4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action dimension: 70\n",
            "State dimension: 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PATCH 1"
      ],
      "metadata": {
        "id": "tDEj8KrHm9R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "base_dir = '/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/data/chunk-000'\n",
        "\n",
        "# Function to parse string array into numpy array\n",
        "def parse_array_string(s):\n",
        "    if isinstance(s, str):\n",
        "        s = s.strip('[]')\n",
        "        return np.array([float(x) for x in s.strip().split() if x], dtype=np.float32)\n",
        "    return np.array(s, dtype=np.float32)\n",
        "\n",
        "# Process all parquet files in the directory\n",
        "for file_name in os.listdir(base_dir):\n",
        "    if file_name.endswith('.parquet'):\n",
        "        parquet_path = os.path.join(base_dir, file_name)\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "\n",
        "        # Apply conversion\n",
        "        df['observation.state'] = df['observation.state'].apply(parse_array_string)\n",
        "        df['action'] = df['action'].apply(parse_array_string)\n",
        "\n",
        "        # Overwrite parquet file\n",
        "        df.to_parquet(parquet_path)\n",
        "        print(f\" Patched: {file_name}\")\n",
        "\n",
        "print(\"\\nAll parquet files patched: string arrays converted to real arrays.\")\n"
      ],
      "metadata": {
        "id": "oOUInZZLq4kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363cf504-8fb8-48d1-e1be-a73ff1839c04"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Patched: Episode_000021.parquet\n",
            " Patched: Episode_000010.parquet\n",
            " Patched: Episode_000014.parquet\n",
            " Patched: Episode_000004.parquet\n",
            " Patched: Episode_000019.parquet\n",
            " Patched: Episode_000002.parquet\n",
            " Patched: Episode_000008.parquet\n",
            " Patched: Episode_000028.parquet\n",
            " Patched: Episode_000024.parquet\n",
            " Patched: Episode_000013.parquet\n",
            " Patched: Episode_000017.parquet\n",
            " Patched: Episode_000023.parquet\n",
            " Patched: Episode_000020.parquet\n",
            " Patched: Episode_000000.parquet\n",
            " Patched: Episode_000007.parquet\n",
            " Patched: Episode_000012.parquet\n",
            " Patched: Episode_000006.parquet\n",
            " Patched: Episode_000025.parquet\n",
            " Patched: Episode_000018.parquet\n",
            " Patched: Episode_000015.parquet\n",
            " Patched: Episode_000026.parquet\n",
            " Patched: Episode_000027.parquet\n",
            " Patched: Episode_000003.parquet\n",
            " Patched: Episode_000016.parquet\n",
            " Patched: Episode_000030.parquet\n",
            " Patched: Episode_000005.parquet\n",
            " Patched: Episode_000022.parquet\n",
            " Patched: Episode_000011.parquet\n",
            " Patched: Episode_000001.parquet\n",
            " Patched: Episode_000029.parquet\n",
            " Patched: Episode_000009.parquet\n",
            "\n",
            "All parquet files patched: string arrays converted to real arrays.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "extract_path = '/content/Kuka_Kr60ha_DataLogs'  # adjust as needed\n",
        "\n",
        "json_files = []\n",
        "jsonl_files = []\n",
        "\n",
        "# Collect .json and .jsonl files\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.json'):\n",
        "            json_files.append(os.path.join(root, file))\n",
        "        elif file.endswith('.jsonl'):\n",
        "            jsonl_files.append(os.path.join(root, file))\n",
        "\n",
        "print(\"\\n Found JSON files:\")\n",
        "for jf in json_files:\n",
        "    print(f\"→ {jf}\")\n",
        "    with open(jf, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"\\n Preview of {jf}:\")\n",
        "    from pprint import pprint\n",
        "    pprint(data)\n",
        "    print(\"\\n---\\n\")\n",
        "\n",
        "print(\"\\n Found JSONL files:\")\n",
        "for jf in jsonl_files:\n",
        "    print(f\"→ {jf}\")\n",
        "    with open(jf, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"\\n Preview of first 2 entries in {jf}:\")\n",
        "    for line in lines[:2]:  # preview first 2 lines\n",
        "        data = json.loads(line)\n",
        "        pprint(data)\n",
        "    print(\"\\n---\\n\")\n"
      ],
      "metadata": {
        "id": "hB-duNGxLcFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c194c0f9-f07f-499a-c4b4-cf7d2902e972"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Found JSON files:\n",
            "→ /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/info.json\n",
            "\n",
            " Preview of /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/info.json:\n",
            "{'chunks_size': 1000,\n",
            " 'codebase_version': 'v2.0',\n",
            " 'data_path': 'data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet',\n",
            " 'features': {'action': {'dtype': 'float32',\n",
            "                         'names': ['joint_1',\n",
            "                                   'joint_2',\n",
            "                                   'joint_3',\n",
            "                                   'joint_4',\n",
            "                                   'joint_5',\n",
            "                                   'joint_6',\n",
            "                                   'joint_7'],\n",
            "                         'shape': [7]},\n",
            "              'episode_index': {'dtype': 'int64', 'names': None, 'shape': [1]},\n",
            "              'frame_index': {'dtype': 'int64', 'names': None, 'shape': [1]},\n",
            "              'index': {'dtype': 'int64', 'names': None, 'shape': [1]},\n",
            "              'observation.images.webcam': {'dtype': 'video',\n",
            "                                            'has_audio': False,\n",
            "                                            'names': ['height',\n",
            "                                                      'width',\n",
            "                                                      'channels'],\n",
            "                                            'shape': [480, 640, 3],\n",
            "                                            'video.channels': 3,\n",
            "                                            'video.codec': 'mp4v',\n",
            "                                            'video.fps': 20.0,\n",
            "                                            'video.height': 480,\n",
            "                                            'video.is_depth_map': False,\n",
            "                                            'video.pix_fmt': 'bgr24',\n",
            "                                            'video.width': 640},\n",
            "              'observation.state': {'dtype': 'float32',\n",
            "                                    'names': ['joint_1',\n",
            "                                              'joint_2',\n",
            "                                              'joint_3',\n",
            "                                              'joint_4',\n",
            "                                              'joint_5',\n",
            "                                              'joint_6',\n",
            "                                              'joint_7'],\n",
            "                                    'shape': [7]},\n",
            "              'task_index': {'dtype': 'int64', 'names': None, 'shape': [1]},\n",
            "              'timestamp': {'dtype': 'float32', 'names': None, 'shape': [1]}},\n",
            " 'fps': 20,\n",
            " 'robot_type': 'KUKA_KR60HA',\n",
            " 'splits': {'train': '0:31'},\n",
            " 'total_chunks': 1,\n",
            " 'total_episodes': 31,\n",
            " 'total_frames': 28072,\n",
            " 'total_tasks': 1,\n",
            " 'total_videos': 31,\n",
            " 'video_path': 'videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4'}\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            " Found JSONL files:\n",
            "→ /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes.jsonl\n",
            "\n",
            " Preview of first 2 entries in /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes.jsonl:\n",
            "{'episode_index': 0,\n",
            " 'length': 762,\n",
            " 'tasks': ['Pick up the boxes and place them in the container.']}\n",
            "{'episode_index': 1,\n",
            " 'length': 822,\n",
            " 'tasks': ['Pick up the boxes and place them in the container.']}\n",
            "\n",
            "---\n",
            "\n",
            "→ /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes_stats.jsonl\n",
            "\n",
            " Preview of first 2 entries in /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes_stats.jsonl:\n",
            "{'episode_index': 0,\n",
            " 'stats': {'action': {'count': [762],\n",
            "                      'max': [22.7, 14.6, 104.34, 0.94, 64.76, 126.68, 2300.0],\n",
            "                      'mean': [3.9249176771653715,\n",
            "                               -62.87680203149566,\n",
            "                               88.26254254199479,\n",
            "                               0.02397981758530184,\n",
            "                               17.695693111548604,\n",
            "                               85.71676300525012,\n",
            "                               924.250986178477],\n",
            "                      'min': [-19.34, -90.0, 12.66, -0.0, 0.0, 0.0, -425.6],\n",
            "                      'std': [4.449372937919018,\n",
            "                              19.071139099619536,\n",
            "                              10.170490793305593,\n",
            "                              0.13787233474216892,\n",
            "                              7.959137148240646,\n",
            "                              27.33588274055479,\n",
            "                              681.6521336548695]},\n",
            "           'episode_index': {'count': [762],\n",
            "                             'max': [0],\n",
            "                             'mean': [0.0],\n",
            "                             'min': [0],\n",
            "                             'std': [0.0]},\n",
            "           'frame_index': {'count': [762],\n",
            "                           'max': [761],\n",
            "                           'mean': [380.5],\n",
            "                           'min': [0],\n",
            "                           'std': [220.11474280474718]},\n",
            "           'index': {'count': [762],\n",
            "                     'max': [761],\n",
            "                     'mean': [380.5],\n",
            "                     'min': [0],\n",
            "                     'std': [220.11474280474718]},\n",
            "           'observation.state': {'count': [762],\n",
            "                                 'max': [22.7,\n",
            "                                         14.6,\n",
            "                                         104.34,\n",
            "                                         0.94,\n",
            "                                         64.76,\n",
            "                                         126.68,\n",
            "                                         2300.0],\n",
            "                                 'mean': [3.920376994750673,\n",
            "                                          -62.90581777952715,\n",
            "                                          88.26230632152236,\n",
            "                                          0.02397981758530184,\n",
            "                                          17.67184796719165,\n",
            "                                          85.59484699475144,\n",
            "                                          924.0763142624664],\n",
            "                                 'min': [-19.34,\n",
            "                                         -90.0,\n",
            "                                         12.66,\n",
            "                                         -0.0,\n",
            "                                         0.0,\n",
            "                                         0.0,\n",
            "                                         -425.6],\n",
            "                                 'std': [4.45161002010353,\n",
            "                                         19.095548334901345,\n",
            "                                         10.170448345829492,\n",
            "                                         0.13787233474216892,\n",
            "                                         7.984857042810058,\n",
            "                                         27.510182039989427,\n",
            "                                         681.5491382225426]},\n",
            "           'task_index': {'count': [762],\n",
            "                          'max': [0],\n",
            "                          'mean': [0.0],\n",
            "                          'min': [0],\n",
            "                          'std': [0.0]},\n",
            "           'timestamp': {'count': [762],\n",
            "                         'max': [38.05],\n",
            "                         'mean': [19.025],\n",
            "                         'min': [0.0],\n",
            "                         'std': [11.00573714023736]}}}\n",
            "{'episode_index': 1,\n",
            " 'stats': {'action': {'count': [822],\n",
            "                      'max': [22.445118,\n",
            "                              3.513796,\n",
            "                              104.34,\n",
            "                              1.84264,\n",
            "                              68.795393,\n",
            "                              117.551766,\n",
            "                              2300.0],\n",
            "                      'mean': [5.001543588807793,\n",
            "                               -54.13749974087585,\n",
            "                               78.03115511435527,\n",
            "                               0.7261336703163033,\n",
            "                               37.2922579854014,\n",
            "                               81.67771817031633,\n",
            "                               879.2070950924565],\n",
            "                      'min': [-18.622665,\n",
            "                              -90.0,\n",
            "                              35.411095,\n",
            "                              -0.022694,\n",
            "                              0.0,\n",
            "                              -247.233992,\n",
            "                              -425.6],\n",
            "                      'std': [4.608308846250208,\n",
            "                              18.729258203491206,\n",
            "                              12.214286771070041,\n",
            "                              0.6164177959685433,\n",
            "                              19.24365195688939,\n",
            "                              47.670975042584324,\n",
            "                              698.5543571292603]},\n",
            "           'episode_index': {'count': [822],\n",
            "                             'max': [1],\n",
            "                             'mean': [1.0],\n",
            "                             'min': [1],\n",
            "                             'std': [0.0]},\n",
            "           'frame_index': {'count': [822],\n",
            "                           'max': [821],\n",
            "                           'mean': [410.5],\n",
            "                           'min': [0],\n",
            "                           'std': [237.43525433262855]},\n",
            "           'index': {'count': [822],\n",
            "                     'max': [1583],\n",
            "                     'mean': [1172.5],\n",
            "                     'min': [762],\n",
            "                     'std': [237.43525433262855]},\n",
            "           'observation.state': {'count': [822],\n",
            "                                 'max': [22.445118,\n",
            "                                         3.513796,\n",
            "                                         104.34,\n",
            "                                         1.84264,\n",
            "                                         68.795393,\n",
            "                                         117.551766,\n",
            "                                         2300.0],\n",
            "                                 'mean': [4.9973343430657,\n",
            "                                          -54.16439755109483,\n",
            "                                          78.03093613625306,\n",
            "                                          0.7261336703163033,\n",
            "                                          37.27015336253035,\n",
            "                                          81.56470113868616,\n",
            "                                          878.4397958223834],\n",
            "                                 'min': [-18.622665,\n",
            "                                         -90.0,\n",
            "                                         35.411095,\n",
            "                                         -0.022694,\n",
            "                                         0.0,\n",
            "                                         -247.233992,\n",
            "                                         -425.6],\n",
            "                                 'std': [4.611294196483308,\n",
            "                                         18.764832112992718,\n",
            "                                         12.214070575858388,\n",
            "                                         0.6164177959685433,\n",
            "                                         19.276012907848195,\n",
            "                                         47.754285178805446,\n",
            "                                         698.3363583670557]},\n",
            "           'task_index': {'count': [822],\n",
            "                          'max': [0],\n",
            "                          'mean': [0.0],\n",
            "                          'min': [0],\n",
            "                          'std': [0.0]},\n",
            "           'timestamp': {'count': [822],\n",
            "                         'max': [41.05],\n",
            "                         'mean': [20.525000000000002],\n",
            "                         'min': [0.0],\n",
            "                         'std': [11.871762716631427]}}}\n",
            "\n",
            "---\n",
            "\n",
            "→ /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/tasks.jsonl\n",
            "\n",
            " Preview of first 2 entries in /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/tasks.jsonl:\n",
            "{'task': 'Pick up the boxes and place them in the container.', 'task_index': 0}\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pvdPMXRtlhy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_array_string(s):\n",
        "    if isinstance(s, str):\n",
        "        s = s.strip('[]')\n",
        "        return np.array([float(x) for x in s.strip().split() if x], dtype=np.float32)\n",
        "    return np.array(s, dtype=np.float32)\n",
        "\n",
        "for file_name in os.listdir(base_dir):\n",
        "    if file_name.endswith('.parquet'):\n",
        "        parquet_path = os.path.join(base_dir, file_name)\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "\n",
        "        df['observation.state'] = df['observation.state'].apply(parse_array_string)\n",
        "        df['action'] = df['action'].apply(parse_array_string)\n",
        "        state_vector = df['observation.state'].iloc[0]\n",
        "        action_vector = df['action'].iloc[0]\n",
        "\n",
        "        print(f\"\\n File: {file_name}\")\n",
        "        print(f\"State vector:\\n{state_vector}\")\n",
        "        print(f\"State length: {len(state_vector)}\")\n",
        "        print(f\"Action vector:\\n{action_vector}\")"
      ],
      "metadata": {
        "id": "0JdDBkrdtLRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e65ab4-a96f-4869-8db6-f898ad4a7cc6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " File: Episode_000021.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.  -0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.  -0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000010.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.  -0.  -0.   0.  -0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.  -0.  -0.   0.  -0.]\n",
            "\n",
            " File: Episode_000014.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000004.parquet\n",
            "State vector:\n",
            "[   3.46  -67.89   90.18   -0.     18.17   92.9  1392.62]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[   3.46  -67.89   90.18   -0.     18.17   92.9  1392.62]\n",
            "\n",
            " File: Episode_000019.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000002.parquet\n",
            "State vector:\n",
            "[  0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[  0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000008.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000028.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.  -0.   0.  -0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.  -0.   0.  -0.   0.]\n",
            "\n",
            " File: Episode_000024.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.  -0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.  -0.   0.   0.]\n",
            "\n",
            " File: Episode_000013.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.  -0.  -0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.  -0.  -0.]\n",
            "\n",
            " File: Episode_000017.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.  -0.  -0.  -0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.  -0.  -0.  -0.   0.]\n",
            "\n",
            " File: Episode_000023.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000020.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.  -0.  -0.   0.  -0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.  -0.  -0.   0.  -0.]\n",
            "\n",
            " File: Episode_000000.parquet\n",
            "State vector:\n",
            "[   0.    -90.     90.      0.      0.      0.   1259.52]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[   0.    -90.     90.      0.      0.      0.   1259.52]\n",
            "\n",
            " File: Episode_000007.parquet\n",
            "State vector:\n",
            "[  5.52 -49.    63.27   1.84  68.79  93.51 535.71]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[  5.52 -49.    63.27   1.84  68.79  93.51 535.71]\n",
            "\n",
            " File: Episode_000012.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000006.parquet\n",
            "State vector:\n",
            "[  0.   -89.99  89.99   0.     0.     0.   694.05]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[  0.   -89.99  89.99   0.     0.     0.   694.05]\n",
            "\n",
            " File: Episode_000025.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.  -0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.  -0.   0.   0.]\n",
            "\n",
            " File: Episode_000018.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000015.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000026.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.  -0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.  -0.   0.   0.]\n",
            "\n",
            " File: Episode_000027.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000003.parquet\n",
            "State vector:\n",
            "[  0. -90.  90.   0.  -0.  -0.  -0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[  0. -90.  90.   0.  -0.  -0.  -0.]\n",
            "\n",
            " File: Episode_000016.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000030.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000005.parquet\n",
            "State vector:\n",
            "[   5.34  -50.6    65.55    1.68   64.51   93.46 -572.62]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[   5.34  -50.6    65.55    1.68   64.51   93.46 -572.62]\n",
            "\n",
            " File: Episode_000022.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.  -0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.  -0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000011.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000001.parquet\n",
            "State vector:\n",
            "[  0.  -90.   90.    0.    0.    0.  761.9]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[  0.  -90.   90.    0.    0.    0.  761.9]\n",
            "\n",
            " File: Episode_000029.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.  -0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.  -0.   0.   0.   0.]\n",
            "\n",
            " File: Episode_000009.parquet\n",
            "State vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n",
            "State length: 7\n",
            "Action vector:\n",
            "[ -0. -90.  90.   0.   0.   0.   0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-2KuyLOliZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PATCH 2- Although modality includes prompts but i commented now"
      ],
      "metadata": {
        "id": "ORkW0e3xnbQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the new modality\n",
        "\n",
        "{\n",
        "  \"state\": {\n",
        "    \"single_arm\": {\n",
        "      \"start\": 0,\n",
        "      \"end\": 7\n",
        "    }\n",
        "  },\n",
        "  \"action\": {\n",
        "    \"single_arm\": {\n",
        "      \"start\": 0,\n",
        "      \"end\": 7\n",
        "    }\n",
        "  },\n",
        "  \"video\": {\n",
        "    \"webcam\": {\n",
        "      \"original_key\": \"video.webcam\",#observation.images.webcam\",\n",
        "      \"shape\": [480, 640, 3],\n",
        "      \"names\": [\"height\", \"width\", \"channels\"],\n",
        "      \"info\": {\n",
        "        \"video.fps\": 20.0,\n",
        "        \"video.height\": 480,\n",
        "        \"video.width\": 640,\n",
        "        \"video.channels\": 3,\n",
        "        \"video.codec\": \"mp4v\",\n",
        "        \"video.pix_fmt\": \"bgr24\",\n",
        "        \"video.is_depth_map\": False,\n",
        "        \"has_audio\": False\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "#  \"annotation\": {\n",
        "#   \"human.task_description\": {},\n",
        " #  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "Ol17IviFvLA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab63780d-3aeb-4bd3-e493-237ea03124ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'state': {'single_arm': {'start': 0, 'end': 7}},\n",
              " 'action': {'single_arm': {'start': 0, 'end': 7}},\n",
              " 'video': {'webcam': {'original_key': 'video.webcam',\n",
              "   'shape': [480, 640, 3],\n",
              "   'names': ['height', 'width', 'channels'],\n",
              "   'info': {'video.fps': 20.0,\n",
              "    'video.height': 480,\n",
              "    'video.width': 640,\n",
              "    'video.channels': 3,\n",
              "    'video.codec': 'mp4v',\n",
              "    'video.pix_fmt': 'bgr24',\n",
              "    'video.is_depth_map': False,\n",
              "    'has_audio': False}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iVkcJZ3oljJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PATCH 3- Json had empty lines and ensured they are removed"
      ],
      "metadata": {
        "id": "oP04BepwnnSH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HDDWfLMSn3YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "extract_path = '/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset'  # update to your base folder\n",
        "jsonl_files = []\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.jsonl'):\n",
        "            jsonl_files.append(os.path.join(root, file))\n",
        "for jsonl_file in jsonl_files:\n",
        "    with open(jsonl_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    empty_lines = [i for i, line in enumerate(lines) if not line.strip()]\n",
        "\n",
        "    if empty_lines:\n",
        "        print(f\"\\n{jsonl_file}\")\n",
        "        print(f\"   Found {len(empty_lines)} empty lines at positions: {empty_lines}\")\n",
        "\n",
        "        # Optional: Clean the file (remove blank lines)\n",
        "        cleaned_lines = [line for line in lines if line.strip()]\n",
        "        with open(jsonl_file, \"w\") as f:\n",
        "            f.writelines(cleaned_lines)\n",
        "        print(\"Cleaned: removed blank lines.\")\n",
        "    else:\n",
        "        print(f\"\\n {jsonl_file}\")\n",
        "        print(\"   No empty lines found.\")\n"
      ],
      "metadata": {
        "id": "sR2NcaobwpO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa655f28-ac54-42c8-ee80-ed68b668d58f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes.jsonl\n",
            "   No empty lines found.\n",
            "\n",
            " /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes_stats.jsonl\n",
            "   No empty lines found.\n",
            "\n",
            " /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/tasks.jsonl\n",
            "   No empty lines found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfxBmE0Xljy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "jsonl_files = []\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.jsonl'):\n",
        "            jsonl_files.append(os.path.join(root, file))\n",
        "\n",
        "for jsonl_file in jsonl_files:\n",
        "    with open(jsonl_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    total_lines = len(lines)\n",
        "    print(f\"\\nFile: {jsonl_file}\")\n",
        "    print(f\"Total lines: {total_lines}\")\n",
        "\n",
        "    # Preview first 5 non-empty lines\n",
        "    non_empty_preview = [line.strip() for line in lines if line.strip()][:5]\n",
        "    for idx, line in enumerate(non_empty_preview):\n",
        "        print(f\"Line {idx+1}: {line}\")\n"
      ],
      "metadata": {
        "id": "7ZWgSjZBwpRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbd9067-6015-4816-8d72-1cdcebe9554c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "File: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes.jsonl\n",
            "Total lines: 31\n",
            "Line 1: {\"episode_index\": 0, \"tasks\": [\"Pick up the boxes and place them in the container.\"], \"length\": 762}\n",
            "Line 2: {\"episode_index\": 1, \"tasks\": [\"Pick up the boxes and place them in the container.\"], \"length\": 822}\n",
            "Line 3: {\"episode_index\": 2, \"tasks\": [\"Pick up the boxes and place them in the container.\"], \"length\": 839}\n",
            "Line 4: {\"episode_index\": 3, \"tasks\": [\"Pick up the boxes and place them in the container.\"], \"length\": 982}\n",
            "Line 5: {\"episode_index\": 4, \"tasks\": [\"Pick up the boxes and place them in the container.\"], \"length\": 971}\n",
            "\n",
            "File: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/episodes_stats.jsonl\n",
            "Total lines: 31\n",
            "Line 1: {\"episode_index\": 0, \"stats\": {\"action\": {\"min\": [-19.34, -90.0, 12.66, -0.0, 0.0, 0.0, -425.6], \"max\": [22.7, 14.6, 104.34, 0.94, 64.76, 126.68, 2300.0], \"mean\": [3.9249176771653715, -62.87680203149566, 88.26254254199479, 0.02397981758530184, 17.695693111548604, 85.71676300525012, 924.250986178477], \"std\": [4.449372937919018, 19.071139099619536, 10.170490793305593, 0.13787233474216892, 7.959137148240646, 27.33588274055479, 681.6521336548695], \"count\": [762]}, \"observation.state\": {\"min\": [-19.34, -90.0, 12.66, -0.0, 0.0, 0.0, -425.6], \"max\": [22.7, 14.6, 104.34, 0.94, 64.76, 126.68, 2300.0], \"mean\": [3.920376994750673, -62.90581777952715, 88.26230632152236, 0.02397981758530184, 17.67184796719165, 85.59484699475144, 924.0763142624664], \"std\": [4.45161002010353, 19.095548334901345, 10.170448345829492, 0.13787233474216892, 7.984857042810058, 27.510182039989427, 681.5491382225426], \"count\": [762]}, \"timestamp\": {\"min\": [0.0], \"max\": [38.05], \"mean\": [19.025], \"std\": [11.00573714023736], \"count\": [762]}, \"frame_index\": {\"min\": [0], \"max\": [761], \"mean\": [380.5], \"std\": [220.11474280474718], \"count\": [762]}, \"episode_index\": {\"min\": [0], \"max\": [0], \"mean\": [0.0], \"std\": [0.0], \"count\": [762]}, \"index\": {\"min\": [0], \"max\": [761], \"mean\": [380.5], \"std\": [220.11474280474718], \"count\": [762]}, \"task_index\": {\"min\": [0], \"max\": [0], \"mean\": [0.0], \"std\": [0.0], \"count\": [762]}}}\n",
            "Line 2: {\"episode_index\": 1, \"stats\": {\"action\": {\"min\": [-18.622665, -90.0, 35.411095, -0.022694, 0.0, -247.233992, -425.6], \"max\": [22.445118, 3.513796, 104.34, 1.84264, 68.795393, 117.551766, 2300.0], \"mean\": [5.001543588807793, -54.13749974087585, 78.03115511435527, 0.7261336703163033, 37.2922579854014, 81.67771817031633, 879.2070950924565], \"std\": [4.608308846250208, 18.729258203491206, 12.214286771070041, 0.6164177959685433, 19.24365195688939, 47.670975042584324, 698.5543571292603], \"count\": [822]}, \"observation.state\": {\"min\": [-18.622665, -90.0, 35.411095, -0.022694, 0.0, -247.233992, -425.6], \"max\": [22.445118, 3.513796, 104.34, 1.84264, 68.795393, 117.551766, 2300.0], \"mean\": [4.9973343430657, -54.16439755109483, 78.03093613625306, 0.7261336703163033, 37.27015336253035, 81.56470113868616, 878.4397958223834], \"std\": [4.611294196483308, 18.764832112992718, 12.214070575858388, 0.6164177959685433, 19.276012907848195, 47.754285178805446, 698.3363583670557], \"count\": [822]}, \"timestamp\": {\"min\": [0.0], \"max\": [41.05], \"mean\": [20.525000000000002], \"std\": [11.871762716631427], \"count\": [822]}, \"frame_index\": {\"min\": [0], \"max\": [821], \"mean\": [410.5], \"std\": [237.43525433262855], \"count\": [822]}, \"episode_index\": {\"min\": [1], \"max\": [1], \"mean\": [1.0], \"std\": [0.0], \"count\": [822]}, \"index\": {\"min\": [762], \"max\": [1583], \"mean\": [1172.5], \"std\": [237.43525433262855], \"count\": [822]}, \"task_index\": {\"min\": [0], \"max\": [0], \"mean\": [0.0], \"std\": [0.0], \"count\": [822]}}}\n",
            "Line 3: {\"episode_index\": 2, \"stats\": {\"action\": {\"min\": [-22.316499, -90.0, 56.146769, -2.637591, -0.0, -75.798745, -1149.4], \"max\": [29.918292, -5.453003, 104.58946, 1.84264, 68.795393, 214.726203, 2300.0], \"mean\": [3.7428558736591184, -56.241345646007154, 79.53953501907023, 0.6560051787842672, 34.89201412753276, 84.79421446841484, 755.0802530834324], \"std\": [3.766036620711936, 19.07238764224125, 10.638768625438411, 0.771554904319078, 20.344766942412548, 40.48433325464206, 816.3826232403837], \"count\": [839]}, \"observation.state\": {\"min\": [-22.316499, -90.0, 56.146769, -2.637591, -0.0, -75.798745, -1149.4], \"max\": [29.918292, -5.453003, 104.58946, 1.84264, 68.795393, 214.726203, 2300.0], \"mean\": [3.7428558736591184, -56.241345646007154, 79.53953501907023, 0.6560051787842672, 34.89201412753276, 84.79421446841484, 755.0802530834324], \"std\": [3.766036620711936, 19.072387642241246, 10.638768625438411, 0.771554904319078, 20.344766942412544, 40.48433325464206, 816.3826232403839], \"count\": [839]}, \"timestamp\": {\"min\": [0.0], \"max\": [41.9], \"mean\": [20.95], \"std\": [12.117136625457354], \"count\": [839]}, \"frame_index\": {\"min\": [0], \"max\": [838], \"mean\": [419.0], \"std\": [242.34273250914705], \"count\": [839]}, \"episode_index\": {\"min\": [2], \"max\": [2], \"mean\": [2.0], \"std\": [0.0], \"count\": [839]}, \"index\": {\"min\": [1584], \"max\": [2422], \"mean\": [2003.0], \"std\": [242.34273250914705], \"count\": [839]}, \"task_index\": {\"min\": [0], \"max\": [0], \"mean\": [0.0], \"std\": [0.0], \"count\": [839]}}}\n",
            "Line 4: {\"episode_index\": 3, \"stats\": {\"action\": {\"min\": [-22.316499, -90.0, 61.415117, -2.637591, -36.013728, 0.0, -923.21], \"max\": [24.217106, -7.669729, 104.58946, 180.166493, 68.795393, 263.008536, 2300.0], \"mean\": [3.9008943716904296, -55.2322090539715, 78.67871798778003, 5.85159663238289, 34.11918454175151, 89.11607058961306, 737.4632369378829], \"std\": [4.021973153111676, 18.989839009435713, 10.456784292174863, 25.499469206040256, 21.67666405475756, 42.5098358398499, 787.7984858681901], \"count\": [982]}, \"observation.state\": {\"min\": [-22.316499, -90.0, 61.415117, -2.637591, -36.013728, 0.0, -923.21], \"max\": [24.217106, -7.669729, 104.58946, 180.166493, 68.795393, 263.008536, 2300.0], \"mean\": [3.9008943716904296, -55.2322090539715, 78.67871798778005, 5.85159663238289, 34.11918454175151, 89.11607058961306, 737.4632369378829], \"std\": [4.021973153111676, 18.989839009435713, 10.45678429217486, 25.499469206040256, 21.67666405475756, 42.5098358398499, 787.7984858681901], \"count\": [982]}, \"timestamp\": {\"min\": [0.0], \"max\": [49.05], \"mean\": [24.525000000000002], \"std\": [14.181164150614247], \"count\": [982]}, \"frame_index\": {\"min\": [0], \"max\": [981], \"mean\": [490.5], \"std\": [283.6232830122849], \"count\": [982]}, \"episode_index\": {\"min\": [3], \"max\": [3], \"mean\": [3.0], \"std\": [0.0], \"count\": [982]}, \"index\": {\"min\": [2423], \"max\": [3404], \"mean\": [2913.5], \"std\": [283.6232830122849], \"count\": [982]}, \"task_index\": {\"min\": [0], \"max\": [0], \"mean\": [0.0], \"std\": [0.0], \"count\": [982]}}}\n",
            "Line 5: {\"episode_index\": 4, \"stats\": {\"action\": {\"min\": [-21.770134, -90.0, 43.996625, -0.673478, -0.0, 0.0, -1002.38], \"max\": [22.7, -0.34783, 104.34, 0.94, 48.288972, 111.447606, 2300.0], \"mean\": [4.007763674562307, -60.39210050051493, 87.711915192585, -0.000486582904222451, 19.31808517095778, 91.449628246138, 862.1237440823892], \"std\": [3.773037504461645, 18.00831563659269, 9.001574143679242, 0.1801668388262427, 5.920569662113966, 12.634314529170233, 756.0017393413461], \"count\": [971]}, \"observation.state\": {\"min\": [-21.770134, -90.0, 43.996625, -0.673478, -0.0, 0.0, -1002.38], \"max\": [22.7, -0.34783, 104.34, 0.94, 48.288972, 111.447606, 2300.0], \"mean\": [4.007763674562307, -60.39210050051493, 87.711915192585, -0.00048658290422244914, 19.31808517095778, 91.449628246138, 862.1237440823892], \"std\": [3.773037504461645, 18.00831563659269, 9.001574143679242, 0.18016683882624274, 5.920569662113966, 12.634314529170231, 756.0017393413461], \"count\": [971]}, \"timestamp\": {\"min\": [0.0], \"max\": [48.5], \"mean\": [24.25], \"std\": [14.022392805794595], \"count\": [971]}, \"frame_index\": {\"min\": [0.0], \"max\": [970.0], \"mean\": [485.0], \"std\": [280.4478561158919], \"count\": [971]}, \"episode_index\": {\"min\": [4], \"max\": [4], \"mean\": [4.0], \"std\": [0.0], \"count\": [971]}, \"index\": {\"min\": [3405.0], \"max\": [4375.0], \"mean\": [3890.0], \"std\": [280.4478561158919], \"count\": [971]}, \"task_index\": {\"min\": [0.0], \"max\": [0.0], \"mean\": [0.0], \"std\": [0.0], \"count\": [971]}}}\n",
            "\n",
            "File: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/tasks.jsonl\n",
            "Total lines: 1\n",
            "Line 1: {\"task_index\": 0, \"task\": \"Pick up the boxes and place them in the container.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlMSRssXlkOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PATCH 4 . modifying the JSON file"
      ],
      "metadata": {
        "id": "0ciI3kkQoB95"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkvRBxTSUsy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "meta_dir = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta\"\n",
        "\n",
        "# Walk through meta directory and find all info.json files\n",
        "for root, dirs, files in os.walk(meta_dir):\n",
        "    for file in files:\n",
        "        if file == \"info.json\":\n",
        "            info_json_path = os.path.join(root, file)\n",
        "\n",
        "            # Load info.json\n",
        "            with open(info_json_path, \"r\") as f:\n",
        "                info_data = json.load(f)\n",
        "\n",
        "            # Navigate to webcam metadata\n",
        "            webcam_meta = info_data.get(\"features\", {}).get(\"observation.images.webcam\")\n",
        "            if webcam_meta is None:\n",
        "                print(f\" Skipping {info_json_path}: 'observation.images.webcam' section not found.\")\n",
        "                continue\n",
        "\n",
        "            # Extract video.* and has_audio keys\n",
        "            info_block = {}\n",
        "            for key in list(webcam_meta.keys()):\n",
        "                if key.startswith(\"video.\") or key == \"has_audio\":\n",
        "                    info_block[key] = webcam_meta.pop(key)\n",
        "\n",
        "            # Insert the extracted info under 'info'\n",
        "            webcam_meta[\"info\"] = info_block\n",
        "\n",
        "            # Save the updated info.json back to disk\n",
        "            with open(info_json_path, \"w\") as f:\n",
        "                json.dump(info_data, f, indent=4)\n",
        "\n",
        "            print(f\" Patched: {info_json_path}\")\n"
      ],
      "metadata": {
        "id": "GKl2v1fPyQ6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3c109f-5925-49b6-8003-c395e7b077fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Patched: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5tKGbobAlk5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## this patch may not be needed in the future"
      ],
      "metadata": {
        "id": "4SJTccqAU72O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "meta_dir = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta\"\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(meta_dir):\n",
        "    for file in files:\n",
        "        if file == \"info.json\":\n",
        "            info_json_path = os.path.join(root, file)\n",
        "\n",
        "            # Load info.json\n",
        "            with open(info_json_path, \"r\") as f:\n",
        "                info_data = json.load(f)\n",
        "\n",
        "            features = info_data.get(\"features\", {})\n",
        "\n",
        "            # Check if observation.images.webcam exists\n",
        "            if \"observation.images.webcam\" in features:\n",
        "                # Rename the key to video.webcam\n",
        "                features[\"video.webcam\"] = features.pop(\"observation.images.webcam\")\n",
        "                print(f\" Patched key: observation.images.webcam → video.webcam in {info_json_path}\")\n",
        "            else:\n",
        "                print(f\" Skipping {info_json_path}: 'observation.images.webcam' key not found.\")\n",
        "\n",
        "            # Save the updated info.json\n",
        "            with open(info_json_path, \"w\") as f:\n",
        "                json.dump(info_data, f, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJkDwtlnULpw",
        "outputId": "83b107ab-6816-4ece-d054-2943d2dc8cf2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Patched key: observation.images.webcam → video.webcam in /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4F5QZQ1yllYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNUltDqBXCJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Checking info.json ---\")\n",
        "with open(info_json_path, \"r\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "OY2kvy4qoQQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4992ee6c-c75a-42f8-c75a-1d6ab4308f61"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Checking info.json ---\n",
            "{\n",
            "    \"codebase_version\": \"v2.0\",\n",
            "    \"robot_type\": \"KUKA_KR60HA\",\n",
            "    \"total_episodes\": 31,\n",
            "    \"total_frames\": 28072,\n",
            "    \"total_tasks\": 1,\n",
            "    \"total_videos\": 31,\n",
            "    \"total_chunks\": 1,\n",
            "    \"chunks_size\": 1000,\n",
            "    \"fps\": 20,\n",
            "    \"splits\": {\n",
            "        \"train\": \"0:31\"\n",
            "    },\n",
            "    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n",
            "    \"video_path\": \"videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4\",\n",
            "    \"features\": {\n",
            "        \"action\": {\n",
            "            \"dtype\": \"float32\",\n",
            "            \"shape\": [\n",
            "                7\n",
            "            ],\n",
            "            \"names\": [\n",
            "                \"joint_1\",\n",
            "                \"joint_2\",\n",
            "                \"joint_3\",\n",
            "                \"joint_4\",\n",
            "                \"joint_5\",\n",
            "                \"joint_6\",\n",
            "                \"joint_7\"\n",
            "            ]\n",
            "        },\n",
            "        \"observation.state\": {\n",
            "            \"dtype\": \"float32\",\n",
            "            \"shape\": [\n",
            "                7\n",
            "            ],\n",
            "            \"names\": [\n",
            "                \"joint_1\",\n",
            "                \"joint_2\",\n",
            "                \"joint_3\",\n",
            "                \"joint_4\",\n",
            "                \"joint_5\",\n",
            "                \"joint_6\",\n",
            "                \"joint_7\"\n",
            "            ]\n",
            "        },\n",
            "        \"timestamp\": {\n",
            "            \"dtype\": \"float32\",\n",
            "            \"shape\": [\n",
            "                1\n",
            "            ],\n",
            "            \"names\": null\n",
            "        },\n",
            "        \"frame_index\": {\n",
            "            \"dtype\": \"int64\",\n",
            "            \"shape\": [\n",
            "                1\n",
            "            ],\n",
            "            \"names\": null\n",
            "        },\n",
            "        \"episode_index\": {\n",
            "            \"dtype\": \"int64\",\n",
            "            \"shape\": [\n",
            "                1\n",
            "            ],\n",
            "            \"names\": null\n",
            "        },\n",
            "        \"index\": {\n",
            "            \"dtype\": \"int64\",\n",
            "            \"shape\": [\n",
            "                1\n",
            "            ],\n",
            "            \"names\": null\n",
            "        },\n",
            "        \"task_index\": {\n",
            "            \"dtype\": \"int64\",\n",
            "            \"shape\": [\n",
            "                1\n",
            "            ],\n",
            "            \"names\": null\n",
            "        },\n",
            "        \"video.webcam\": {\n",
            "            \"dtype\": \"video\",\n",
            "            \"shape\": [\n",
            "                480,\n",
            "                640,\n",
            "                3\n",
            "            ],\n",
            "            \"names\": [\n",
            "                \"height\",\n",
            "                \"width\",\n",
            "                \"channels\"\n",
            "            ],\n",
            "            \"info\": {\n",
            "                \"video.fps\": 20.0,\n",
            "                \"video.height\": 480,\n",
            "                \"video.width\": 640,\n",
            "                \"video.channels\": 3,\n",
            "                \"video.codec\": \"mp4v\",\n",
            "                \"video.pix_fmt\": \"bgr24\",\n",
            "                \"video.is_depth_map\": false,\n",
            "                \"has_audio\": false\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVa5Luibll4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJKHAYcNUOYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to your parquet files\n",
        "data_dir = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/data/chunk-000\"\n",
        "\n",
        "# Rename files\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.startswith(\"Episode_\") and filename.endswith(\".parquet\"):\n",
        "        old_path = os.path.join(data_dir, filename)\n",
        "        new_filename = filename.lower()\n",
        "        new_path = os.path.join(data_dir, new_filename)\n",
        "        os.rename(old_path, new_path)\n",
        "        print(f\"Renamed {filename} → {new_filename}\")\n",
        "\n",
        "print(\"All files renamed to lowercase.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sVjsBEUioTt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2022be-1cc3-417f-a2dd-d73739130bda"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed Episode_000021.parquet → episode_000021.parquet\n",
            "Renamed Episode_000010.parquet → episode_000010.parquet\n",
            "Renamed Episode_000014.parquet → episode_000014.parquet\n",
            "Renamed Episode_000004.parquet → episode_000004.parquet\n",
            "Renamed Episode_000019.parquet → episode_000019.parquet\n",
            "Renamed Episode_000002.parquet → episode_000002.parquet\n",
            "Renamed Episode_000008.parquet → episode_000008.parquet\n",
            "Renamed Episode_000028.parquet → episode_000028.parquet\n",
            "Renamed Episode_000024.parquet → episode_000024.parquet\n",
            "Renamed Episode_000013.parquet → episode_000013.parquet\n",
            "Renamed Episode_000017.parquet → episode_000017.parquet\n",
            "Renamed Episode_000023.parquet → episode_000023.parquet\n",
            "Renamed Episode_000020.parquet → episode_000020.parquet\n",
            "Renamed Episode_000000.parquet → episode_000000.parquet\n",
            "Renamed Episode_000007.parquet → episode_000007.parquet\n",
            "Renamed Episode_000012.parquet → episode_000012.parquet\n",
            "Renamed Episode_000006.parquet → episode_000006.parquet\n",
            "Renamed Episode_000025.parquet → episode_000025.parquet\n",
            "Renamed Episode_000018.parquet → episode_000018.parquet\n",
            "Renamed Episode_000015.parquet → episode_000015.parquet\n",
            "Renamed Episode_000026.parquet → episode_000026.parquet\n",
            "Renamed Episode_000027.parquet → episode_000027.parquet\n",
            "Renamed Episode_000003.parquet → episode_000003.parquet\n",
            "Renamed Episode_000016.parquet → episode_000016.parquet\n",
            "Renamed Episode_000030.parquet → episode_000030.parquet\n",
            "Renamed Episode_000005.parquet → episode_000005.parquet\n",
            "Renamed Episode_000022.parquet → episode_000022.parquet\n",
            "Renamed Episode_000011.parquet → episode_000011.parquet\n",
            "Renamed Episode_000001.parquet → episode_000001.parquet\n",
            "Renamed Episode_000029.parquet → episode_000029.parquet\n",
            "Renamed Episode_000009.parquet → episode_000009.parquet\n",
            "All files renamed to lowercase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ct7NomXlmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning / Training Part"
      ],
      "metadata": {
        "id": "_ksf3-haVSp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "code = \"\"\"\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "data_config = DATA_CONFIG_MAP[\"so100\"]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "print(\" GR00T config loaded successfully.\")\n",
        "print(\"Available modalities:\", modality_config.keys())\n",
        "\"\"\"\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n"
      ],
      "metadata": {
        "id": "JmXBU8O8PtEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f730e77d-e383-42c3-f1b6-b42c9eedfbba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GR00T config loaded successfully.\n",
            "Available modalities: dict_keys(['video', 'state', 'action', 'language'])\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 18:25:10.860375: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 18:25:10.910006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 18:25:10.910062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 18:25:10.911318: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 18:25:10.918602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 18:25:11.857459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nA_YMS40lm2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AxVbM0j0OSaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta\n"
      ],
      "metadata": {
        "id": "o5lXi2g5Z6oT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VL07OwYwcvMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9o8UX_uFUX0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "modality = {\n",
        "    \"state\": {\n",
        "        \"single_arm\": {\n",
        "            \"start\": 0,\n",
        "            \"end\": 7\n",
        "        }\n",
        "    },\n",
        "    \"action\": {\n",
        "        \"single_arm\": {\n",
        "            \"start\": 0,\n",
        "            \"end\": 7\n",
        "        }\n",
        "    },\n",
        "    \"video\": {\n",
        "        \"webcam\": {\n",
        "            \"original_key\": \"video.webcam\",\n",
        "            \"shape\": [480, 640, 3],\n",
        "            \"names\": [\"height\", \"width\", \"channels\"],\n",
        "            \"info\": {\n",
        "                \"video.fps\": 20.0,\n",
        "                \"video.height\": 480,\n",
        "                \"video.width\": 640,\n",
        "                \"video.channels\": 3,\n",
        "                \"video.codec\": \"mp4v\",\n",
        "                \"video.pix_fmt\": \"bgr24\",\n",
        "                \"video.is_depth_map\": False,\n",
        "                \"has_audio\": False\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/modality.json\", \"w\") as f:\n",
        "    json.dump(modality, f, indent=2)\n"
      ],
      "metadata": {
        "id": "OIMV1CHwZ7NT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "info_path = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/info.json\"\n",
        "with open(info_path, \"r\") as f:\n",
        "    info_data = json.load(f)\n",
        "\n",
        "print(\"Keys in features:\", info_data.get(\"features\", {}).keys())\n"
      ],
      "metadata": {
        "id": "Sxvtc74zaVYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d378b4e9-191b-4a71-e103-4ca23b7ba6ae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in features: dict_keys(['action', 'observation.state', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index', 'video.webcam'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbqKVOkoAYXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "info_path = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/info.json\"\n",
        "\n",
        "# Load and patch keys\n",
        "with open(info_path, \"r\") as f:\n",
        "    info_data = json.load(f)\n",
        "\n",
        "features = info_data.get(\"features\", {})\n",
        "\n",
        "# Remap webcam\n",
        "if \"observation.images.webcam\" in features:\n",
        "    features[\"video.webcam\"] = features.pop(\"observation.images.webcam\")\n",
        "\n",
        "# Remap state and action if needed\n",
        "if \"observation.state\" in features:\n",
        "    features[\"state.single_arm\"] = features.pop(\"observation.state\")\n",
        "\n",
        "if \"action\" in features:\n",
        "    features[\"action.single_arm\"] = features.pop(\"action\")\n",
        "\n",
        "# Save back the updated file\n",
        "with open(info_path, \"w\") as f:\n",
        "    json.dump(info_data, f, indent=2)\n",
        "\n",
        "print(\" info.json updated with correct modality keys.\")\n"
      ],
      "metadata": {
        "id": "8zdeCRpEacS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda705ad-ffb4-4d19-c588-d1d51c2f0558"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " info.json updated with correct modality keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImcpioiCAZCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upadte the Data.config File : Adding the Kuka Custom config"
      ],
      "metadata": {
        "id": "hLDCW8TvVv5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Isaac-GR00T/gr00t/experiment/data_config.py\n",
        "# ⬇️ Paste your updated file here with KukaCustomDataConfig and 'kuka_custom' in DATA_CONFIG_MAP\n",
        "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "from gr00t.data.dataset import ModalityConfig\n",
        "from gr00t.data.transform.base import ComposedModalityTransform, ModalityTransform\n",
        "from gr00t.data.transform.concat import ConcatTransform\n",
        "from gr00t.data.transform.state_action import (\n",
        "    StateActionSinCosTransform,\n",
        "    StateActionToTensor,\n",
        "    StateActionTransform,\n",
        ")\n",
        "from gr00t.data.transform.video import (\n",
        "    VideoColorJitter,\n",
        "    VideoCrop,\n",
        "    VideoResize,\n",
        "    VideoToNumpy,\n",
        "    VideoToTensor,\n",
        ")\n",
        "from gr00t.model.transforms import GR00TTransform\n",
        "\n",
        "\n",
        "class BaseDataConfig(ABC):\n",
        "    @abstractmethod\n",
        "    def modality_config(self) -> dict[str, ModalityConfig]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def transform(self) -> ModalityTransform:\n",
        "        pass\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "class Gr1ArmsOnlyDataConfig(BaseDataConfig):\n",
        "    video_keys = [\"video.ego_view\"]\n",
        "    state_keys = [\n",
        "        \"state.left_arm\",\n",
        "        \"state.right_arm\",\n",
        "        \"state.left_hand\",\n",
        "        \"state.right_hand\",\n",
        "    ]\n",
        "    action_keys = [\n",
        "        \"action.left_arm\",\n",
        "        \"action.right_arm\",\n",
        "        \"action.left_hand\",\n",
        "        \"action.right_hand\",\n",
        "    ]\n",
        "    language_keys = [\"annotation.human.action.task_description\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))\n",
        "\n",
        "    def modality_config(self) -> dict[str, ModalityConfig]:\n",
        "        video_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.video_keys,\n",
        "        )\n",
        "\n",
        "        state_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.state_keys,\n",
        "        )\n",
        "\n",
        "        action_modality = ModalityConfig(\n",
        "            delta_indices=self.action_indices,\n",
        "            modality_keys=self.action_keys,\n",
        "        )\n",
        "\n",
        "        language_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.language_keys,\n",
        "        )\n",
        "\n",
        "        modality_configs = {\n",
        "            \"video\": video_modality,\n",
        "            \"state\": state_modality,\n",
        "            \"action\": action_modality,\n",
        "            \"language\": language_modality,\n",
        "        }\n",
        "\n",
        "        return modality_configs\n",
        "\n",
        "    def transform(self) -> ModalityTransform:\n",
        "        transforms = [\n",
        "            # video transforms\n",
        "            VideoToTensor(apply_to=self.video_keys),\n",
        "            VideoCrop(apply_to=self.video_keys, scale=0.95),\n",
        "            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=\"linear\"),\n",
        "            VideoColorJitter(\n",
        "                apply_to=self.video_keys,\n",
        "                brightness=0.3,\n",
        "                contrast=0.4,\n",
        "                saturation=0.5,\n",
        "                hue=0.08,\n",
        "            ),\n",
        "            VideoToNumpy(apply_to=self.video_keys),\n",
        "            # state transforms\n",
        "            StateActionToTensor(apply_to=self.state_keys),\n",
        "            StateActionSinCosTransform(apply_to=self.state_keys),\n",
        "            # action transforms\n",
        "            StateActionToTensor(apply_to=self.action_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.action_keys,\n",
        "                normalization_modes={key: \"min_max\" for key in self.action_keys},\n",
        "            ),\n",
        "            # concat transforms\n",
        "            ConcatTransform(\n",
        "                video_concat_order=self.video_keys,\n",
        "                state_concat_order=self.state_keys,\n",
        "                action_concat_order=self.action_keys,\n",
        "            ),\n",
        "            # model-specific transform\n",
        "            GR00TTransform(\n",
        "                state_horizon=len(self.observation_indices),\n",
        "                action_horizon=len(self.action_indices),\n",
        "                max_state_dim=64,\n",
        "                max_action_dim=32,\n",
        "            ),\n",
        "        ]\n",
        "        return ComposedModalityTransform(transforms=transforms)\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "class So100DataConfig(BaseDataConfig):\n",
        "    video_keys = [\"video.webcam\"]\n",
        "    state_keys = [\"state.single_arm\", \"state.gripper\"]\n",
        "    action_keys = [\"action.single_arm\", \"action.gripper\"]\n",
        "    language_keys = [\"annotation.human.task_description\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))\n",
        "\n",
        "    def modality_config(self) -> dict[str, ModalityConfig]:\n",
        "        video_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.video_keys,\n",
        "        )\n",
        "\n",
        "        state_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.state_keys,\n",
        "        )\n",
        "\n",
        "        action_modality = ModalityConfig(\n",
        "            delta_indices=self.action_indices,\n",
        "            modality_keys=self.action_keys,\n",
        "        )\n",
        "\n",
        "        language_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.language_keys,\n",
        "        )\n",
        "\n",
        "        modality_configs = {\n",
        "            \"video\": video_modality,\n",
        "            \"state\": state_modality,\n",
        "            \"action\": action_modality,\n",
        "            \"language\": language_modality,\n",
        "        }\n",
        "\n",
        "        return modality_configs\n",
        "\n",
        "    def transform(self) -> ModalityTransform:\n",
        "        transforms = [\n",
        "            # video transforms\n",
        "            VideoToTensor(apply_to=self.video_keys),\n",
        "            VideoCrop(apply_to=self.video_keys, scale=0.95),\n",
        "            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=\"linear\"),\n",
        "            VideoColorJitter(\n",
        "                apply_to=self.video_keys,\n",
        "                brightness=0.3,\n",
        "                contrast=0.4,\n",
        "                saturation=0.5,\n",
        "                hue=0.08,\n",
        "            ),\n",
        "            VideoToNumpy(apply_to=self.video_keys),\n",
        "            # state transforms\n",
        "            StateActionToTensor(apply_to=self.state_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.state_keys,\n",
        "                normalization_modes={key: \"min_max\" for key in self.state_keys},\n",
        "            ),\n",
        "            # action transforms\n",
        "            StateActionToTensor(apply_to=self.action_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.action_keys,\n",
        "                normalization_modes={key: \"min_max\" for key in self.action_keys},\n",
        "            ),\n",
        "            # concat transforms\n",
        "            ConcatTransform(\n",
        "                video_concat_order=self.video_keys,\n",
        "                state_concat_order=self.state_keys,\n",
        "                action_concat_order=self.action_keys,\n",
        "            ),\n",
        "            # model-specific transform\n",
        "            GR00TTransform(\n",
        "                state_horizon=len(self.observation_indices),\n",
        "                action_horizon=len(self.action_indices),\n",
        "                max_state_dim=64,\n",
        "                max_action_dim=32,\n",
        "            ),\n",
        "        ]\n",
        "        return ComposedModalityTransform(transforms=transforms)\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "class Gr1FullUpperBodyDataConfig(BaseDataConfig):\n",
        "    video_keys = [\"video.front_view\"]\n",
        "    state_keys = [\n",
        "        \"state.left_arm\",\n",
        "        \"state.right_arm\",\n",
        "        \"state.left_hand\",\n",
        "        \"state.right_hand\",\n",
        "        \"state.waist\",\n",
        "        \"state.neck\",\n",
        "    ]\n",
        "    action_keys = [\n",
        "        \"action.left_arm\",\n",
        "        \"action.right_arm\",\n",
        "        \"action.left_hand\",\n",
        "        \"action.right_hand\",\n",
        "        \"action.waist\",\n",
        "        \"action.neck\",\n",
        "    ]\n",
        "    language_keys = [\"annotation.human.action.task_description\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))\n",
        "\n",
        "    def modality_config(self):\n",
        "        video_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.video_keys,\n",
        "        )\n",
        "        state_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.state_keys,\n",
        "        )\n",
        "        action_modality = ModalityConfig(\n",
        "            delta_indices=self.action_indices,\n",
        "            modality_keys=self.action_keys,\n",
        "        )\n",
        "        language_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.language_keys,\n",
        "        )\n",
        "        modality_configs = {\n",
        "            \"video\": video_modality,\n",
        "            \"state\": state_modality,\n",
        "            \"action\": action_modality,\n",
        "            \"language\": language_modality,\n",
        "        }\n",
        "        return modality_configs\n",
        "\n",
        "    def transform(self):\n",
        "        transforms = [\n",
        "            # video transforms\n",
        "            VideoToTensor(apply_to=self.video_keys),\n",
        "            VideoCrop(apply_to=self.video_keys, scale=0.95),\n",
        "            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=\"linear\"),\n",
        "            VideoColorJitter(\n",
        "                apply_to=self.video_keys,\n",
        "                brightness=0.3,\n",
        "                contrast=0.4,\n",
        "                saturation=0.5,\n",
        "                hue=0.08,\n",
        "            ),\n",
        "            VideoToNumpy(apply_to=self.video_keys),\n",
        "            # state transforms\n",
        "            StateActionToTensor(apply_to=self.state_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.state_keys,\n",
        "                normalization_modes={key: \"min_max\" for key in self.state_keys},\n",
        "            ),\n",
        "            # action transforms\n",
        "            StateActionToTensor(apply_to=self.action_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.action_keys,\n",
        "                normalization_modes={key: \"min_max\" for key in self.action_keys},\n",
        "            ),\n",
        "            # concat transforms\n",
        "            ConcatTransform(\n",
        "                video_concat_order=self.video_keys,\n",
        "                state_concat_order=self.state_keys,\n",
        "                action_concat_order=self.action_keys,\n",
        "            ),\n",
        "            GR00TTransform(\n",
        "                state_horizon=len(self.observation_indices),\n",
        "                action_horizon=len(self.action_indices),\n",
        "                max_state_dim=64,\n",
        "                max_action_dim=32,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        return ComposedModalityTransform(transforms=transforms)\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "class BimanualPandaGripperDataConfig(BaseDataConfig):\n",
        "    video_keys = [\n",
        "        \"video.right_wrist_view\",\n",
        "        \"video.left_wrist_view\",\n",
        "        \"video.front_view\",\n",
        "    ]\n",
        "    state_keys = [\n",
        "        \"state.right_arm_eef_pos\",\n",
        "        \"state.right_arm_eef_quat\",\n",
        "        \"state.right_gripper_qpos\",\n",
        "        \"state.left_arm_eef_pos\",\n",
        "        \"state.left_arm_eef_quat\",\n",
        "        \"state.left_gripper_qpos\",\n",
        "    ]\n",
        "    action_keys = [\n",
        "        \"action.right_arm_eef_pos\",\n",
        "        \"action.right_arm_eef_rot\",\n",
        "        \"action.right_gripper_close\",\n",
        "        \"action.left_arm_eef_pos\",\n",
        "        \"action.left_arm_eef_rot\",\n",
        "        \"action.left_gripper_close\",\n",
        "    ]\n",
        "\n",
        "    language_keys = [\"annotation.human.action.task_description\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))\n",
        "\n",
        "    def modality_config(self):\n",
        "        video_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.video_keys,\n",
        "        )\n",
        "        state_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.state_keys,\n",
        "        )\n",
        "        action_modality = ModalityConfig(\n",
        "            delta_indices=self.action_indices,\n",
        "            modality_keys=self.action_keys,\n",
        "        )\n",
        "        language_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.language_keys,\n",
        "        )\n",
        "        modality_configs = {\n",
        "            \"video\": video_modality,\n",
        "            \"state\": state_modality,\n",
        "            \"action\": action_modality,\n",
        "            \"language\": language_modality,\n",
        "        }\n",
        "        return modality_configs\n",
        "\n",
        "    def transform(self):\n",
        "        transforms = [\n",
        "            # video transforms\n",
        "            VideoToTensor(apply_to=self.video_keys),\n",
        "            VideoCrop(apply_to=self.video_keys, scale=0.95),\n",
        "            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=\"linear\"),\n",
        "            VideoColorJitter(\n",
        "                apply_to=self.video_keys,\n",
        "                brightness=0.3,\n",
        "                contrast=0.4,\n",
        "                saturation=0.5,\n",
        "                hue=0.08,\n",
        "            ),\n",
        "            VideoToNumpy(apply_to=self.video_keys),\n",
        "            # state transforms\n",
        "            StateActionToTensor(apply_to=self.state_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.state_keys,\n",
        "                normalization_modes={\n",
        "                    \"state.right_arm_eef_pos\": \"min_max\",\n",
        "                    \"state.right_gripper_qpos\": \"min_max\",\n",
        "                    \"state.left_arm_eef_pos\": \"min_max\",\n",
        "                    \"state.left_gripper_qpos\": \"min_max\",\n",
        "                },\n",
        "                target_rotations={\n",
        "                    \"state.right_arm_eef_quat\": \"rotation_6d\",\n",
        "                    \"state.left_arm_eef_quat\": \"rotation_6d\",\n",
        "                },\n",
        "            ),\n",
        "            # action transforms\n",
        "            StateActionToTensor(apply_to=self.action_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.action_keys,\n",
        "                normalization_modes={\n",
        "                    \"action.right_gripper_close\": \"binary\",\n",
        "                    \"action.left_gripper_close\": \"binary\",\n",
        "                },\n",
        "            ),\n",
        "            # concat transforms\n",
        "            ConcatTransform(\n",
        "                video_concat_order=self.video_keys,\n",
        "                state_concat_order=self.state_keys,\n",
        "                action_concat_order=self.action_keys,\n",
        "            ),\n",
        "            GR00TTransform(\n",
        "                state_horizon=len(self.observation_indices),\n",
        "                action_horizon=len(self.action_indices),\n",
        "                max_state_dim=64,\n",
        "                max_action_dim=32,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        return ComposedModalityTransform(transforms=transforms)\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "class BimanualPandaHandDataConfig(BaseDataConfig):\n",
        "    video_keys = [\n",
        "        \"video.right_wrist_view\",\n",
        "        \"video.left_wrist_view\",\n",
        "        \"video.ego_view\",\n",
        "    ]\n",
        "    state_keys = [\n",
        "        \"state.right_arm_eef_pos\",\n",
        "        \"state.right_arm_eef_quat\",\n",
        "        \"state.right_hand\",\n",
        "        \"state.left_arm_eef_pos\",\n",
        "        \"state.left_arm_eef_quat\",\n",
        "        \"state.left_hand\",\n",
        "    ]\n",
        "    action_keys = [\n",
        "        \"action.right_arm_eef_pos\",\n",
        "        \"action.right_arm_eef_rot\",\n",
        "        \"action.right_hand\",\n",
        "        \"action.left_arm_eef_pos\",\n",
        "        \"action.left_arm_eef_rot\",\n",
        "        \"action.left_hand\",\n",
        "    ]\n",
        "    language_keys = [\"annotation.human.action.task_description\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))\n",
        "\n",
        "    def modality_config(self):\n",
        "        video_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.video_keys,\n",
        "        )\n",
        "        state_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.state_keys,\n",
        "        )\n",
        "        action_modality = ModalityConfig(\n",
        "            delta_indices=self.action_indices,\n",
        "            modality_keys=self.action_keys,\n",
        "        )\n",
        "        language_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.language_keys,\n",
        "        )\n",
        "        modality_configs = {\n",
        "            \"video\": video_modality,\n",
        "            \"state\": state_modality,\n",
        "            \"action\": action_modality,\n",
        "            \"language\": language_modality,\n",
        "        }\n",
        "        return modality_configs\n",
        "\n",
        "    def transform(self):\n",
        "        transforms = [\n",
        "            # video transforms\n",
        "            VideoToTensor(apply_to=self.video_keys),\n",
        "            VideoCrop(apply_to=self.video_keys, scale=0.95),\n",
        "            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=\"linear\"),\n",
        "            VideoColorJitter(\n",
        "                apply_to=self.video_keys,\n",
        "                brightness=0.3,\n",
        "                contrast=0.4,\n",
        "                saturation=0.5,\n",
        "                hue=0.08,\n",
        "            ),\n",
        "            VideoToNumpy(apply_to=self.video_keys),\n",
        "            # state transforms\n",
        "            StateActionToTensor(apply_to=self.state_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.state_keys,\n",
        "                normalization_modes={\n",
        "                    \"state.right_arm_eef_pos\": \"min_max\",\n",
        "                    \"state.right_hand\": \"min_max\",\n",
        "                    \"state.left_arm_eef_pos\": \"min_max\",\n",
        "                    \"state.left_hand\": \"min_max\",\n",
        "                },\n",
        "                target_rotations={\n",
        "                    \"state.right_arm_eef_quat\": \"rotation_6d\",\n",
        "                    \"state.left_arm_eef_quat\": \"rotation_6d\",\n",
        "                },\n",
        "            ),\n",
        "            # action transforms\n",
        "            StateActionToTensor(apply_to=self.action_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.action_keys,\n",
        "                normalization_modes={\n",
        "                    \"action.right_hand\": \"min_max\",\n",
        "                    \"action.left_hand\": \"min_max\",\n",
        "                },\n",
        "            ),\n",
        "            # concat transforms\n",
        "            ConcatTransform(\n",
        "                video_concat_order=self.video_keys,\n",
        "                state_concat_order=self.state_keys,\n",
        "                action_concat_order=self.action_keys,\n",
        "            ),\n",
        "            GR00TTransform(\n",
        "                state_horizon=len(self.observation_indices),\n",
        "                action_horizon=len(self.action_indices),\n",
        "                max_state_dim=64,\n",
        "                max_action_dim=32,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        return ComposedModalityTransform(transforms=transforms)\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "class SinglePandaGripperDataConfig(BaseDataConfig):\n",
        "    video_keys = [\n",
        "        \"video.left_view\",\n",
        "        \"video.right_view\",\n",
        "        \"video.wrist_view\",\n",
        "    ]\n",
        "    state_keys = [\n",
        "        \"state.end_effector_position_relative\",\n",
        "        \"state.end_effector_rotation_relative\",\n",
        "        \"state.gripper_qpos\",\n",
        "        \"state.base_position\",\n",
        "        \"state.base_rotation\",\n",
        "    ]\n",
        "    action_keys = [\n",
        "        \"action.end_effector_position\",\n",
        "        \"action.end_effector_rotation\",\n",
        "        \"action.gripper_close\",\n",
        "        \"action.base_motion\",\n",
        "        \"action.control_mode\",\n",
        "    ]\n",
        "\n",
        "    language_keys = [\"annotation.human.action.task_description\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))\n",
        "\n",
        "    def modality_config(self):\n",
        "        video_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.video_keys,\n",
        "        )\n",
        "        state_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.state_keys,\n",
        "        )\n",
        "        action_modality = ModalityConfig(\n",
        "            delta_indices=self.action_indices,\n",
        "            modality_keys=self.action_keys,\n",
        "        )\n",
        "        language_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.language_keys,\n",
        "        )\n",
        "        modality_configs = {\n",
        "            \"video\": video_modality,\n",
        "            \"state\": state_modality,\n",
        "            \"action\": action_modality,\n",
        "            \"language\": language_modality,\n",
        "        }\n",
        "        return modality_configs\n",
        "\n",
        "    def transform(self):\n",
        "        transforms = [\n",
        "            # video transforms\n",
        "            VideoToTensor(apply_to=self.video_keys),\n",
        "            VideoCrop(apply_to=self.video_keys, scale=0.95),\n",
        "            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=\"linear\"),\n",
        "            VideoColorJitter(\n",
        "                apply_to=self.video_keys,\n",
        "                brightness=0.3,\n",
        "                contrast=0.4,\n",
        "                saturation=0.5,\n",
        "                hue=0.08,\n",
        "            ),\n",
        "            VideoToNumpy(apply_to=self.video_keys),\n",
        "            # state transforms\n",
        "            StateActionToTensor(apply_to=self.state_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.state_keys,\n",
        "                normalization_modes={\n",
        "                    \"state.end_effector_position_relative\": \"min_max\",\n",
        "                    \"state.end_effector_rotation_relative\": \"min_max\",\n",
        "                    \"state.gripper_qpos\": \"min_max\",\n",
        "                    \"state.base_position\": \"min_max\",\n",
        "                    \"state.base_rotation\": \"min_max\",\n",
        "                },\n",
        "                target_rotations={\n",
        "                    \"state.end_effector_rotation_relative\": \"rotation_6d\",\n",
        "                    \"state.base_rotation\": \"rotation_6d\",\n",
        "                },\n",
        "            ),\n",
        "            # action transforms\n",
        "            StateActionToTensor(apply_to=self.action_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.action_keys,\n",
        "                normalization_modes={\n",
        "                    \"action.end_effector_position\": \"min_max\",\n",
        "                    \"action.end_effector_rotation\": \"min_max\",\n",
        "                    \"action.gripper_close\": \"binary\",\n",
        "                    \"action.base_motion\": \"min_max\",\n",
        "                    \"action.control_mode\": \"binary\",\n",
        "                },\n",
        "            ),\n",
        "            # concat transforms\n",
        "            ConcatTransform(\n",
        "                video_concat_order=self.video_keys,\n",
        "                state_concat_order=self.state_keys,\n",
        "                action_concat_order=self.action_keys,\n",
        "            ),\n",
        "            GR00TTransform(\n",
        "                state_horizon=len(self.observation_indices),\n",
        "                action_horizon=len(self.action_indices),\n",
        "                max_state_dim=64,\n",
        "                max_action_dim=32,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        return ComposedModalityTransform(transforms=transforms)\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "class Gr1ArmsWaistDataConfig(Gr1ArmsOnlyDataConfig):\n",
        "    video_keys = [\"video.ego_view\"]\n",
        "    state_keys = [\n",
        "        \"state.left_arm\",\n",
        "        \"state.right_arm\",\n",
        "        \"state.left_hand\",\n",
        "        \"state.right_hand\",\n",
        "        \"state.waist\",\n",
        "    ]\n",
        "    action_keys = [\n",
        "        \"action.left_arm\",\n",
        "        \"action.right_arm\",\n",
        "        \"action.left_hand\",\n",
        "        \"action.right_hand\",\n",
        "        \"action.waist\",\n",
        "    ]\n",
        "    language_keys = [\"annotation.human.coarse_action\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))\n",
        "\n",
        "    def modality_config(self):\n",
        "        return super().modality_config()\n",
        "\n",
        "    def transform(self):\n",
        "        return super().transform()\n",
        "\n",
        "class KukaCustomDataConfig(BaseDataConfig):\n",
        "    video_keys = [\"video.webcam\"]\n",
        "    state_keys = [\"state.single_arm\"]\n",
        "    action_keys = [\"action.single_arm\"]\n",
        "    observation_indices = [0]\n",
        "    action_indices = list(range(16))  # adjust if different\n",
        "\n",
        "    def modality_config(self) -> dict[str, ModalityConfig]:\n",
        "        video_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.video_keys,\n",
        "        )\n",
        "\n",
        "        state_modality = ModalityConfig(\n",
        "            delta_indices=self.observation_indices,\n",
        "            modality_keys=self.state_keys,\n",
        "        )\n",
        "\n",
        "        action_modality = ModalityConfig(\n",
        "            delta_indices=self.action_indices,\n",
        "            modality_keys=self.action_keys,\n",
        "        )\n",
        "\n",
        "        modality_configs = {\n",
        "            \"video\": video_modality,\n",
        "            \"state\": state_modality,\n",
        "            \"action\": action_modality,\n",
        "        }\n",
        "\n",
        "        return modality_configs\n",
        "\n",
        "    def transform(self) -> ModalityTransform:\n",
        "        transforms = [\n",
        "            # video transforms\n",
        "            VideoToTensor(apply_to=self.video_keys),\n",
        "            VideoCrop(apply_to=self.video_keys, scale=0.95),\n",
        "            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=\"linear\"),\n",
        "            VideoColorJitter(\n",
        "                apply_to=self.video_keys,\n",
        "                brightness=0.3,\n",
        "                contrast=0.4,\n",
        "                saturation=0.5,\n",
        "                hue=0.08,\n",
        "            ),\n",
        "            VideoToNumpy(apply_to=self.video_keys),\n",
        "            # state transforms\n",
        "            StateActionToTensor(apply_to=self.state_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.state_keys,\n",
        "                normalization_modes={key: \"min_max\" for key in self.state_keys},\n",
        "            ),\n",
        "            # action transforms\n",
        "            StateActionToTensor(apply_to=self.action_keys),\n",
        "            StateActionTransform(\n",
        "                apply_to=self.action_keys,\n",
        "                normalization_modes={key: \"min_max\" for key in self.action_keys},\n",
        "            ),\n",
        "            # concat transforms\n",
        "            ConcatTransform(\n",
        "                video_concat_order=self.video_keys,\n",
        "                state_concat_order=self.state_keys,\n",
        "                action_concat_order=self.action_keys,\n",
        "            ),\n",
        "            # model-specific transform\n",
        "            GR00TTransform(\n",
        "                state_horizon=len(self.observation_indices),\n",
        "                action_horizon=len(self.action_indices),\n",
        "                max_state_dim=64,\n",
        "                max_action_dim=32,\n",
        "            ),\n",
        "        ]\n",
        "        return ComposedModalityTransform(transforms=transforms)\n",
        "\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "DATA_CONFIG_MAP = {\n",
        "    \"gr1_arms_waist\": Gr1ArmsWaistDataConfig(),\n",
        "    \"gr1_arms_only\": Gr1ArmsOnlyDataConfig(),\n",
        "    \"gr1_full_upper_body\": Gr1FullUpperBodyDataConfig(),\n",
        "    \"bimanual_panda_gripper\": BimanualPandaGripperDataConfig(),\n",
        "    \"bimanual_panda_hand\": BimanualPandaHandDataConfig(),\n",
        "    \"single_panda_gripper\": SinglePandaGripperDataConfig(),\n",
        "    \"so100\": So100DataConfig(),\n",
        "    \"kuka_custom\": KukaCustomDataConfig(),\n",
        "}"
      ],
      "metadata": {
        "id": "oFhM3O6vdT7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40813719-d509-4547-ebea-d55882718290"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Isaac-GR00T/gr00t/experiment/data_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9dhVZs4AZ65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6Wqa7eQc1u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Isaac-GR00T\n",
        "!python3.10 -m pip install -e . --no-deps\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qE7nVNXus1T",
        "outputId": "556c3f51-130a-4e37-ee2c-8fc2827e163f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Isaac-GR00T\n",
            "Obtaining file:///content/Isaac-GR00T\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gr00t\n",
            "  Building editable for gr00t (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gr00t: filename=gr00t-0.1.0-0.editable-py3-none-any.whl size=12598 sha256=cfafb06da46f78a96e373c3087bb62cb50e424e647ade8d3b4e8e57b4b89ecac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nxu_wi3t/wheels/d0/7f/47/4901afd9f5b81cc3e9e77fa1f45165719958da01a60acc919a\n",
            "Successfully built gr00t\n",
            "Installing collected packages: gr00t\n",
            "  Attempting uninstall: gr00t\n",
            "    Found existing installation: gr00t 0.1.0\n",
            "    Uninstalling gr00t-0.1.0:\n",
            "      Successfully uninstalled gr00t-0.1.0\n",
            "Successfully installed gr00t-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-BMB_PUcAa4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRaining 2"
      ],
      "metadata": {
        "id": "qOYahfR3n0lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GOhUTOGOXTRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.10 -c \"from gr00t.experiment.data_config import DATA_CONFIG_MAP; print(DATA_CONFIG_MAP.keys())\"\n"
      ],
      "metadata": {
        "id": "M-I4v3GJkozp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9482abfc-f0c9-4a6c-fde6-a05d308cb569"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 18:26:27.560483: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 18:26:27.610653: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 18:26:27.610705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 18:26:27.611992: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 18:26:27.619256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 18:26:28.564437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "dict_keys(['gr1_arms_waist', 'gr1_arms_only', 'gr1_full_upper_body', 'bimanual_panda_gripper', 'bimanual_panda_hand', 'single_panda_gripper', 'so100', 'kuka_custom'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "info_path = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/info.json\"\n",
        "\n",
        "with open(info_path, \"r\") as f:\n",
        "    info = json.load(f)\n",
        "\n",
        "features = info[\"features\"]\n",
        "\n",
        "# Add dummy gripper fields\n",
        "features[\"state.gripper\"] = {\n",
        "    \"dtype\": \"float32\",\n",
        "    \"shape\": [1],\n",
        "    \"names\": [\"gripper_pos\"]\n",
        "}\n",
        "features[\"action.gripper\"] = {\n",
        "    \"dtype\": \"float32\",\n",
        "    \"shape\": [1],\n",
        "    \"names\": [\"gripper_cmd\"]\n",
        "}\n",
        "\n",
        "with open(info_path, \"w\") as f:\n",
        "    json.dump(info, f, indent=2)\n",
        "\n",
        "print(\"Patched info.json with dummy gripper fields.\")\n"
      ],
      "metadata": {
        "id": "UNKeIy25ludA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad529c0-5785-4ef9-9fc4-c678cbdef8d3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched info.json with dummy gripper fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "base_path = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000\"\n",
        "old_folder = os.path.join(base_path, \"observation.images.webcam\")\n",
        "new_folder = os.path.join(base_path, \"videos.webcam\")\n",
        "\n",
        "# Rename the folder\n",
        "if os.path.exists(old_folder):\n",
        "    os.rename(old_folder, new_folder)\n",
        "    print(f\"Renamed folder:\\n{old_folder} → {new_folder}\")\n",
        "else:\n",
        "    print(f\"Folder not found: {old_folder}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bc2c4h1OyxG",
        "outputId": "9b7a8f5e-e57c-4e01-a4ad-73610fbc5ae8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder not found: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/observation.images.webcam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000\"\n",
        "\n",
        "\n",
        "old_folder = os.path.join(base_path, \"observation.images.webcam\")\n",
        "new_folder = os.path.join(base_path, \"videos.webcam\")\n",
        "\n",
        "if os.path.exists(old_folder):\n",
        "    os.rename(old_folder, new_folder)\n",
        "    print(f\"Renamed folder:\\n{old_folder} → {new_folder}\")\n",
        "else:\n",
        "    print(f\"Folder not found (skipping step 1): {old_folder}\")\n",
        "\n",
        "old_folder_2 = os.path.join(base_path, \"videos.webcam\")\n",
        "new_folder_2 = os.path.join(base_path, \"video.webcam\")\n",
        "\n",
        "if os.path.exists(old_folder_2):\n",
        "    os.rename(old_folder_2, new_folder_2)\n",
        "    print(f\"Renamed folder:\\n{old_folder_2} → {new_folder_2}\")\n",
        "else:\n",
        "    print(f\"Folder not found (skipping step 2): {old_folder_2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CadbfjHAPBGd",
        "outputId": "9e346f8f-fb0e-4976-abf0-d7f675f57b37"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder not found (skipping step 1): /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/observation.images.webcam\n",
            "Renamed folder:\n",
            "/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/videos.webcam → /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/video.webcam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "video_dir = '/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/videos/chunk-000/video.webcam'\n",
        "print(sorted(os.listdir(video_dir)))\n"
      ],
      "metadata": {
        "id": "BPlW__94ufgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b295dc-77b8-4ff1-ac58-e2eccd3d99e1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['episode_000000.mp4', 'episode_000001.mp4', 'episode_000002.mp4', 'episode_000003.mp4', 'episode_000004.mp4', 'episode_000005.mp4', 'episode_000006.mp4', 'episode_000007.mp4', 'episode_000008.mp4', 'episode_000009.mp4', 'episode_000010.mp4', 'episode_000011.mp4', 'episode_000012.mp4', 'episode_000013.mp4', 'episode_000014.mp4', 'episode_000015.mp4', 'episode_000016.mp4', 'episode_000017.mp4', 'episode_000018.mp4', 'episode_000019.mp4', 'episode_000020.mp4', 'episode_000021.mp4', 'episode_000022.mp4', 'episode_000023.mp4', 'episode_000024.mp4', 'episode_000025.mp4', 'episode_000026.mp4', 'episode_000027.mp4', 'episode_000028.mp4', 'episode_000029.mp4', 'episode_000030.mp4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.10 -c \"import torch; print('Torch:', torch.__version__)\"\n",
        "!nvcc --version || !nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VazvgI9vVDN",
        "outputId": "596d68f4-f039-4fb6-dc2d-589358eba68d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.5.1+cu124\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R_FtOxdyXafz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeJRDOFbXatp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWaCcp9avjgf",
        "outputId": "b1fe54fb-6df1-472d-d74e-6943a3316a76"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187797312 sha256=b267f80a08e516292cdd748056a2178a45b8abedf7fca123292eb17c21c8c87c\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.7.4.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y8_ekl5nDHj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8p1iINXyw8vx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os; os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "1MZ9vZrPxbKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggWchKhw-r5",
        "outputId": "23491207-9d77-4501-be89-a2971b30e657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 16 08:24:29 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   48C    P8             16W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeRtYLMvWDll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  !nvidia-smi -q -d PIDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evCj8km_xD53",
        "outputId": "27121015-6c67-4737-ae66-c168b793612c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============NVSMI LOG==============\n",
            "\n",
            "Timestamp                                 : Fri May 16 08:24:31 2025\n",
            "Driver Version                            : 550.54.15\n",
            "CUDA Version                              : 12.4\n",
            "\n",
            "Attached GPUs                             : 1\n",
            "GPU 00000000:00:03.0\n",
            "    Processes                             : None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def print_gpu_memory(tag):\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
        "    reserved_mem = torch.cuda.memory_reserved(0) / (1024 ** 3)\n",
        "    allocated_mem = torch.cuda.memory_allocated(0) / (1024 ** 3)\n",
        "    free_mem = reserved_mem - allocated_mem\n",
        "    actual_free = total_mem - reserved_mem\n",
        "    print(f\"\\n GPU Memory ({tag})\")\n",
        "    print(f\"Total: {total_mem:.2f} GB | Reserved: {reserved_mem:.2f} GB | Allocated: {allocated_mem:.2f} GB | Free Reserved: {free_mem:.2f} GB | Actual Free: {actual_free:.2f} GB\")\n",
        "\n",
        "# Before training script\n",
        "print_gpu_memory(\"Before Script Start\")\n",
        "\n",
        "!python scripts/gr00t_finetune.py \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --output_dir /content/gr00t_finetuned_kuka \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --batch_size 4 \\\n",
        "  --max_steps 5000 \\\n",
        "  --save_steps 2500 \\\n",
        "  --no-tune_llm \\\n",
        "  --tune_visual \\\n",
        "  --tune_projector \\\n",
        "  --tune_diffusion_model \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --video_backend decord \\\n",
        "  --report_to tensorboard \\\n",
        "  --num_gpus 1\n",
        "\n",
        "# After training script (if it completes or crashes and returns)\n",
        "print_gpu_memory(\"After Script End\")\n"
      ],
      "metadata": {
        "id": "xeXNH0F5_er7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59dd718f-46ce-45d5-fb8b-4dcec8fc1e2a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " GPU Memory (Before Script Start)\n",
            "Total: 39.56 GB | Reserved: 0.00 GB | Allocated: 0.00 GB | Free Reserved: 0.00 GB | Actual Free: 39.56 GB\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 18:27:26.546568: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 18:27:26.596118: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 18:27:26.596171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 18:27:26.597419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 18:27:26.604575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 18:27:27.718748: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "==================================================\n",
            "GR00T FINE-TUNING CONFIGURATION:\n",
            "==================================================\n",
            "dataset_path: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset\n",
            "output_dir: /content/gr00t_finetuned_kuka\n",
            "data_config: kuka_custom\n",
            "batch_size: 4\n",
            "max_steps: 5000\n",
            "num_gpus: 1\n",
            "save_steps: 2500\n",
            "base_model_path: nvidia/GR00T-N1-2B\n",
            "tune_llm: False\n",
            "tune_visual: True\n",
            "tune_projector: True\n",
            "tune_diffusion_model: True\n",
            "resume: False\n",
            "learning_rate: 0.0001\n",
            "weight_decay: 1e-05\n",
            "warmup_ratio: 0.05\n",
            "lora_rank: 0\n",
            "lora_alpha: 16\n",
            "lora_dropout: 0.1\n",
            "dataloader_num_workers: 8\n",
            "report_to: tensorboard\n",
            "embodiment_tag: new_embodiment\n",
            "video_backend: decord\n",
            "==================================================\n",
            "\n",
            "Using 1 GPUs\n",
            "Failed to load dataset statistics: [Errno 2] No such file or directory: '/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset/meta/stats.json'\n",
            "Calculating dataset statistics for Full_Kuka_Dataset\n",
            "Collecting all parquet files...: 100% 31/31 [00:00<00:00, 224.81it/s]\n",
            "Computing statistics for observation.state...\n",
            "Computing statistics for action...\n",
            "Computing statistics for timestamp...\n",
            "Computing statistics for frame_index...\n",
            "Computing statistics for episode_index...\n",
            "Computing statistics for index...\n",
            "Computing statistics for task_index...\n",
            "Initialized dataset Full_Kuka_Dataset with EmbodimentTag.NEW_EMBODIMENT\n",
            "Loading pretrained dual brain from nvidia/GR00T-N1-2B\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Fetching 6 files:   0% 0/6 [00:00<?, ?it/s]\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 12.9MB/s]\n",
            "Fetching 6 files:  17% 1/6 [00:00<00:01,  4.57it/s]\n",
            "config.json: 100% 1.67k/1.67k [00:00<00:00, 15.7MB/s]\n",
            "\n",
            "metadata.json: 100% 31.5k/31.5k [00:00<00:00, 8.88MB/s]\n",
            "\n",
            "LICENSE: 100% 4.06k/4.06k [00:00<00:00, 42.8MB/s]\n",
            "\n",
            "README.md: 100% 394/394 [00:00<00:00, 4.37MB/s]\n",
            "\n",
            "model.safetensors:   0% 0.00/4.38G [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   1% 31.5M/4.38G [00:00<00:17, 245MB/s]\u001b[A\n",
            "model.safetensors:   2% 73.4M/4.38G [00:00<00:13, 310MB/s]\u001b[A\n",
            "model.safetensors:   3% 115M/4.38G [00:00<00:12, 341MB/s] \u001b[A\n",
            "model.safetensors:   4% 157M/4.38G [00:00<00:12, 341MB/s]\u001b[A\n",
            "model.safetensors:   5% 199M/4.38G [00:00<00:12, 340MB/s]\u001b[A\n",
            "model.safetensors:   6% 241M/4.38G [00:00<00:11, 353MB/s]\u001b[A\n",
            "model.safetensors:   6% 283M/4.38G [00:00<00:11, 364MB/s]\u001b[A\n",
            "model.safetensors:   7% 325M/4.38G [00:00<00:10, 376MB/s]\u001b[A\n",
            "model.safetensors:   8% 367M/4.38G [00:01<00:10, 374MB/s]\u001b[A\n",
            "model.safetensors:   9% 409M/4.38G [00:01<00:10, 382MB/s]\u001b[A\n",
            "model.safetensors:  10% 451M/4.38G [00:01<00:10, 379MB/s]\u001b[A\n",
            "model.safetensors:  11% 493M/4.38G [00:01<00:10, 380MB/s]\u001b[A\n",
            "model.safetensors:  12% 535M/4.38G [00:01<00:10, 382MB/s]\u001b[A\n",
            "model.safetensors:  13% 577M/4.38G [00:01<00:10, 379MB/s]\u001b[A\n",
            "model.safetensors:  14% 619M/4.38G [00:01<00:10, 352MB/s]\u001b[A\n",
            "model.safetensors:  15% 661M/4.38G [00:01<00:11, 335MB/s]\u001b[A\n",
            "model.safetensors:  16% 703M/4.38G [00:01<00:10, 346MB/s]\u001b[A\n",
            "model.safetensors:  17% 744M/4.38G [00:02<00:10, 361MB/s]\u001b[A\n",
            "model.safetensors:  18% 786M/4.38G [00:02<00:09, 365MB/s]\u001b[A\n",
            "model.safetensors:  19% 828M/4.38G [00:02<00:09, 368MB/s]\u001b[A\n",
            "model.safetensors:  20% 870M/4.38G [00:02<00:09, 362MB/s]\u001b[A\n",
            "model.safetensors:  21% 912M/4.38G [00:02<00:09, 366MB/s]\u001b[A\n",
            "model.safetensors:  22% 954M/4.38G [00:02<00:09, 363MB/s]\u001b[A\n",
            "model.safetensors:  23% 996M/4.38G [00:02<00:09, 352MB/s]\u001b[A\n",
            "model.safetensors:  24% 1.04G/4.38G [00:02<00:09, 349MB/s]\u001b[A\n",
            "model.safetensors:  25% 1.08G/4.38G [00:03<00:09, 353MB/s]\u001b[A\n",
            "model.safetensors:  26% 1.12G/4.38G [00:03<00:09, 350MB/s]\u001b[A\n",
            "model.safetensors:  27% 1.16G/4.38G [00:03<00:09, 352MB/s]\u001b[A\n",
            "model.safetensors:  28% 1.21G/4.38G [00:03<00:08, 354MB/s]\u001b[A\n",
            "model.safetensors:  28% 1.25G/4.38G [00:03<00:08, 351MB/s]\u001b[A\n",
            "model.safetensors:  29% 1.29G/4.38G [00:03<00:08, 352MB/s]\u001b[A\n",
            "model.safetensors:  30% 1.33G/4.38G [00:03<00:08, 350MB/s]\u001b[A\n",
            "model.safetensors:  31% 1.37G/4.38G [00:03<00:09, 318MB/s]\u001b[A\n",
            "model.safetensors:  32% 1.42G/4.38G [00:04<00:08, 332MB/s]\u001b[A\n",
            "model.safetensors:  33% 1.46G/4.38G [00:04<00:09, 316MB/s]\u001b[A\n",
            "model.safetensors:  34% 1.50G/4.38G [00:04<00:09, 307MB/s]\u001b[A\n",
            "model.safetensors:  35% 1.54G/4.38G [00:04<00:08, 321MB/s]\u001b[A\n",
            "model.safetensors:  36% 1.58G/4.38G [00:04<00:08, 330MB/s]\u001b[A\n",
            "model.safetensors:  37% 1.63G/4.38G [00:04<00:08, 322MB/s]\u001b[A\n",
            "model.safetensors:  38% 1.67G/4.38G [00:04<00:08, 333MB/s]\u001b[A\n",
            "model.safetensors:  39% 1.71G/4.38G [00:04<00:07, 343MB/s]\u001b[A\n",
            "model.safetensors:  40% 1.75G/4.38G [00:05<00:07, 346MB/s]\u001b[A\n",
            "model.safetensors:  41% 1.79G/4.38G [00:05<00:07, 339MB/s]\u001b[A\n",
            "model.safetensors:  42% 1.84G/4.38G [00:05<00:07, 336MB/s]\u001b[A\n",
            "model.safetensors:  43% 1.88G/4.38G [00:05<00:07, 342MB/s]\u001b[A\n",
            "model.safetensors:  44% 1.92G/4.38G [00:05<00:07, 348MB/s]\u001b[A\n",
            "model.safetensors:  45% 1.96G/4.38G [00:05<00:06, 346MB/s]\u001b[A\n",
            "model.safetensors:  46% 2.00G/4.38G [00:05<00:06, 341MB/s]\u001b[A\n",
            "model.safetensors:  47% 2.04G/4.38G [00:05<00:06, 347MB/s]\u001b[A\n",
            "model.safetensors:  48% 2.09G/4.38G [00:06<00:06, 347MB/s]\u001b[A\n",
            "model.safetensors:  49% 2.13G/4.38G [00:06<00:06, 340MB/s]\u001b[A\n",
            "model.safetensors:  50% 2.17G/4.38G [00:06<00:06, 346MB/s]\u001b[A\n",
            "model.safetensors:  51% 2.21G/4.38G [00:06<00:06, 347MB/s]\u001b[A\n",
            "model.safetensors:  51% 2.25G/4.38G [00:06<00:06, 351MB/s]\u001b[A\n",
            "model.safetensors:  52% 2.30G/4.38G [00:06<00:05, 352MB/s]\u001b[A\n",
            "model.safetensors:  53% 2.34G/4.38G [00:06<00:05, 351MB/s]\u001b[A\n",
            "model.safetensors:  54% 2.38G/4.38G [00:06<00:05, 350MB/s]\u001b[A\n",
            "model.safetensors:  55% 2.42G/4.38G [00:06<00:05, 329MB/s]\u001b[A\n",
            "model.safetensors:  56% 2.46G/4.38G [00:07<00:05, 338MB/s]\u001b[A\n",
            "model.safetensors:  57% 2.51G/4.38G [00:07<00:05, 344MB/s]\u001b[A\n",
            "model.safetensors:  58% 2.55G/4.38G [00:07<00:05, 309MB/s]\u001b[A\n",
            "model.safetensors:  59% 2.59G/4.38G [00:07<00:05, 308MB/s]\u001b[A\n",
            "model.safetensors:  60% 2.62G/4.38G [00:07<00:05, 309MB/s]\u001b[A\n",
            "model.safetensors:  61% 2.66G/4.38G [00:07<00:05, 320MB/s]\u001b[A\n",
            "model.safetensors:  62% 2.71G/4.38G [00:07<00:05, 314MB/s]\u001b[A\n",
            "model.safetensors:  63% 2.75G/4.38G [00:08<00:05, 318MB/s]\u001b[A\n",
            "model.safetensors:  64% 2.79G/4.38G [00:08<00:04, 322MB/s]\u001b[A\n",
            "model.safetensors:  65% 2.83G/4.38G [00:08<00:04, 314MB/s]\u001b[A\n",
            "model.safetensors:  66% 2.87G/4.38G [00:08<00:05, 296MB/s]\u001b[A\n",
            "model.safetensors:  66% 2.90G/4.38G [00:08<00:05, 285MB/s]\u001b[A\n",
            "model.safetensors:  67% 2.94G/4.38G [00:08<00:05, 282MB/s]\u001b[A\n",
            "model.safetensors:  68% 2.98G/4.38G [00:08<00:04, 294MB/s]\u001b[A\n",
            "model.safetensors:  69% 3.01G/4.38G [00:08<00:04, 298MB/s]\u001b[A\n",
            "model.safetensors:  70% 3.05G/4.38G [00:09<00:04, 314MB/s]\u001b[A\n",
            "model.safetensors:  71% 3.09G/4.38G [00:09<00:03, 331MB/s]\u001b[A\n",
            "model.safetensors:  72% 3.14G/4.38G [00:09<00:03, 340MB/s]\u001b[A\n",
            "model.safetensors:  73% 3.18G/4.38G [00:09<00:03, 348MB/s]\u001b[A\n",
            "model.safetensors:  73% 3.22G/4.38G [00:09<00:03, 353MB/s]\u001b[A\n",
            "model.safetensors:  74% 3.26G/4.38G [00:09<00:03, 346MB/s]\u001b[A\n",
            "model.safetensors:  75% 3.30G/4.38G [00:09<00:03, 345MB/s]\u001b[A\n",
            "model.safetensors:  76% 3.34G/4.38G [00:09<00:02, 346MB/s]\u001b[A\n",
            "model.safetensors:  77% 3.39G/4.38G [00:09<00:02, 353MB/s]\u001b[A\n",
            "model.safetensors:  78% 3.43G/4.38G [00:10<00:04, 225MB/s]\u001b[A\n",
            "model.safetensors:  79% 3.47G/4.38G [00:10<00:03, 254MB/s]\u001b[A\n",
            "model.safetensors:  80% 3.51G/4.38G [00:10<00:03, 278MB/s]\u001b[A\n",
            "model.safetensors:  81% 3.55G/4.38G [00:10<00:02, 295MB/s]\u001b[A\n",
            "model.safetensors:  82% 3.60G/4.38G [00:10<00:02, 288MB/s]\u001b[A\n",
            "model.safetensors:  83% 3.64G/4.38G [00:10<00:02, 310MB/s]\u001b[A\n",
            "model.safetensors:  84% 3.68G/4.38G [00:11<00:02, 324MB/s]\u001b[A\n",
            "model.safetensors:  85% 3.72G/4.38G [00:11<00:01, 335MB/s]\u001b[A\n",
            "model.safetensors:  86% 3.76G/4.38G [00:11<00:01, 349MB/s]\u001b[A\n",
            "model.safetensors:  87% 3.81G/4.38G [00:11<00:01, 325MB/s]\u001b[A\n",
            "model.safetensors:  88% 3.85G/4.38G [00:11<00:01, 336MB/s]\u001b[A\n",
            "model.safetensors:  89% 3.89G/4.38G [00:11<00:01, 343MB/s]\u001b[A\n",
            "model.safetensors:  90% 3.93G/4.38G [00:11<00:01, 340MB/s]\u001b[A\n",
            "model.safetensors:  91% 3.97G/4.38G [00:11<00:01, 340MB/s]\u001b[A\n",
            "model.safetensors:  92% 4.02G/4.38G [00:12<00:01, 342MB/s]\u001b[A\n",
            "model.safetensors:  93% 4.06G/4.38G [00:12<00:00, 350MB/s]\u001b[A\n",
            "model.safetensors:  94% 4.10G/4.38G [00:12<00:00, 348MB/s]\u001b[A\n",
            "model.safetensors:  95% 4.14G/4.38G [00:12<00:00, 346MB/s]\u001b[A\n",
            "model.safetensors:  96% 4.18G/4.38G [00:12<00:00, 296MB/s]\u001b[A\n",
            "model.safetensors:  96% 4.22G/4.38G [00:12<00:00, 289MB/s]\u001b[A\n",
            "model.safetensors:  97% 4.26G/4.38G [00:12<00:00, 299MB/s]\u001b[A\n",
            "model.safetensors:  98% 4.30G/4.38G [00:12<00:00, 311MB/s]\u001b[A\n",
            "model.safetensors:  99% 4.34G/4.38G [00:13<00:00, 320MB/s]\u001b[A\n",
            "model.safetensors: 100% 4.38G/4.38G [00:13<00:00, 332MB/s]\n",
            "Fetching 6 files: 100% 6/6 [00:14<00:00,  2.35s/it]\n",
            "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in SiglipVisionModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Some weights of the model checkpoint at /root/.cache/huggingface/hub/models--nvidia--GR00T-N1-2B/snapshots/32e1fd2507f7739fad443e6b449c8188e0e02fcb were not used when initializing GR00T_N1: ['action_head.decode_layer.bias', 'action_head.decode_layer.weight']\n",
            "- This IS expected if you are initializing GR00T_N1 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GR00T_N1 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Run name: /content/gr00t_finetuned_kuka\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "train dataloader length: 7018\n",
            "train dataset length: 28072\n",
            "GPU memory before training: 8.159651279449463 GB\n",
            "TensorBoard logs will be saved to: /content/gr00t_finetuned_kuka/runs\n",
            "  0% 0/5000 [00:00<?, ?it/s]The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "{'loss': 0.4843, 'grad_norm': 8.155009269714355, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
            "{'loss': 0.5062, 'grad_norm': 6.241395473480225, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
            "{'loss': 0.3272, 'grad_norm': 2.0962915420532227, 'learning_rate': 1.2e-05, 'epoch': 0.0}\n",
            "{'loss': 0.1821, 'grad_norm': 2.049894094467163, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2771, 'grad_norm': 2.9386868476867676, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2059, 'grad_norm': 3.3682539463043213, 'learning_rate': 2.4e-05, 'epoch': 0.01}\n",
            "{'loss': 0.137, 'grad_norm': 1.9163901805877686, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}\n",
            "{'loss': 0.1232, 'grad_norm': 1.2577784061431885, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}\n",
            "{'loss': 0.1017, 'grad_norm': 2.304697036743164, 'learning_rate': 3.6e-05, 'epoch': 0.01}\n",
            "{'loss': 0.1503, 'grad_norm': 1.8934948444366455, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
            "{'loss': 0.146, 'grad_norm': 1.1109157800674438, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.02}\n",
            "{'loss': 0.1176, 'grad_norm': 1.7879924774169922, 'learning_rate': 4.8e-05, 'epoch': 0.02}\n",
            "{'loss': 0.1365, 'grad_norm': 1.8192085027694702, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.02}\n",
            "{'loss': 0.1498, 'grad_norm': 2.577816963195801, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0741, 'grad_norm': 0.5839712023735046, 'learning_rate': 6e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0868, 'grad_norm': 0.7457613945007324, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0947, 'grad_norm': 0.9228950142860413, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0986, 'grad_norm': 1.058661937713623, 'learning_rate': 7.2e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0789, 'grad_norm': 1.3238633871078491, 'learning_rate': 7.6e-05, 'epoch': 0.03}\n",
            "{'loss': 0.1074, 'grad_norm': 1.8530805110931396, 'learning_rate': 8e-05, 'epoch': 0.03}\n",
            "{'loss': 0.109, 'grad_norm': 2.2874996662139893, 'learning_rate': 8.4e-05, 'epoch': 0.03}\n",
            "{'loss': 0.1386, 'grad_norm': 1.0602909326553345, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.03}\n",
            "{'loss': 0.095, 'grad_norm': 1.1760008335113525, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.03}\n",
            "{'loss': 0.1317, 'grad_norm': 0.6914915442466736, 'learning_rate': 9.6e-05, 'epoch': 0.03}\n",
            "{'loss': 0.1135, 'grad_norm': 0.9027654528617859, 'learning_rate': 0.0001, 'epoch': 0.04}\n",
            "{'loss': 0.0759, 'grad_norm': 1.3173027038574219, 'learning_rate': 9.999890641901125e-05, 'epoch': 0.04}\n",
            "{'loss': 0.1388, 'grad_norm': 0.9294147491455078, 'learning_rate': 9.99956257238817e-05, 'epoch': 0.04}\n",
            "{'loss': 0.0939, 'grad_norm': 0.8726794123649597, 'learning_rate': 9.999015805811965e-05, 'epoch': 0.04}\n",
            "{'loss': 0.1163, 'grad_norm': 0.7147972583770752, 'learning_rate': 9.998250366089848e-05, 'epoch': 0.04}\n",
            "{'loss': 0.1001, 'grad_norm': 1.7434228658676147, 'learning_rate': 9.997266286704631e-05, 'epoch': 0.04}\n",
            "{'loss': 0.105, 'grad_norm': 0.8572043776512146, 'learning_rate': 9.996063610703137e-05, 'epoch': 0.04}\n",
            "{'loss': 0.1016, 'grad_norm': 0.7347209453582764, 'learning_rate': 9.994642390694308e-05, 'epoch': 0.05}\n",
            "{'loss': 0.1245, 'grad_norm': 0.9010230898857117, 'learning_rate': 9.993002688846913e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0755, 'grad_norm': 0.4703925848007202, 'learning_rate': 9.991144576886823e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0965, 'grad_norm': 1.1002635955810547, 'learning_rate': 9.989068136093873e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0457, 'grad_norm': 0.8649870157241821, 'learning_rate': 9.986773457298311e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0934, 'grad_norm': 1.4223177433013916, 'learning_rate': 9.984260640876821e-05, 'epoch': 0.05}\n",
            "{'loss': 0.1227, 'grad_norm': 0.820125937461853, 'learning_rate': 9.981529796748134e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0802, 'grad_norm': 0.4621414542198181, 'learning_rate': 9.97858104436822e-05, 'epoch': 0.06}\n",
            "{'loss': 0.1101, 'grad_norm': 0.4734655022621155, 'learning_rate': 9.975414512725057e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0705, 'grad_norm': 1.0662933588027954, 'learning_rate': 9.972030340333001e-05, 'epoch': 0.06}\n",
            "{'loss': 0.1021, 'grad_norm': 1.0705125331878662, 'learning_rate': 9.968428675226714e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0617, 'grad_norm': 0.9304569959640503, 'learning_rate': 9.964609674954696e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0865, 'grad_norm': 1.7060140371322632, 'learning_rate': 9.96057350657239e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0773, 'grad_norm': 1.5241564512252808, 'learning_rate': 9.956320346634876e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0658, 'grad_norm': 0.7551470398902893, 'learning_rate': 9.95185038118915e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0655, 'grad_norm': 1.5651572942733765, 'learning_rate': 9.94716380576598e-05, 'epoch': 0.07}\n",
            "{'loss': 0.1093, 'grad_norm': 0.8021873831748962, 'learning_rate': 9.942260825371358e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0668, 'grad_norm': 0.6365687251091003, 'learning_rate': 9.937141654477528e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0592, 'grad_norm': 0.36067837476730347, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0699, 'grad_norm': 0.3191044330596924, 'learning_rate': 9.926255646355804e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0328, 'grad_norm': 0.5220887660980225, 'learning_rate': 9.92048928531717e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0963, 'grad_norm': 0.9416165351867676, 'learning_rate': 9.914507686137019e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0599, 'grad_norm': 0.827949583530426, 'learning_rate': 9.90831111046988e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0761, 'grad_norm': 0.722559928894043, 'learning_rate': 9.901899829374047e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0729, 'grad_norm': 0.5834609866142273, 'learning_rate': 9.895274123299723e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0702, 'grad_norm': 0.9025991559028625, 'learning_rate': 9.888434282076758e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0849, 'grad_norm': 1.2471132278442383, 'learning_rate': 9.881380604901964e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0799, 'grad_norm': 0.8518099188804626, 'learning_rate': 9.87411340032603e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0733, 'grad_norm': 1.0887503623962402, 'learning_rate': 9.86663298624003e-05, 'epoch': 0.09}\n",
            "{'loss': 0.1074, 'grad_norm': 1.3685715198516846, 'learning_rate': 9.858939689861506e-05, 'epoch': 0.09}\n",
            "{'loss': 0.063, 'grad_norm': 1.0198112726211548, 'learning_rate': 9.851033847720166e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0663, 'grad_norm': 1.0899956226348877, 'learning_rate': 9.842915805643155e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0596, 'grad_norm': 0.278604120016098, 'learning_rate': 9.834585918739936e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0953, 'grad_norm': 0.8197522759437561, 'learning_rate': 9.826044551386744e-05, 'epoch': 0.09}\n",
            "{'loss': 0.1016, 'grad_norm': 1.1822235584259033, 'learning_rate': 9.817292077210659e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0557, 'grad_norm': 0.7194699048995972, 'learning_rate': 9.808328879073251e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0714, 'grad_norm': 1.1863454580307007, 'learning_rate': 9.799155349053851e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0442, 'grad_norm': 0.5627518892288208, 'learning_rate': 9.789771888432375e-05, 'epoch': 0.1}\n",
            "{'loss': 0.1062, 'grad_norm': 0.6740462779998779, 'learning_rate': 9.780178907671789e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0746, 'grad_norm': 0.8841698169708252, 'learning_rate': 9.77037682640015e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0358, 'grad_norm': 0.41790342330932617, 'learning_rate': 9.760366073392246e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0777, 'grad_norm': 0.9394013285636902, 'learning_rate': 9.750147086550844e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0742, 'grad_norm': 0.3511839509010315, 'learning_rate': 9.739720312887535e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0823, 'grad_norm': 0.5145900845527649, 'learning_rate': 9.729086208503174e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0657, 'grad_norm': 1.1476749181747437, 'learning_rate': 9.718245238567939e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0508, 'grad_norm': 0.6792407035827637, 'learning_rate': 9.707197877300974e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0935, 'grad_norm': 1.3700227737426758, 'learning_rate': 9.695944607949649e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0423, 'grad_norm': 0.6070777177810669, 'learning_rate': 9.684485922768422e-05, 'epoch': 0.11}\n",
            "{'loss': 0.088, 'grad_norm': 1.0691553354263306, 'learning_rate': 9.672822322997305e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0739, 'grad_norm': 1.2840651273727417, 'learning_rate': 9.660954318839933e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0703, 'grad_norm': 0.6934924721717834, 'learning_rate': 9.648882429441257e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0519, 'grad_norm': 0.8730550408363342, 'learning_rate': 9.636607182864827e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0527, 'grad_norm': 0.750953197479248, 'learning_rate': 9.624129116069694e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0929, 'grad_norm': 1.4306397438049316, 'learning_rate': 9.611448774886924e-05, 'epoch': 0.12}\n",
            "{'loss': 0.066, 'grad_norm': 0.9554431438446045, 'learning_rate': 9.598566713995718e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0868, 'grad_norm': 0.44475626945495605, 'learning_rate': 9.58548349689915e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0603, 'grad_norm': 0.9864271283149719, 'learning_rate': 9.572199695899522e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0604, 'grad_norm': 0.6103668212890625, 'learning_rate': 9.558715892073323e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0547, 'grad_norm': 0.8111691474914551, 'learning_rate': 9.545032675245813e-05, 'epoch': 0.13}\n",
            "{'loss': 0.062, 'grad_norm': 0.4493682384490967, 'learning_rate': 9.531150643965223e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0797, 'grad_norm': 1.0769944190979004, 'learning_rate': 9.517070405476575e-05, 'epoch': 0.13}\n",
            "{'loss': 0.057, 'grad_norm': 0.4469873607158661, 'learning_rate': 9.502792575695112e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0575, 'grad_norm': 0.6679672002792358, 'learning_rate': 9.488317779179361e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0566, 'grad_norm': 0.5918867588043213, 'learning_rate': 9.473646649103818e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0573, 'grad_norm': 0.5217810273170471, 'learning_rate': 9.458779827231237e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0592, 'grad_norm': 0.27201008796691895, 'learning_rate': 9.443717963884569e-05, 'epoch': 0.14}\n",
            "{'loss': 0.067, 'grad_norm': 0.8812028765678406, 'learning_rate': 9.428461717918511e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0797, 'grad_norm': 2.021655559539795, 'learning_rate': 9.413011756690685e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0547, 'grad_norm': 0.9120716452598572, 'learning_rate': 9.397368756032445e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0538, 'grad_norm': 1.2938907146453857, 'learning_rate': 9.381533400219318e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0452, 'grad_norm': 0.7651987075805664, 'learning_rate': 9.365506381941066e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0655, 'grad_norm': 0.6353893280029297, 'learning_rate': 9.349288402271388e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0827, 'grad_norm': 1.3328028917312622, 'learning_rate': 9.332880170637252e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0694, 'grad_norm': 1.1975091695785522, 'learning_rate': 9.316282404787871e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0671, 'grad_norm': 1.2299941778182983, 'learning_rate': 9.299495830763286e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0434, 'grad_norm': 0.4351477026939392, 'learning_rate': 9.282521182862629e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0292, 'grad_norm': 0.3640119135379791, 'learning_rate': 9.265359203611987e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0628, 'grad_norm': 1.1321929693222046, 'learning_rate': 9.248010643731935e-05, 'epoch': 0.16}\n",
            "{'loss': 0.087, 'grad_norm': 0.3620963394641876, 'learning_rate': 9.230476262104677e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0689, 'grad_norm': 0.6639105081558228, 'learning_rate': 9.212756825740873e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0582, 'grad_norm': 1.0647705793380737, 'learning_rate': 9.194853109746074e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0471, 'grad_norm': 0.5998029708862305, 'learning_rate': 9.176765897286813e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0388, 'grad_norm': 0.33947592973709106, 'learning_rate': 9.158495979556358e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0514, 'grad_norm': 0.6200560927391052, 'learning_rate': 9.140044155740101e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0553, 'grad_norm': 0.8152250051498413, 'learning_rate': 9.121411232980588e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0733, 'grad_norm': 1.0878220796585083, 'learning_rate': 9.102598026342222e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0553, 'grad_norm': 0.7447304725646973, 'learning_rate': 9.083605358775612e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0601, 'grad_norm': 0.5007872581481934, 'learning_rate': 9.064434061081562e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0671, 'grad_norm': 0.44653499126434326, 'learning_rate': 9.045084971874738e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0504, 'grad_norm': 0.5285784006118774, 'learning_rate': 9.025558937546988e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0499, 'grad_norm': 0.7279407978057861, 'learning_rate': 9.005856812230304e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0724, 'grad_norm': 0.4684208929538727, 'learning_rate': 8.98597945775948e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0458, 'grad_norm': 0.5698432326316833, 'learning_rate': 8.965927743634391e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0516, 'grad_norm': 0.5243446230888367, 'learning_rate': 8.945702546981969e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0431, 'grad_norm': 0.8723600506782532, 'learning_rate': 8.92530475251784e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0515, 'grad_norm': 0.8957272171974182, 'learning_rate': 8.90473525250761e-05, 'epoch': 0.18}\n",
            "{'loss': 0.061, 'grad_norm': 0.6828311085700989, 'learning_rate': 8.883994946727849e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0738, 'grad_norm': 1.984514594078064, 'learning_rate': 8.863084742426719e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0593, 'grad_norm': 0.6947140693664551, 'learning_rate': 8.842005554284296e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0495, 'grad_norm': 0.4703863561153412, 'learning_rate': 8.820758304372557e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0672, 'grad_norm': 0.8802976608276367, 'learning_rate': 8.799343922115044e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0451, 'grad_norm': 0.4074244797229767, 'learning_rate': 8.77776334424621e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0693, 'grad_norm': 1.264402985572815, 'learning_rate': 8.756017514770443e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0398, 'grad_norm': 0.44469544291496277, 'learning_rate': 8.73410738492077e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0665, 'grad_norm': 0.7194556593894958, 'learning_rate': 8.71203391311725e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0528, 'grad_norm': 0.5734315514564514, 'learning_rate': 8.689798064925049e-05, 'epoch': 0.2}\n",
            "{'loss': 0.042, 'grad_norm': 0.46887868642807007, 'learning_rate': 8.6674008130122e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0488, 'grad_norm': 0.6980953812599182, 'learning_rate': 8.644843137107059e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0592, 'grad_norm': 0.47407642006874084, 'learning_rate': 8.622126023955446e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0378, 'grad_norm': 0.3455584645271301, 'learning_rate': 8.599250467277483e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0523, 'grad_norm': 0.3695380985736847, 'learning_rate': 8.576217467724128e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0435, 'grad_norm': 1.4195470809936523, 'learning_rate': 8.553028032833397e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0494, 'grad_norm': 0.32937029004096985, 'learning_rate': 8.529683176986295e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0438, 'grad_norm': 0.729784369468689, 'learning_rate': 8.506183921362443e-05, 'epoch': 0.21}\n",
            "{'loss': 0.089, 'grad_norm': 1.5468716621398926, 'learning_rate': 8.482531293895412e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0387, 'grad_norm': 0.9382857084274292, 'learning_rate': 8.458726329227747e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0423, 'grad_norm': 0.312355101108551, 'learning_rate': 8.434770068665723e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0509, 'grad_norm': 1.9176603555679321, 'learning_rate': 8.410663560133784e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0391, 'grad_norm': 0.3217453062534332, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0362, 'grad_norm': 0.5607755184173584, 'learning_rate': 8.362004023673474e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0337, 'grad_norm': 0.3631984293460846, 'learning_rate': 8.337453124270863e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0373, 'grad_norm': 0.347001314163208, 'learning_rate': 8.31275623385675e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0514, 'grad_norm': 0.2863647937774658, 'learning_rate': 8.287914432753123e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0745, 'grad_norm': 0.7173418998718262, 'learning_rate': 8.262928807620843e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0676, 'grad_norm': 0.24432311952114105, 'learning_rate': 8.237800451412095e-05, 'epoch': 0.22}\n",
            "{'loss': 0.072, 'grad_norm': 0.42547178268432617, 'learning_rate': 8.212530463322583e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0381, 'grad_norm': 0.2883872389793396, 'learning_rate': 8.18711994874345e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0521, 'grad_norm': 0.7246316075325012, 'learning_rate': 8.161570019212921e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0386, 'grad_norm': 0.8019322156906128, 'learning_rate': 8.135881792367686e-05, 'epoch': 0.23}\n",
            "{'loss': 0.034, 'grad_norm': 0.48611336946487427, 'learning_rate': 8.110056391894005e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0411, 'grad_norm': 0.5754430294036865, 'learning_rate': 8.084094947478556e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0538, 'grad_norm': 0.35005924105644226, 'learning_rate': 8.057998594759022e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0647, 'grad_norm': 1.3574318885803223, 'learning_rate': 8.031768475274413e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0745, 'grad_norm': 1.1192355155944824, 'learning_rate': 8.005405736415126e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0378, 'grad_norm': 0.6656318306922913, 'learning_rate': 7.978911531372765e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0432, 'grad_norm': 0.4038100242614746, 'learning_rate': 7.952287019089685e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0434, 'grad_norm': 0.7894827127456665, 'learning_rate': 7.925533364208309e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0334, 'grad_norm': 0.6310957074165344, 'learning_rate': 7.898651737020166e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0535, 'grad_norm': 0.674538791179657, 'learning_rate': 7.871643313414718e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0315, 'grad_norm': 0.31149908900260925, 'learning_rate': 7.844509274827907e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0781, 'grad_norm': 0.4215116798877716, 'learning_rate': 7.817250808190483e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0712, 'grad_norm': 0.818939745426178, 'learning_rate': 7.789869105876083e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0577, 'grad_norm': 0.9365382194519043, 'learning_rate': 7.762365365649067e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0454, 'grad_norm': 0.3371772766113281, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0415, 'grad_norm': 0.5791118144989014, 'learning_rate': 7.70699658915369e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0425, 'grad_norm': 0.8970719575881958, 'learning_rate': 7.679133974894983e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0487, 'grad_norm': 0.3859212398529053, 'learning_rate': 7.651154166637025e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0371, 'grad_norm': 0.3161240816116333, 'learning_rate': 7.623058388307269e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0423, 'grad_norm': 0.428288996219635, 'learning_rate': 7.594847868906076e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0417, 'grad_norm': 0.5459276437759399, 'learning_rate': 7.566523842452958e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0343, 'grad_norm': 0.36099639534950256, 'learning_rate': 7.538087547932585e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0432, 'grad_norm': 0.7690373659133911, 'learning_rate': 7.509540229240601e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0438, 'grad_norm': 0.4586021304130554, 'learning_rate': 7.480883135129211e-05, 'epoch': 0.26}\n",
            "{'loss': 0.049, 'grad_norm': 0.5540937781333923, 'learning_rate': 7.452117519152542e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0592, 'grad_norm': 1.309579610824585, 'learning_rate': 7.423244639611826e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0761, 'grad_norm': 0.506648600101471, 'learning_rate': 7.394265759500348e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0309, 'grad_norm': 0.4203230142593384, 'learning_rate': 7.365182146448205e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0416, 'grad_norm': 0.5013275146484375, 'learning_rate': 7.335995072666848e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0373, 'grad_norm': 0.5750855207443237, 'learning_rate': 7.30670581489344e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0723, 'grad_norm': 0.8129166960716248, 'learning_rate': 7.277315654334997e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0452, 'grad_norm': 0.7111604809761047, 'learning_rate': 7.247825876612353e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0525, 'grad_norm': 0.41470274329185486, 'learning_rate': 7.218237771703921e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0435, 'grad_norm': 0.6978652477264404, 'learning_rate': 7.188552633889259e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0367, 'grad_norm': 0.5613283514976501, 'learning_rate': 7.158771761692464e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0293, 'grad_norm': 0.2369542121887207, 'learning_rate': 7.128896457825364e-05, 'epoch': 0.28}\n",
            "{'loss': 0.043, 'grad_norm': 0.979820728302002, 'learning_rate': 7.09892802913053e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0311, 'grad_norm': 0.6591001152992249, 'learning_rate': 7.068867786524116e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0678, 'grad_norm': 0.8683773279190063, 'learning_rate': 7.038717044938519e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0389, 'grad_norm': 0.7235487699508667, 'learning_rate': 7.008477123264848e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0354, 'grad_norm': 0.529984176158905, 'learning_rate': 6.978149344295242e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0517, 'grad_norm': 0.3791074752807617, 'learning_rate': 6.947735034665002e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0523, 'grad_norm': 1.6368052959442139, 'learning_rate': 6.917235524794558e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0453, 'grad_norm': 0.5544537305831909, 'learning_rate': 6.886652148831279e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0408, 'grad_norm': 0.7442135810852051, 'learning_rate': 6.855986244591104e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0454, 'grad_norm': 0.4492416977882385, 'learning_rate': 6.825239153500029e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0359, 'grad_norm': 0.3822805881500244, 'learning_rate': 6.794412220535426e-05, 'epoch': 0.29}\n",
            "{'loss': 0.035, 'grad_norm': 0.4567802846431732, 'learning_rate': 6.763506794167208e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0366, 'grad_norm': 0.7721964716911316, 'learning_rate': 6.732524226298841e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0472, 'grad_norm': 0.4648665189743042, 'learning_rate': 6.701465872208216e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0377, 'grad_norm': 0.7821949124336243, 'learning_rate': 6.670333090488356e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0425, 'grad_norm': 0.4190770387649536, 'learning_rate': 6.639127242987988e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0564, 'grad_norm': 0.37050989270210266, 'learning_rate': 6.607849694751977e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0487, 'grad_norm': 0.4343629479408264, 'learning_rate': 6.576501813961609e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0371, 'grad_norm': 0.5447453260421753, 'learning_rate': 6.545084971874738e-05, 'epoch': 0.31}\n",
            "{'loss': 0.07, 'grad_norm': 0.7808027267456055, 'learning_rate': 6.513600542765817e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0413, 'grad_norm': 0.4574684500694275, 'learning_rate': 6.48204990386577e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0446, 'grad_norm': 0.7899447679519653, 'learning_rate': 6.450434435301751e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0431, 'grad_norm': 0.6373552083969116, 'learning_rate': 6.418755520036775e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0321, 'grad_norm': 0.6157373189926147, 'learning_rate': 6.387014543809223e-05, 'epoch': 0.31}\n",
            "{'loss': 0.1748, 'grad_norm': 0.4988804757595062, 'learning_rate': 6.355212895072223e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0507, 'grad_norm': 0.4902857542037964, 'learning_rate': 6.323351964932908e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0618, 'grad_norm': 0.588800311088562, 'learning_rate': 6.291433147091583e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0334, 'grad_norm': 0.4993416368961334, 'learning_rate': 6.259457837780742e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0405, 'grad_norm': 0.5793545842170715, 'learning_rate': 6.227427435703997e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0477, 'grad_norm': 0.47062769532203674, 'learning_rate': 6.195343341974899e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0586, 'grad_norm': 0.6622616648674011, 'learning_rate': 6.163206960055651e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0698, 'grad_norm': 1.0456619262695312, 'learning_rate': 6.131019695695702e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0439, 'grad_norm': 0.8064051866531372, 'learning_rate': 6.0987829568702656e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0602, 'grad_norm': 0.4649011194705963, 'learning_rate': 6.066498153718735e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0351, 'grad_norm': 0.5199092030525208, 'learning_rate': 6.034166698482984e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0498, 'grad_norm': 0.37297308444976807, 'learning_rate': 6.001790005445607e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0325, 'grad_norm': 0.737527072429657, 'learning_rate': 5.969369490868042e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0477, 'grad_norm': 0.5845549702644348, 'learning_rate': 5.9369065729286245e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0451, 'grad_norm': 0.6898839473724365, 'learning_rate': 5.90440267166055e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0307, 'grad_norm': 0.7462940216064453, 'learning_rate': 5.871859208889759e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0359, 'grad_norm': 0.5581718683242798, 'learning_rate': 5.8392776081727385e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0356, 'grad_norm': 1.009968638420105, 'learning_rate': 5.8066592947342555e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0528, 'grad_norm': 0.2719227075576782, 'learning_rate': 5.7740056954050084e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0341, 'grad_norm': 0.38170745968818665, 'learning_rate': 5.74131823855921e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0589, 'grad_norm': 0.7513880133628845, 'learning_rate': 5.7085983540521216e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0427, 'grad_norm': 0.8912397623062134, 'learning_rate': 5.675847473157485e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0396, 'grad_norm': 0.2839777171611786, 'learning_rate': 5.6430670285049314e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0496, 'grad_norm': 0.9667369723320007, 'learning_rate': 5.6102584540173006e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0547, 'grad_norm': 0.6554617285728455, 'learning_rate': 5.577423184847932e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0379, 'grad_norm': 0.8156272172927856, 'learning_rate': 5.544562657317863e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0407, 'grad_norm': 0.5507166385650635, 'learning_rate': 5.511678308853026e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0371, 'grad_norm': 0.4169680178165436, 'learning_rate': 5.478771577921351e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0478, 'grad_norm': 0.25400644540786743, 'learning_rate': 5.445843903969854e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0446, 'grad_norm': 0.5393954515457153, 'learning_rate': 5.4128967273616625e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0392, 'grad_norm': 0.26119184494018555, 'learning_rate': 5.379931489313016e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0521, 'grad_norm': 0.4397520422935486, 'learning_rate': 5.3469496318302204e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0537, 'grad_norm': 0.6997697353363037, 'learning_rate': 5.313952597646568e-05, 'epoch': 0.36}\n",
            "{'loss': 0.038, 'grad_norm': 0.47686266899108887, 'learning_rate': 5.280941830159227e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0435, 'grad_norm': 0.6883424520492554, 'learning_rate': 5.247918773366112e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0308, 'grad_norm': 0.7557028532028198, 'learning_rate': 5.214884871802703e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0577, 'grad_norm': 0.6049020290374756, 'learning_rate': 5.1818415704788725e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0248, 'grad_norm': 0.5928959846496582, 'learning_rate': 5.148790314815663e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0396, 'grad_norm': 0.8550077676773071, 'learning_rate': 5.1157325505820694e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0263, 'grad_norm': 0.29962313175201416, 'learning_rate': 5.0826697238317935e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0316, 'grad_norm': 0.9034901857376099, 'learning_rate': 5.0496032808399815e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0242, 'grad_norm': 0.443377822637558, 'learning_rate': 5.016534668039976e-05, 'epoch': 0.37}\n",
            "{'loss': 0.028, 'grad_norm': 0.278978556394577, 'learning_rate': 4.9834653319600246e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0316, 'grad_norm': 0.47553661465644836, 'learning_rate': 4.950396719160018e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0366, 'grad_norm': 0.3516263961791992, 'learning_rate': 4.917330276168208e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0226, 'grad_norm': 0.48021388053894043, 'learning_rate': 4.884267449417931e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0229, 'grad_norm': 0.4315895438194275, 'learning_rate': 4.851209685184338e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0309, 'grad_norm': 0.1337786465883255, 'learning_rate': 4.818158429521129e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0318, 'grad_norm': 0.30845504999160767, 'learning_rate': 4.785115128197298e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0392, 'grad_norm': 0.20959830284118652, 'learning_rate': 4.7520812266338885e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0247, 'grad_norm': 0.9533453583717346, 'learning_rate': 4.7190581698407725e-05, 'epoch': 0.39}\n",
            "{'loss': 0.037, 'grad_norm': 0.4128059446811676, 'learning_rate': 4.6860474023534335e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0211, 'grad_norm': 0.4853571057319641, 'learning_rate': 4.65305036816978e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0236, 'grad_norm': 0.5485201478004456, 'learning_rate': 4.620068510686985e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0311, 'grad_norm': 0.20272743701934814, 'learning_rate': 4.5871032726383386e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0386, 'grad_norm': 0.5814071893692017, 'learning_rate': 4.554156096030149e-05, 'epoch': 0.39}\n",
            "{'loss': 0.026, 'grad_norm': 0.30177798867225647, 'learning_rate': 4.5212284220786494e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0206, 'grad_norm': 0.34947070479393005, 'learning_rate': 4.488321691146975e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0228, 'grad_norm': 0.298837810754776, 'learning_rate': 4.4554373426821374e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0223, 'grad_norm': 0.14594118297100067, 'learning_rate': 4.4225768151520694e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0361, 'grad_norm': 0.47475916147232056, 'learning_rate': 4.3897415459827e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0381, 'grad_norm': 0.7271401286125183, 'learning_rate': 4.3569329714950704e-05, 'epoch': 0.4}\n",
            "{'loss': 0.025, 'grad_norm': 0.1476742923259735, 'learning_rate': 4.324152526842517e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0484, 'grad_norm': 0.6794809699058533, 'learning_rate': 4.291401645947879e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0246, 'grad_norm': 0.7103285789489746, 'learning_rate': 4.2586817614407895e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0258, 'grad_norm': 0.5138121843338013, 'learning_rate': 4.2259943045949934e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0232, 'grad_norm': 0.45435789227485657, 'learning_rate': 4.1933407052657456e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0309, 'grad_norm': 0.6395657062530518, 'learning_rate': 4.160722391827262e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0489, 'grad_norm': 0.2363988608121872, 'learning_rate': 4.1281407911102425e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0389, 'grad_norm': 0.493447870016098, 'learning_rate': 4.095597328339452e-05, 'epoch': 0.41}\n",
            "{'loss': 0.058, 'grad_norm': 0.5332230925559998, 'learning_rate': 4.063093427071376e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0177, 'grad_norm': 0.1742551177740097, 'learning_rate': 4.0306305091319595e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0391, 'grad_norm': 0.7176623344421387, 'learning_rate': 3.9982099945543945e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0338, 'grad_norm': 0.4049190878868103, 'learning_rate': 3.965833301517017e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0349, 'grad_norm': 0.246448814868927, 'learning_rate': 3.933501846281267e-05, 'epoch': 0.42}\n",
            "{'loss': 0.02, 'grad_norm': 0.4495603144168854, 'learning_rate': 3.901217043129735e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0221, 'grad_norm': 0.2975108027458191, 'learning_rate': 3.8689803043043e-05, 'epoch': 0.42}\n",
            "{'loss': 0.034, 'grad_norm': 0.7131044864654541, 'learning_rate': 3.836793039944349e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0404, 'grad_norm': 0.24895547330379486, 'learning_rate': 3.8046566580251e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0363, 'grad_norm': 0.2474958300590515, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.43}\n",
            "{'loss': 0.027, 'grad_norm': 0.3053233027458191, 'learning_rate': 3.74054216221926e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0273, 'grad_norm': 0.20222540199756622, 'learning_rate': 3.7085668529084184e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0257, 'grad_norm': 0.6678858995437622, 'learning_rate': 3.676648035067093e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0487, 'grad_norm': 0.6138326525688171, 'learning_rate': 3.6447871049277796e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0118, 'grad_norm': 0.43011388182640076, 'learning_rate': 3.612985456190778e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0345, 'grad_norm': 0.25065386295318604, 'learning_rate': 3.581244479963225e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0219, 'grad_norm': 0.34455329179763794, 'learning_rate': 3.5495655646982505e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0245, 'grad_norm': 0.39229273796081543, 'learning_rate': 3.517950096134232e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0319, 'grad_norm': 0.5760024189949036, 'learning_rate': 3.4863994572341843e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0272, 'grad_norm': 0.38560959696769714, 'learning_rate': 3.4549150281252636e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0369, 'grad_norm': 0.8337284326553345, 'learning_rate': 3.423498186038393e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0194, 'grad_norm': 0.27074798941612244, 'learning_rate': 3.392150305248024e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0253, 'grad_norm': 0.34104856848716736, 'learning_rate': 3.360872757012011e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0188, 'grad_norm': 0.520075798034668, 'learning_rate': 3.329666909511645e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0243, 'grad_norm': 0.5396057963371277, 'learning_rate': 3.298534127791785e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0388, 'grad_norm': 0.3635886609554291, 'learning_rate': 3.267475773701161e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0253, 'grad_norm': 0.3855355381965637, 'learning_rate': 3.236493205832795e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0279, 'grad_norm': 0.35100290179252625, 'learning_rate': 3.205587779464576e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0421, 'grad_norm': 0.44363948702812195, 'learning_rate': 3.1747608464999725e-05, 'epoch': 0.45}\n",
            "{'loss': 0.022, 'grad_norm': 0.348655641078949, 'learning_rate': 3.144013755408895e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0206, 'grad_norm': 1.5225354433059692, 'learning_rate': 3.113347851168721e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0194, 'grad_norm': 0.3424755334854126, 'learning_rate': 3.082764475205442e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0345, 'grad_norm': 0.4688204228878021, 'learning_rate': 3.052264965335e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0195, 'grad_norm': 0.26170769333839417, 'learning_rate': 3.0218506557047598e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0396, 'grad_norm': 0.3690344989299774, 'learning_rate': 2.991522876735154e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0706, 'grad_norm': 0.5594027638435364, 'learning_rate': 2.9612829550614836e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0309, 'grad_norm': 0.2697688639163971, 'learning_rate': 2.931132213475884e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0353, 'grad_norm': 0.21515199542045593, 'learning_rate': 2.9010719708694722e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0417, 'grad_norm': 0.7191771864891052, 'learning_rate': 2.8711035421746367e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0365, 'grad_norm': 0.30914798378944397, 'learning_rate': 2.8412282383075363e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0244, 'grad_norm': 0.3331865966320038, 'learning_rate': 2.811447366110741e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0217, 'grad_norm': 0.3448178470134735, 'learning_rate': 2.7817622282960815e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0291, 'grad_norm': 0.30659428238868713, 'learning_rate': 2.7521741233876496e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0206, 'grad_norm': 0.33577218651771545, 'learning_rate': 2.7226843456650037e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0357, 'grad_norm': 0.15014322102069855, 'learning_rate': 2.693294185106562e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0154, 'grad_norm': 0.1593938171863556, 'learning_rate': 2.6640049273331515e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0102, 'grad_norm': 0.18995581567287445, 'learning_rate': 2.6348178535517966e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0213, 'grad_norm': 0.4935462176799774, 'learning_rate': 2.6057342404996522e-05, 'epoch': 0.48}\n",
            "{'loss': 0.023, 'grad_norm': 0.3004457950592041, 'learning_rate': 2.5767553603881767e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0364, 'grad_norm': 0.2679193615913391, 'learning_rate': 2.547882480847461e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0273, 'grad_norm': 0.5465593934059143, 'learning_rate': 2.5191168648707887e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0414, 'grad_norm': 0.16962426900863647, 'learning_rate': 2.490459770759398e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0288, 'grad_norm': 0.40941324830055237, 'learning_rate': 2.4619124520674146e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0347, 'grad_norm': 0.48404404520988464, 'learning_rate': 2.433476157547044e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0356, 'grad_norm': 0.35192957520484924, 'learning_rate': 2.405152131093926e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0232, 'grad_norm': 0.419779509305954, 'learning_rate': 2.3769416116927335e-05, 'epoch': 0.49}\n",
            "{'loss': 0.024, 'grad_norm': 0.23278547823429108, 'learning_rate': 2.3488458333629777e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0229, 'grad_norm': 0.4024643898010254, 'learning_rate': 2.3208660251050158e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0338, 'grad_norm': 1.326277494430542, 'learning_rate': 2.29300341084631e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0257, 'grad_norm': 0.4570125639438629, 'learning_rate': 2.2652592093878666e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0344, 'grad_norm': 0.659003496170044, 'learning_rate': 2.237634634350934e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0268, 'grad_norm': 0.680294930934906, 'learning_rate': 2.2101308941239203e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0163, 'grad_norm': 0.32166242599487305, 'learning_rate': 2.182749191809518e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0431, 'grad_norm': 0.734305202960968, 'learning_rate': 2.1554907251720945e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0444, 'grad_norm': 0.5953511595726013, 'learning_rate': 2.128356686585282e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0441, 'grad_norm': 0.3924506902694702, 'learning_rate': 2.1013482629798333e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0419, 'grad_norm': 1.1429640054702759, 'learning_rate': 2.0744666357916925e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0313, 'grad_norm': 0.7892730236053467, 'learning_rate': 2.0477129809103147e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0316, 'grad_norm': 0.4657488167285919, 'learning_rate': 2.0210884686272368e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0213, 'grad_norm': 0.36459511518478394, 'learning_rate': 1.9945942635848748e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0447, 'grad_norm': 0.5202158689498901, 'learning_rate': 1.9682315247255894e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0272, 'grad_norm': 0.19028709828853607, 'learning_rate': 1.942001405240979e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0265, 'grad_norm': 0.43247416615486145, 'learning_rate': 1.9159050525214452e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0382, 'grad_norm': 0.5452983379364014, 'learning_rate': 1.8899436081059975e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0235, 'grad_norm': 0.13559524714946747, 'learning_rate': 1.8641182076323148e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0546, 'grad_norm': 0.6078236103057861, 'learning_rate': 1.838429980787081e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0332, 'grad_norm': 0.48742446303367615, 'learning_rate': 1.8128800512565513e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0178, 'grad_norm': 0.6929879784584045, 'learning_rate': 1.787469536677419e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0369, 'grad_norm': 0.5871888995170593, 'learning_rate': 1.7621995485879062e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0305, 'grad_norm': 0.20785143971443176, 'learning_rate': 1.7370711923791567e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0256, 'grad_norm': 0.5019211173057556, 'learning_rate': 1.712085567246878e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0394, 'grad_norm': 0.7442824244499207, 'learning_rate': 1.6872437661432517e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0276, 'grad_norm': 0.3165775239467621, 'learning_rate': 1.662546875729138e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0257, 'grad_norm': 0.4211536645889282, 'learning_rate': 1.637995976326527e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0152, 'grad_norm': 0.2120390683412552, 'learning_rate': 1.6135921418712956e-05, 'epoch': 0.53}\n",
            "{'loss': 0.031, 'grad_norm': 0.512189507484436, 'learning_rate': 1.5893364398662176e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0354, 'grad_norm': 0.4319930672645569, 'learning_rate': 1.5652299313342773e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0304, 'grad_norm': 1.3390318155288696, 'learning_rate': 1.5412736707722537e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0272, 'grad_norm': 0.40825921297073364, 'learning_rate': 1.517468706104589e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0248, 'grad_norm': 0.33916205167770386, 'learning_rate': 1.4938160786375572e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0202, 'grad_norm': 0.27369847893714905, 'learning_rate': 1.470316823013707e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0351, 'grad_norm': 0.35671260952949524, 'learning_rate': 1.4469719671666043e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0389, 'grad_norm': 0.5314967632293701, 'learning_rate': 1.4237825322758736e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0277, 'grad_norm': 0.3134046196937561, 'learning_rate': 1.4007495327225162e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0191, 'grad_norm': 0.11990012228488922, 'learning_rate': 1.3778739760445552e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0212, 'grad_norm': 0.3284377157688141, 'learning_rate': 1.3551568628929434e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0294, 'grad_norm': 0.42332762479782104, 'learning_rate': 1.3325991869878013e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0301, 'grad_norm': 0.5906912684440613, 'learning_rate': 1.3102019350749528e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0247, 'grad_norm': 0.34583020210266113, 'learning_rate': 1.2879660868827508e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0231, 'grad_norm': 0.3493465185165405, 'learning_rate': 1.2658926150792322e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0196, 'grad_norm': 0.43813157081604004, 'learning_rate': 1.243982485229559e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0191, 'grad_norm': 0.5519669055938721, 'learning_rate': 1.2222366557537911e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0163, 'grad_norm': 0.3351110517978668, 'learning_rate': 1.2006560778849578e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0136, 'grad_norm': 0.27015072107315063, 'learning_rate': 1.1792416956274444e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0327, 'grad_norm': 0.2876354157924652, 'learning_rate': 1.157994445715706e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0288, 'grad_norm': 0.1702524721622467, 'learning_rate': 1.1369152575732822e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0229, 'grad_norm': 0.6928739547729492, 'learning_rate': 1.1160050532721528e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0186, 'grad_norm': 0.5924605131149292, 'learning_rate': 1.095264747492391e-05, 'epoch': 0.57}\n",
            "{'loss': 0.031, 'grad_norm': 0.37923094630241394, 'learning_rate': 1.0746952474821614e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0181, 'grad_norm': 0.16090381145477295, 'learning_rate': 1.0542974530180327e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0228, 'grad_norm': 0.37862199544906616, 'learning_rate': 1.0340722563656107e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0407, 'grad_norm': 0.11809265613555908, 'learning_rate': 1.0140205422405214e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0199, 'grad_norm': 0.2844051122665405, 'learning_rate': 9.941431877696955e-06, 'epoch': 0.57}\n",
            "{'loss': 0.0407, 'grad_norm': 0.35841482877731323, 'learning_rate': 9.744410624530148e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0139, 'grad_norm': 0.1605997085571289, 'learning_rate': 9.549150281252633e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0346, 'grad_norm': 0.14499755203723907, 'learning_rate': 9.355659389184396e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0225, 'grad_norm': 0.4773518145084381, 'learning_rate': 9.163946412243896e-06, 'epoch': 0.58}\n",
            "{'loss': 0.038, 'grad_norm': 0.5182441473007202, 'learning_rate': 8.974019736577777e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0241, 'grad_norm': 0.48309797048568726, 'learning_rate': 8.785887670194138e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0335, 'grad_norm': 0.6883280873298645, 'learning_rate': 8.599558442598998e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0208, 'grad_norm': 0.5157297849655151, 'learning_rate': 8.415040204436426e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0177, 'grad_norm': 0.1631268709897995, 'learning_rate': 8.232341027131885e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0197, 'grad_norm': 0.3991020619869232, 'learning_rate': 8.051468902539272e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0203, 'grad_norm': 0.5842088460922241, 'learning_rate': 7.872431742591268e-06, 'epoch': 0.59}\n",
            "{'loss': 0.031, 'grad_norm': 0.12050333619117737, 'learning_rate': 7.695237378953223e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0193, 'grad_norm': 0.5891525149345398, 'learning_rate': 7.519893562680663e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0182, 'grad_norm': 0.271279901266098, 'learning_rate': 7.3464079638801365e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0364, 'grad_norm': 0.48489853739738464, 'learning_rate': 7.174788171373731e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0158, 'grad_norm': 0.40593355894088745, 'learning_rate': 7.005041692367154e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0282, 'grad_norm': 0.42719292640686035, 'learning_rate': 6.837175952121306e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0268, 'grad_norm': 0.30045759677886963, 'learning_rate': 6.671198293627479e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0425, 'grad_norm': 0.24119675159454346, 'learning_rate': 6.5071159772861436e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0247, 'grad_norm': 0.8249558210372925, 'learning_rate': 6.344936180589351e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0141, 'grad_norm': 0.19609907269477844, 'learning_rate': 6.184665997806832e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0245, 'grad_norm': 0.07940971106290817, 'learning_rate': 6.026312439675552e-06, 'epoch': 0.61}\n",
            "{'loss': 0.016, 'grad_norm': 0.3005876839160919, 'learning_rate': 5.869882433093155e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0151, 'grad_norm': 0.1708875596523285, 'learning_rate': 5.715382820814885e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0109, 'grad_norm': 0.2169555425643921, 'learning_rate': 5.562820361154314e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0326, 'grad_norm': 0.433243066072464, 'learning_rate': 5.412201727687644e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0187, 'grad_norm': 0.30023160576820374, 'learning_rate': 5.263533508961827e-06, 'epoch': 0.61}\n",
            "{'loss': 0.031, 'grad_norm': 0.28469106554985046, 'learning_rate': 5.116822208206396e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0263, 'grad_norm': 0.6013382077217102, 'learning_rate': 4.972074243048897e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0169, 'grad_norm': 0.3250264525413513, 'learning_rate': 4.829295945234258e-06, 'epoch': 0.62}\n",
            "{'loss': 0.018, 'grad_norm': 0.12254750728607178, 'learning_rate': 4.688493560347773e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0157, 'grad_norm': 0.43026071786880493, 'learning_rate': 4.549673247541875e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0276, 'grad_norm': 0.17741400003433228, 'learning_rate': 4.412841079266777e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0282, 'grad_norm': 0.447009801864624, 'learning_rate': 4.27800304100478e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0182, 'grad_norm': 0.32449033856391907, 'learning_rate': 4.145165031008508e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0295, 'grad_norm': 0.3558964431285858, 'learning_rate': 4.01433286004283e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0201, 'grad_norm': 0.07782962173223495, 'learning_rate': 3.885512251130763e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0567, 'grad_norm': 0.5772177577018738, 'learning_rate': 3.75870883930306e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0164, 'grad_norm': 0.1803242266178131, 'learning_rate': 3.6339281713517303e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0308, 'grad_norm': 0.15723560750484467, 'learning_rate': 3.511175705587433e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0227, 'grad_norm': 1.307934284210205, 'learning_rate': 3.390456811600673e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0203, 'grad_norm': 0.307120680809021, 'learning_rate': 3.271776770026963e-06, 'epoch': 0.63}\n",
            "{'loss': 0.021, 'grad_norm': 0.11574964225292206, 'learning_rate': 3.155140772315773e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0141, 'grad_norm': 0.3622455894947052, 'learning_rate': 3.040553920503503e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0168, 'grad_norm': 1.2734501361846924, 'learning_rate': 2.928021226990263e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0122, 'grad_norm': 0.17449454963207245, 'learning_rate': 2.817547614320615e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0355, 'grad_norm': 0.6323870420455933, 'learning_rate': 2.7091379149682685e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0229, 'grad_norm': 0.30779531598091125, 'learning_rate': 2.602796871124663e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0119, 'grad_norm': 0.20155338943004608, 'learning_rate': 2.4985291344915674e-06, 'epoch': 0.64}\n",
            "{'loss': 0.021, 'grad_norm': 0.4169122278690338, 'learning_rate': 2.3963392660775575e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0159, 'grad_norm': 0.3764108121395111, 'learning_rate': 2.296231735998511e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0151, 'grad_norm': 0.12023456394672394, 'learning_rate': 2.1982109232821178e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0267, 'grad_norm': 0.4186410903930664, 'learning_rate': 2.102281115676258e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0238, 'grad_norm': 0.3001558184623718, 'learning_rate': 2.008446509461498e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0248, 'grad_norm': 0.8708650469779968, 'learning_rate': 1.91671120926748e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0155, 'grad_norm': 0.5071313381195068, 'learning_rate': 1.8270792278934302e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0219, 'grad_norm': 0.6738083362579346, 'learning_rate': 1.7395544861325718e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0288, 'grad_norm': 0.32887962460517883, 'learning_rate': 1.6541408126006463e-06, 'epoch': 0.66}\n",
            "{'loss': 0.022, 'grad_norm': 0.46788662672042847, 'learning_rate': 1.5708419435684462e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0131, 'grad_norm': 0.18654324114322662, 'learning_rate': 1.4896615227983468e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0168, 'grad_norm': 0.3159922957420349, 'learning_rate': 1.4106031013849496e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0279, 'grad_norm': 0.4370347559452057, 'learning_rate': 1.333670137599713e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0286, 'grad_norm': 0.4280470311641693, 'learning_rate': 1.2588659967397e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0397, 'grad_norm': 0.7992679476737976, 'learning_rate': 1.1861939509803687e-06, 'epoch': 0.67}\n",
            "{'loss': 0.0156, 'grad_norm': 0.1786147952079773, 'learning_rate': 1.1156571792324211e-06, 'epoch': 0.67}\n",
            "{'loss': 0.0218, 'grad_norm': 0.2455267608165741, 'learning_rate': 1.0472587670027678e-06, 'epoch': 0.67}\n",
            "{'loss': 0.0188, 'grad_norm': 0.11630648374557495, 'learning_rate': 9.810017062595322e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0155, 'grad_norm': 0.2827771306037903, 'learning_rate': 9.168888953011989e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0243, 'grad_norm': 0.9605304598808289, 'learning_rate': 8.549231386298151e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0288, 'grad_norm': 0.4980810582637787, 'learning_rate': 7.951071468283167e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0267, 'grad_norm': 0.2819823622703552, 'learning_rate': 7.374435364419674e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0291, 'grad_norm': 0.4808177053928375, 'learning_rate': 6.819348298638839e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0181, 'grad_norm': 0.10907915979623795, 'learning_rate': 6.285834552247128e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0287, 'grad_norm': 0.1605813056230545, 'learning_rate': 5.773917462864264e-07, 'epoch': 0.68}\n",
            "{'loss': 0.017, 'grad_norm': 0.320207417011261, 'learning_rate': 5.283619423401998e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0305, 'grad_norm': 1.5205748081207275, 'learning_rate': 4.814961881085045e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0296, 'grad_norm': 0.1906607449054718, 'learning_rate': 4.367965336512403e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0156, 'grad_norm': 0.36878702044487, 'learning_rate': 3.9426493427611177e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0317, 'grad_norm': 0.6605727076530457, 'learning_rate': 3.5390325045304706e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0648, 'grad_norm': 0.43458643555641174, 'learning_rate': 3.157132477328628e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0255, 'grad_norm': 0.5180740356445312, 'learning_rate': 2.796965966699927e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0214, 'grad_norm': 0.8069599866867065, 'learning_rate': 2.458548727494292e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0182, 'grad_norm': 0.37787729501724243, 'learning_rate': 2.1418955631781202e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0295, 'grad_norm': 0.22905689477920532, 'learning_rate': 1.847020325186577e-07, 'epoch': 0.69}\n",
            "{'loss': 0.022, 'grad_norm': 0.3330804705619812, 'learning_rate': 1.5739359123178587e-07, 'epoch': 0.7}\n",
            "{'loss': 0.017, 'grad_norm': 0.13305144011974335, 'learning_rate': 1.3226542701689215e-07, 'epoch': 0.7}\n",
            "{'loss': 0.0183, 'grad_norm': 0.16539321839809418, 'learning_rate': 1.0931863906127327e-07, 'epoch': 0.7}\n",
            "{'loss': 0.0265, 'grad_norm': 0.19716691970825195, 'learning_rate': 8.855423113177664e-08, 'epoch': 0.7}\n",
            "{'loss': 0.0333, 'grad_norm': 0.1869381070137024, 'learning_rate': 6.997311153086883e-08, 'epoch': 0.7}\n",
            "{'loss': 0.027, 'grad_norm': 0.7507035136222839, 'learning_rate': 5.3576093056922906e-08, 'epoch': 0.7}\n",
            "{'loss': 0.0233, 'grad_norm': 1.0930581092834473, 'learning_rate': 3.936389296864129e-08, 'epoch': 0.7}\n",
            "{'loss': 0.0239, 'grad_norm': 0.3214176297187805, 'learning_rate': 2.7337132953697554e-08, 'epoch': 0.71}\n",
            "{'loss': 0.0502, 'grad_norm': 1.4576612710952759, 'learning_rate': 1.749633910153592e-08, 'epoch': 0.71}\n",
            "{'loss': 0.0227, 'grad_norm': 0.9918457269668579, 'learning_rate': 9.841941880361916e-09, 'epoch': 0.71}\n",
            "{'loss': 0.0356, 'grad_norm': 0.5073988437652588, 'learning_rate': 4.3742761183018784e-09, 'epoch': 0.71}\n",
            "{'loss': 0.0241, 'grad_norm': 0.2513270676136017, 'learning_rate': 1.0935809887702154e-09, 'epoch': 0.71}\n",
            "{'loss': 0.0283, 'grad_norm': 0.42173856496810913, 'learning_rate': 0.0, 'epoch': 0.71}\n",
            "{'train_runtime': 1773.093, 'train_samples_per_second': 11.28, 'train_steps_per_second': 2.82, 'train_loss': 0.0488430608138442, 'epoch': 0.71}\n",
            "100% 5000/5000 [29:33<00:00,  2.82it/s]\n",
            "\n",
            " GPU Memory (After Script End)\n",
            "Total: 39.56 GB | Reserved: 0.00 GB | Allocated: 0.00 GB | Free Reserved: 0.00 GB | Actual Free: 39.56 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "6aBSojxZT5Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/gr00t_finetuned_kuka.zip /content/gr00t_finetuned_kuka\n"
      ],
      "metadata": {
        "id": "KFi6UaRvAE-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25871314-4408-499e-f05f-6fee6dfff611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/gr00t_finetuned_kuka/ (stored 0%)\n",
            "  adding: content/gr00t_finetuned_kuka/config.json (deflated 58%)\n",
            "  adding: content/gr00t_finetuned_kuka/runs/ (stored 0%)\n",
            "  adding: content/gr00t_finetuned_kuka/runs/May10_09-21-54_d7f6549b8c05/ (stored 0%)\n",
            "  adding: content/gr00t_finetuned_kuka/runs/May10_09-21-54_d7f6549b8c05/events.out.tfevents.1746868917.d7f6549b8c05.9538.0 (deflated 70%)\n",
            "  adding: content/gr00t_finetuned_kuka/model-00001-of-00002.safetensors (deflated 38%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/ (stored 0%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/config.json (deflated 58%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/rng_state.pth (deflated 25%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/model-00001-of-00002.safetensors (deflated 38%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/model-00002-of-00002.safetensors (deflated 26%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/experiment_cfg/ (stored 0%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/experiment_cfg/metadata.json (deflated 84%)\n",
            "  adding: content/gr00t_finetuned_kuka/checkpoint-2500/optimizer.pt\n",
            "zip I/O error: No space left on device\n",
            "zip error: Output file write failure (write error on zip file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Into The Groot Policy"
      ],
      "metadata": {
        "id": "ewXuHdMH1fYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Patch1\n"
      ],
      "metadata": {
        "id": "N4HfnWw1dI7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to your script\n",
        "script_path = '/content/eval_policy.py'\n",
        "\n",
        "# Read the current script\n",
        "with open(script_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Check if patch already exists\n",
        "if any('os.environ[\\'MPLBACKEND\\']' in line for line in lines):\n",
        "    print(' Script already patched.')\n",
        "else:\n",
        "    patch_lines = [\n",
        "        'import os\\n',\n",
        "        'os.environ[\\'MPLBACKEND\\'] = \\'Agg\\'\\n',\n",
        "        'import matplotlib\\n',\n",
        "        'matplotlib.use(\\'Agg\\')\\n'\n",
        "    ]\n",
        "\n",
        "    # Insert patch at the top\n",
        "    new_lines = patch_lines + lines\n",
        "\n",
        "    # Write the patched script back\n",
        "    with open(script_path, 'w') as file:\n",
        "        file.writelines(new_lines)\n",
        "\n",
        "    print(' Patched eval_policy.py with os.environ + matplotlib.use(\\'Agg\\')')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XSoLY4BbqND",
        "outputId": "ae51123e-a724-467b-9b1c-37e25b7bb142"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Script already patched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to your script\n",
        "script_path = '/content/eval_policy.py'\n",
        "\n",
        "# Read the current script\n",
        "with open(script_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Check if patch already exists\n",
        "if any('plt.savefig' in line for line in lines):\n",
        "    print(' Script already patched for saving plots.')\n",
        "else:\n",
        "    # Find insertion point and detect indentation\n",
        "    insert_index = None\n",
        "    indent = ''\n",
        "    for i, line in enumerate(lines):\n",
        "        if 'plt.show()' in line or 'print(\"Done\")' in line:\n",
        "            insert_index = i + 1\n",
        "            indent = line[:len(line) - len(line.lstrip())]\n",
        "            break\n",
        "\n",
        "    if insert_index is None:\n",
        "        insert_index = len(lines)  # fallback: add at end\n",
        "\n",
        "    # Patch lines with matching indentation\n",
        "    patch_lines = [\n",
        "        f'{indent}save_dir = \\'/content/seepage\\'\\n',\n",
        "        f'{indent}os.makedirs(save_dir, exist_ok=True)\\n',\n",
        "        f'{indent}plt.savefig(os.path.join(save_dir, f\\'trajectory_{{traj_id}}.png\\'))\\n',\n",
        "        f'{indent}plt.close()\\n'\n",
        "    ]\n",
        "\n",
        "    # Ensure 'import os' is present at the top\n",
        "    if not any('import os' in line for line in lines):\n",
        "        lines.insert(0, 'import os\\n')\n",
        "\n",
        "    # Insert the patch\n",
        "    new_lines = lines[:insert_index] + patch_lines + lines[insert_index:]\n",
        "\n",
        "    # Write the patched script back\n",
        "    with open(script_path, 'w') as file:\n",
        "        file.writelines(new_lines)\n",
        "\n",
        "    print(' Patched eval_policy.py to save plots in /content/seepage/')\n"
      ],
      "metadata": {
        "id": "qwQDr24ne3pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04ef4d4-3979-4309-cd27-15a5fcf284b9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Script already patched for saving plots.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "efMgzdsOdIEH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPJxmfqPVlkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n1BLrDurdHFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add the evaluation time"
      ],
      "metadata": {
        "id": "tNa_hRkmotyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/gr00t_finetuned_kuka \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord \\\n",
        "  --plot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz8yABIUSmYL",
        "outputId": "dbb31831-7239-46ac-e70e-caaaa9e8e62d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 18:58:38.585774: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 18:58:38.636561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 18:58:38.636607: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 18:58:38.637856: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 18:58:38.645230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 18:58:39.882383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.23it/s]\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3517.4474012123706\n",
            "MSE: 3517.4474012123706\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1881.7920932597215\n",
            "MSE: 1881.7920932597215\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1772.9115361263146\n",
            "MSE: 1772.9115361263146\n",
            "Running trajectory: 3\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1633.4519667463328\n",
            "MSE: 1633.4519667463328\n",
            "Running trajectory: 4\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2838.580653236292\n",
            "MSE: 2838.580653236292\n",
            "Running trajectory: 5\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1478.1724538672659\n",
            "MSE: 1478.1724538672659\n",
            "Running trajectory: 6\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3196.940460329699\n",
            "MSE: 3196.940460329699\n",
            "Running trajectory: 7\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1634.0071811558869\n",
            "MSE: 1634.0071811558869\n",
            "Running trajectory: 8\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1193.866950520163\n",
            "MSE: 1193.866950520163\n",
            "Running trajectory: 9\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1369.3825353232767\n",
            "MSE: 1369.3825353232767\n",
            "Running trajectory: 10\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 598.299380752203\n",
            "MSE: 598.299380752203\n",
            "Running trajectory: 11\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 766.1802279418439\n",
            "MSE: 766.1802279418439\n",
            "Running trajectory: 12\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 5231.111824240234\n",
            "MSE: 5231.111824240234\n",
            "Running trajectory: 13\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1643.3999269056412\n",
            "MSE: 1643.3999269056412\n",
            "Running trajectory: 14\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2807.1291066786207\n",
            "MSE: 2807.1291066786207\n",
            "Running trajectory: 15\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2530.708834472985\n",
            "MSE: 2530.708834472985\n",
            "Running trajectory: 16\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 775.1772968681752\n",
            "MSE: 775.1772968681752\n",
            "Running trajectory: 17\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 308.1024267545121\n",
            "MSE: 308.1024267545121\n",
            "Running trajectory: 18\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2887.8154434004614\n",
            "MSE: 2887.8154434004614\n",
            "Running trajectory: 19\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1042.6845438995506\n",
            "MSE: 1042.6845438995506\n",
            "Running trajectory: 20\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1254.365461049687\n",
            "/content/Isaac-GR00T/gr00t/utils/eval.py:95: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(nrows=num_of_joints, ncols=1, figsize=(8, 4 * num_of_joints))\n",
            "MSE: 1254.365461049687\n",
            "Running trajectory: 21\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 721.1072313456971\n",
            "MSE: 721.1072313456971\n",
            "Running trajectory: 22\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1252.688145723023\n",
            "MSE: 1252.688145723023\n",
            "Running trajectory: 23\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1729.8844505470706\n",
            "MSE: 1729.8844505470706\n",
            "Running trajectory: 24\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2367.0887517297406\n",
            "MSE: 2367.0887517297406\n",
            "Running trajectory: 25\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1106.288468073462\n",
            "MSE: 1106.288468073462\n",
            "Running trajectory: 26\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2566.9216962381674\n",
            "MSE: 2566.9216962381674\n",
            "Running trajectory: 27\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1709.256198550884\n",
            "MSE: 1709.256198550884\n",
            "Running trajectory: 28\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1245.4070479839982\n",
            "MSE: 1245.4070479839982\n",
            "Running trajectory: 29\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1154.7216219373317\n",
            "MSE: 1154.7216219373317\n",
            "Running trajectory: 30\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1826.2690678181232\n",
            "MSE: 1826.2690678181232\n",
            " Total Inference Time: 150.76 seconds\n",
            "Average MSE across all trajs: 1807.779367248024\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mse_values = [\n",
        "    3165.53, 2392.60, 859.23, 1130.19, 3113.02, 2436.81, 2773.10, 2036.83,\n",
        "    1103.04, 980.31, 743.19, 817.28, 6712.18, 1252.36, 2987.78, 2430.14,\n",
        "    1202.76, 832.60, 1634.08, 1008.78, 704.30, 600.57, 1601.23, 446.80,\n",
        "    4210.71, 635.34, 3692.95, 1896.08, 2221.23, 1700.73, 1835.12\n",
        "]\n",
        "\n",
        "assert len(mse_values) == 31, \"Expected 31 MSE values\"\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(len(mse_values)), mse_values, marker='o')\n",
        "plt.title('Unnormalized Action MSE Across Trajectories')\n",
        "plt.xlabel('Trajectory Index')\n",
        "plt.ylabel('MSE')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "aqcQpZfzjcYy",
        "outputId": "d6ab5e01-352a-467a-807f-c583bebb45ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "for f in os.listdir('/content'):\n",
        "    if f.endswith('.png'):\n",
        "        display(Image(filename=os.path.join('/content', f)))\n"
      ],
      "metadata": {
        "id": "dnGlqUelVZsy"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/gr00t_finetuned_kuka\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1j3si_Oj5LN",
        "outputId": "590f56e4-f07b-4968-f3c2-b179327ea84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44G\t/content/gr00t_finetuned_kuka\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files and directories inside the fine-tuned folder\n",
        "base_path = \"/content/gr00t_finetuned_kuka\"\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for f in files:\n",
        "        print(os.path.join(root, f))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNLMGLdLkS5U",
        "outputId": "4947e016-d601-43f0-cf86-92bf4e74b2ec"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gr00t_finetuned_kuka/model-00001-of-00002.safetensors\n",
            "/content/gr00t_finetuned_kuka/model-00002-of-00002.safetensors\n",
            "/content/gr00t_finetuned_kuka/config.json\n",
            "/content/gr00t_finetuned_kuka/trainer_state.json\n",
            "/content/gr00t_finetuned_kuka/model.safetensors.index.json\n",
            "/content/gr00t_finetuned_kuka/training_args.bin\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/rng_state.pth\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/optimizer.pt\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/model-00001-of-00002.safetensors\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/model-00002-of-00002.safetensors\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/config.json\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/scheduler.pt\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/trainer_state.json\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/model.safetensors.index.json\n",
            "/content/gr00t_finetuned_kuka/checkpoint-5000/experiment_cfg/metadata.json\n",
            "/content/gr00t_finetuned_kuka/experiment_cfg/metadata.json\n",
            "/content/gr00t_finetuned_kuka/runs/May18_14-53-38_8c1979be7217/events.out.tfevents.1747580020.8c1979be7217.5610.0\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/rng_state.pth\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/optimizer.pt\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/model-00001-of-00002.safetensors\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/model-00002-of-00002.safetensors\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/config.json\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/scheduler.pt\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/trainer_state.json\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/model.safetensors.index.json\n",
            "/content/gr00t_finetuned_kuka/checkpoint-2500/experiment_cfg/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define local and Google Drive directories\n",
        "local_src_dir = '/content/gr00t_finetuned_kuka'\n",
        "gdrive_dest_dir = '/content/drive/MyDrive/gr00t_policy_export'\n",
        "\n",
        "# Create target directory in Google Drive if it doesn't exist\n",
        "os.makedirs(gdrive_dest_dir, exist_ok=True)\n",
        "\n",
        "# List of files to copy\n",
        "files_to_copy = [\n",
        "    'model-00001-of-00002.safetensors',\n",
        "    'model-00002-of-00002.safetensors',\n",
        "    'model.safetensors.index.json',\n",
        "    'config.json',\n",
        "    'experiment_cfg/metadata.json'\n",
        "]\n",
        "\n",
        "# Copy files\n",
        "for file_name in files_to_copy:\n",
        "    src_path = os.path.join(local_src_dir, file_name)\n",
        "    dest_path = os.path.join(gdrive_dest_dir, os.path.basename(file_name))\n",
        "    shutil.copy(src_path, dest_path)\n",
        "\n",
        "print(f\" Files successfully copied to: {gdrive_dest_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsfyTx9YkciA",
        "outputId": "80bec33c-dda9-4e5c-ab7b-ffe3cfc5f432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Files successfully copied to: /content/drive/MyDrive/gr00t_policy_export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h /content/gr00t_finetuned_kuka/model-0000*-of-00002.safetensors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJCxMWt6rjRN",
        "outputId": "cdee2c1b-5c0e-4cfe-adf9-56f0ffb3d0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.6G\t/content/gr00t_finetuned_kuka/model-00001-of-00002.safetensors\n",
            "3.6G\t/content/gr00t_finetuned_kuka/model-00002-of-00002.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxFaAwLaQHPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ke-52pgWQHS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yCMzoARO1ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained"
      ],
      "metadata": {
        "id": "GbtrYvgEfQir"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeP5uWUuhvyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kEr8yzKtgPtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RH4CQCTxgPxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqHOBxiggPz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H140WTMwgP22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Method 1 : Half Precision FP16"
      ],
      "metadata": {
        "id": "rdVK9m_2ijaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_dir = \"/content/gr00t_finetuned_kuka\"\n",
        "for file in os.listdir(model_dir):\n",
        "    print(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYQDDVQ2hG7b",
        "outputId": "b35f2610-b72b-4bba-e51f-af3f1eceb206"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model-00001-of-00002.safetensors\n",
            "model-00002-of-00002.safetensors\n",
            "config.json\n",
            "checkpoint-5000\n",
            "experiment_cfg\n",
            "trainer_state.json\n",
            "runs\n",
            "model.safetensors.index.json\n",
            "checkpoint-2500\n",
            "training_args.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install safetensors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sm9zOZFhTgY",
        "outputId": "4c224c23-15b8-4676-ea09-8905db85531e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_rWbkVuj8iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFFQlh_3j8lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "    from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "    from gr00t.model.policy import Gr00tPolicy\n",
        "    from safetensors.torch import save_file\n",
        "    import torch\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    PRE_TRAINED_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "    FP16_SAVE_PATH = \"/content/gr00t_fp16\"\n",
        "    DATA_KEY = \"kuka_custom\"\n",
        "    EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "    os.makedirs(FP16_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "    data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "    modality_config = data_config.modality_config()\n",
        "    modality_transform = data_config.transform()\n",
        "\n",
        "    print(\"Data config and modalities loaded\")\n",
        "\n",
        "    policy = Gr00tPolicy(\n",
        "        model_path=PRE_TRAINED_MODEL_PATH,\n",
        "        embodiment_tag=EMBODIMENT_TAG,\n",
        "        modality_config=modality_config,\n",
        "        modality_transform=modality_transform,\n",
        "    )\n",
        "\n",
        "    print(\"Fine-tuned GR00T model loaded\")\n",
        "\n",
        "    policy.model = policy.model.half()\n",
        "\n",
        "    # Save in safetensors format\n",
        "    save_file(policy.model.state_dict(), f\"{FP16_SAVE_PATH}/model.safetensors\")\n",
        "    print(\"Model saved as FP16 safetensors\")\n",
        "\n",
        "    shutil.copy(f\"{PRE_TRAINED_MODEL_PATH}/config.json\", f\"{FP16_SAVE_PATH}/config.json\")\n",
        "    index_path = f\"{PRE_TRAINED_MODEL_PATH}/model.safetensors.index.json\"\n",
        "    if os.path.exists(index_path):\n",
        "        shutil.copy(index_path, f\"{FP16_SAVE_PATH}/model.safetensors.index.json\")\n",
        "\n",
        "    print(\"Config and index copied\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5JJSAx9Qgy3",
        "outputId": "91c5b909-af2b-4053-f442-9347f371be0e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Data config and modalities loaded\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Fine-tuned GR00T model loaded\n",
            "Model saved as FP16 safetensors\n",
            "Config and index copied\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:02:22.538628: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:02:22.587806: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:02:22.587857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:02:22.589087: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:02:22.596398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:02:23.537568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.05it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Final for FP16 - RUN THIS"
      ],
      "metadata": {
        "id": "9aHHQgALx2WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "    from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "    from gr00t.model.policy import Gr00tPolicy\n",
        "    from safetensors.torch import save_file\n",
        "    from transformers import AutoConfig\n",
        "    import torch\n",
        "    import os\n",
        "    import shutil\n",
        "    import json\n",
        "\n",
        "    PRE_TRAINED_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "    FP16_SAVE_PATH = \"/content/gr00t_fp16_complete\"\n",
        "    DATA_KEY = \"kuka_custom\"\n",
        "    EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "    # === Create required directories ===\n",
        "    os.makedirs(FP16_SAVE_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.join(FP16_SAVE_PATH, \"experiment_cfg\"), exist_ok=True)\n",
        "\n",
        "    # === Load data config ===\n",
        "    data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "    modality_config = data_config.modality_config()\n",
        "    modality_transform = data_config.transform()\n",
        "    print(\" Data config and modalities loaded\")\n",
        "\n",
        "    # === Load and convert model ===\n",
        "    policy = Gr00tPolicy(\n",
        "        model_path=PRE_TRAINED_MODEL_PATH,\n",
        "        embodiment_tag=EMBODIMENT_TAG,\n",
        "        modality_config=modality_config,\n",
        "        modality_transform=modality_transform,\n",
        "    )\n",
        "    print(\" Fine-tuned GR00T model loaded\")\n",
        "    policy.model = policy.model.half()\n",
        "    print(\" Converted model to FP16\")\n",
        "\n",
        "    # === Save model with safetensors ===\n",
        "    metadata = {\"format\": \"pt\", \"dtype\": \"float16\"}\n",
        "    save_file(policy.model.state_dict(), f\"{FP16_SAVE_PATH}/model.safetensors\", metadata=metadata)\n",
        "    print(\" Model saved in safetensors format\")\n",
        "\n",
        "    # === Save Hugging Face config with dtype ===\n",
        "    try:\n",
        "        config = AutoConfig.from_pretrained(PRE_TRAINED_MODEL_PATH)\n",
        "        config.torch_dtype = \"float16\"\n",
        "        config.save_pretrained(FP16_SAVE_PATH)\n",
        "        print(\" AutoConfig saved with FP16 dtype\")\n",
        "    except Exception as e:\n",
        "        print(f\" Could not save HF AutoConfig: {e}\")\n",
        "\n",
        "    # === Copy standard config/tokenizer files ===\n",
        "    required_files = [\n",
        "        \"config.json\", \"special_tokens_map.json\",\n",
        "        \"tokenizer_config.json\", \"vocab.json\",\n",
        "        \"merges.txt\", \"tokenizer.json\", \"preprocessor_config.json\"\n",
        "    ]\n",
        "    for file in required_files:\n",
        "        src = os.path.join(PRE_TRAINED_MODEL_PATH, file)\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy2(src, FP16_SAVE_PATH)\n",
        "            print(f\" Copied {file}\")\n",
        "\n",
        "    # === Copy experiment_cfg or fallback metadata.json ===\n",
        "    exp_cfg_src = os.path.join(PRE_TRAINED_MODEL_PATH, \"experiment_cfg\")\n",
        "    if os.path.exists(exp_cfg_src):\n",
        "        for file in os.listdir(exp_cfg_src):\n",
        "            src_file = os.path.join(exp_cfg_src, file)\n",
        "            dst_file = os.path.join(FP16_SAVE_PATH, \"experiment_cfg\", file)\n",
        "            shutil.copy2(src_file, dst_file)\n",
        "        print(\" Copied experiment_cfg files\")\n",
        "    else:\n",
        "        fallback_metadata = [\n",
        "            os.path.join(PRE_TRAINED_MODEL_PATH, \"metadata.json\"),\n",
        "            \"/content/Isaac-GR00T/gr00t/experiment_cfg/metadata.json\"\n",
        "        ]\n",
        "        for loc in fallback_metadata:\n",
        "            if os.path.exists(loc):\n",
        "                shutil.copy2(loc, os.path.join(FP16_SAVE_PATH, \"experiment_cfg\", \"metadata.json\"))\n",
        "                print(\" Copied fallback metadata.json\")\n",
        "                break\n",
        "        else:\n",
        "            print(\" Warning: metadata.json not found\")\n",
        "\n",
        "    # === Verify reload of exported model ===\n",
        "    try:\n",
        "        print(\"\\\\n🔍 Verifying saved model...\")\n",
        "        test_policy = Gr00tPolicy(\n",
        "            model_path=FP16_SAVE_PATH,\n",
        "            embodiment_tag=EMBODIMENT_TAG,\n",
        "            modality_config=modality_config,\n",
        "            modality_transform=modality_transform,\n",
        "        )\n",
        "        print(\"Verification successful: model reloaded from FP16 path\")\n",
        "    except Exception as e:\n",
        "        print(f\" Verification failed: {e}\")\n",
        "        print(\" Contents of export directory:\")\n",
        "        print(os.listdir(FP16_SAVE_PATH))\n",
        "        print(\" experiment_cfg contents:\", os.listdir(os.path.join(FP16_SAVE_PATH, \"experiment_cfg\")))\n",
        "        if os.path.exists(f\"{FP16_SAVE_PATH}/config.json\"):\n",
        "            with open(f\"{FP16_SAVE_PATH}/config.json\") as f:\n",
        "                print(\" config.json content:\", json.load(f))\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GWSSb_HCMI7",
        "outputId": "d49626de-65a5-4859-bdce-6a47e8842e2c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            "  Data config and modalities loaded\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            " Fine-tuned GR00T model loaded\n",
            " Converted model to FP16\n",
            " Model saved in safetensors format\n",
            " AutoConfig saved with FP16 dtype\n",
            " Copied config.json\n",
            " Copied experiment_cfg files\n",
            "\n",
            "🔍 Verifying saved model...\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_fp16_complete\n",
            "Loading pretrained dual brain from /content/gr00t_fp16_complete\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_fp16_complete\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Verification successful: model reloaded from FP16 path\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:03:15.778969: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:03:15.828910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:03:15.828963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:03:15.830189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:03:15.837547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:03:16.782063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How much changes"
      ],
      "metadata": {
        "id": "Df7xszyhSPQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "    import torch\n",
        "    from safetensors.torch import load_file\n",
        "    from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "    from gr00t.model.policy import Gr00tPolicy\n",
        "    import os\n",
        "\n",
        "    PRE_TRAINED_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "    FP16_SAVE_PATH = \"/content/gr00t_fp16_complete\"  # Updated to full export path\n",
        "    DATA_KEY = \"kuka_custom\"\n",
        "    EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "    # === Load data configuration ===\n",
        "    data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "    modality_config = data_config.modality_config()\n",
        "    modality_transform = data_config.transform()\n",
        "\n",
        "    # === Load FP32 model ===\n",
        "    policy_fp32 = Gr00tPolicy(\n",
        "        model_path=PRE_TRAINED_MODEL_PATH,\n",
        "        embodiment_tag=EMBODIMENT_TAG,\n",
        "        modality_config=modality_config,\n",
        "        modality_transform=modality_transform,\n",
        "    )\n",
        "    policy_fp32.model = policy_fp32.model.to(\"cpu\")\n",
        "\n",
        "    # === Load FP16 model from safetensors ===\n",
        "    safetensor_path = os.path.join(FP16_SAVE_PATH, \"model.safetensors\")\n",
        "    if not os.path.exists(safetensor_path):\n",
        "        raise FileNotFoundError(f\"FP16 model file not found at {safetensor_path}\")\n",
        "\n",
        "    fp16_state_dict = load_file(safetensor_path)\n",
        "\n",
        "    print(\"\\\\n---  Percent change in weights: FP32 vs FP16 ---\")\n",
        "    total_percent = 0.0\n",
        "    counted = 0\n",
        "\n",
        "    for name, param in policy_fp32.model.state_dict().items():\n",
        "        if name not in fp16_state_dict:\n",
        "            print(f\"[MISSING] {name} not in FP16 model.\")\n",
        "            continue\n",
        "\n",
        "        # Convert both to float32 on CPU for fair comparison\n",
        "        fp32_val = param.detach().to(dtype=torch.float32, device=\"cpu\")\n",
        "        fp16_val = fp16_state_dict[name].detach().to(dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "        if fp32_val.shape != fp16_val.shape:\n",
        "            print(f\"[SHAPE MISMATCH] {name}\")\n",
        "            continue\n",
        "\n",
        "        norm_diff = torch.norm(fp32_val - fp16_val).item()\n",
        "        norm_orig = torch.norm(fp32_val).item()\n",
        "\n",
        "        if norm_orig == 0:\n",
        "            continue\n",
        "\n",
        "        percent_change = (norm_diff / norm_orig) * 100\n",
        "        total_percent += percent_change\n",
        "        counted += 1\n",
        "\n",
        "        print(f\"{name}: {percent_change:.4f}% change\")\n",
        "\n",
        "    if counted > 0:\n",
        "        avg_change = total_percent / counted\n",
        "        print(f\"\\\\n Average percent change across {counted} parameters: {avg_change:.4f}%\")\n",
        "    else:\n",
        "        print(\" No valid parameters compared.\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_ksmxE4Q8Rb",
        "outputId": "c42082cd-11cd-4938-ba1c-754899eb5e92"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "\n",
            "---  Percent change in weights: FP32 vs FP16 ---\n",
            "backbone.model.vision_model.vision_model.embeddings.patch_embedding.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.embeddings.patch_embedding.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.embeddings.position_embedding.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm2.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm2.bias: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.post_layernorm.weight: 0.0000% change\n",
            "backbone.model.vision_model.vision_model.post_layernorm.bias: 0.0000% change\n",
            "backbone.model.language_model.model.embed_tokens.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.0.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.1.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.2.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.3.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.4.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.5.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.6.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.7.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.8.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.9.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.10.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.q_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.k_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.v_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.o_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.mlp.gate_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.mlp.up_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.mlp.down_proj.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.input_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.layers.11.post_attention_layernorm.weight: 0.0000% change\n",
            "backbone.model.language_model.model.norm.weight: 0.0000% change\n",
            "backbone.model.mlp1.0.weight: 0.0000% change\n",
            "backbone.model.mlp1.0.bias: 0.0000% change\n",
            "backbone.model.mlp1.1.weight: 0.0000% change\n",
            "backbone.model.mlp1.1.bias: 0.0000% change\n",
            "backbone.model.mlp1.3.weight: 0.0000% change\n",
            "backbone.model.mlp1.3.bias: 0.0000% change\n",
            "backbone.linear.weight: 0.0000% change\n",
            "backbone.linear.bias: 0.0000% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_1.weight: 0.0000% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_1.bias: 0.0000% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_2.weight: 0.0000% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.0.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.1.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.2.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.3.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.4.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.5.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.6.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.7.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.8.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.9.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.10.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.11.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.12.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.13.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.14.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.norm1.linear.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.norm1.linear.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_q.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_q.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_k.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_k.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_v.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_v.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_out.0.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_out.0.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.ff.net.0.proj.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.ff.net.0.proj.bias: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.ff.net.2.weight: 0.0000% change\n",
            "action_head.model.transformer_blocks.15.ff.net.2.bias: 0.0000% change\n",
            "action_head.model.proj_out_1.weight: 0.0000% change\n",
            "action_head.model.proj_out_1.bias: 0.0000% change\n",
            "action_head.model.proj_out_2.weight: 0.0000% change\n",
            "action_head.model.proj_out_2.bias: 0.0000% change\n",
            "action_head.state_encoder.layer1.W: 0.0000% change\n",
            "action_head.state_encoder.layer1.b: 0.0000% change\n",
            "action_head.state_encoder.layer2.W: 0.0000% change\n",
            "action_head.state_encoder.layer2.b: 0.0000% change\n",
            "action_head.action_encoder.W1.W: 0.0000% change\n",
            "action_head.action_encoder.W1.b: 0.0000% change\n",
            "action_head.action_encoder.W2.W: 0.0000% change\n",
            "action_head.action_encoder.W2.b: 0.0000% change\n",
            "action_head.action_encoder.W3.W: 0.0000% change\n",
            "action_head.action_encoder.W3.b: 0.0000% change\n",
            "action_head.action_decoder.layer1.W: 0.0000% change\n",
            "action_head.action_decoder.layer1.b: 0.0000% change\n",
            "action_head.action_decoder.layer2.W: 0.0000% change\n",
            "action_head.action_decoder.layer2.b: 0.0000% change\n",
            "action_head.position_embedding.weight: 0.0000% change\n",
            "\n",
            " Average percent change across 802 parameters: 0.0000%\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 15:37:42.004551: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 15:37:42.054748: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 15:37:42.054793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 15:37:42.056225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 15:37:42.063749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 15:37:43.008660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.27it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_model(self, model_path):\n",
        "    import os\n",
        "    import json\n",
        "    import torch\n",
        "    from safetensors.torch import load_file\n",
        "    from gr00t.model.gr00t_n1 import GR00T_N1\n",
        "\n",
        "    config_path = os.path.join(model_path, \"config.json\")\n",
        "    safetensor_path = os.path.join(model_path, \"model.safetensors\")\n",
        "\n",
        "    if os.path.exists(safetensor_path):\n",
        "        print(\" Manually loading GR00T model from safetensors\")\n",
        "        with open(config_path, \"r\") as f:\n",
        "            config = json.load(f)\n",
        "        model = GR00T_N1(config)\n",
        "        state_dict = load_file(safetensor_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.eval()\n",
        "        return model\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"model.safetensors not found in {model_path}\")\n"
      ],
      "metadata": {
        "id": "EzPOri1ykiTe"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yjbYlRE1L31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/gr00t_fp16_complete \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIaKZsaRjE7h",
        "outputId": "789d9949-843e-4f69-b32e-d5e31477b933"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:04:38.182719: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:04:38.232150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:04:38.232201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:04:38.233544: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:04:38.240675: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:04:39.371734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_fp16_complete\n",
            "Loading pretrained dual brain from /content/gr00t_fp16_complete\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_fp16_complete\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3666.3196497220088\n",
            "MSE: 3666.3196497220088\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1589.2394912618079\n",
            "MSE: 1589.2394912618079\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1169.6415585321233\n",
            "MSE: 1169.6415585321233\n",
            "Running trajectory: 3\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 944.9404861222233\n",
            "MSE: 944.9404861222233\n",
            "Running trajectory: 4\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1978.6638462480491\n",
            "MSE: 1978.6638462480491\n",
            "Running trajectory: 5\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2350.342664093516\n",
            "MSE: 2350.342664093516\n",
            "Running trajectory: 6\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2341.618928594592\n",
            "MSE: 2341.618928594592\n",
            "Running trajectory: 7\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1263.9531568604689\n",
            "MSE: 1263.9531568604689\n",
            "Running trajectory: 8\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1511.5517332280006\n",
            "MSE: 1511.5517332280006\n",
            "Running trajectory: 9\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1384.9023897424825\n",
            "MSE: 1384.9023897424825\n",
            "Running trajectory: 10\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 725.7690055089828\n",
            "MSE: 725.7690055089828\n",
            "Running trajectory: 11\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 530.4029538255544\n",
            "MSE: 530.4029538255544\n",
            "Running trajectory: 12\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 7447.951148232531\n",
            "MSE: 7447.951148232531\n",
            "Running trajectory: 13\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2057.051266407173\n",
            "MSE: 2057.051266407173\n",
            "Running trajectory: 14\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2620.998306611238\n",
            "MSE: 2620.998306611238\n",
            "Running trajectory: 15\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2520.960105749381\n",
            "MSE: 2520.960105749381\n",
            "Running trajectory: 16\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1237.4789346948837\n",
            "MSE: 1237.4789346948837\n",
            "Running trajectory: 17\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 889.8509877673235\n",
            "MSE: 889.8509877673235\n",
            "Running trajectory: 18\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2580.361952513624\n",
            "MSE: 2580.361952513624\n",
            "Running trajectory: 19\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 851.2324780234574\n",
            "MSE: 851.2324780234574\n",
            "Running trajectory: 20\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 642.8726687140942\n",
            "/content/Isaac-GR00T/gr00t/utils/eval.py:95: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(nrows=num_of_joints, ncols=1, figsize=(8, 4 * num_of_joints))\n",
            "MSE: 642.8726687140942\n",
            "Running trajectory: 21\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1328.041194691023\n",
            "MSE: 1328.041194691023\n",
            "Running trajectory: 22\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1055.8163779577405\n",
            "MSE: 1055.8163779577405\n",
            "Running trajectory: 23\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 757.43344790409\n",
            "MSE: 757.43344790409\n",
            "Running trajectory: 24\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3290.9045560650634\n",
            "MSE: 3290.9045560650634\n",
            "Running trajectory: 25\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 836.0205593427784\n",
            "MSE: 836.0205593427784\n",
            "Running trajectory: 26\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3050.2860523694635\n",
            "MSE: 3050.2860523694635\n",
            "Running trajectory: 27\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2351.543982609739\n",
            "MSE: 2351.543982609739\n",
            "Running trajectory: 28\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1732.5698317307863\n",
            "MSE: 1732.5698317307863\n",
            "Running trajectory: 29\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2096.944379807135\n",
            "MSE: 2096.944379807135\n",
            "Running trajectory: 30\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1020.1051771630748\n",
            "MSE: 1020.1051771630748\n",
            " Total Inference Time: 147.18 seconds\n",
            "Average MSE across all trajs: 1865.3473958740133\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LHSxzmIk1gH0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l0I2C2Ua3oCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5W5Z52cU1gMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Method 2 Magnitude pruning\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xX_tCqAS1gO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "import torch\n",
        "\n",
        "data_config = DATA_CONFIG_MAP[\"kuka_custom\"]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=\"/content/gr00t_finetuned_kuka\",\n",
        "    embodiment_tag=\"new_embodiment\",\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "\n",
        "model = policy.model\n",
        "\n",
        "# List all Linear layers\n",
        "print(\"Found Linear Layers in GR00T:\")\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        print(f\"  - {name}: {module}\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lphx24a1o0S",
        "outputId": "902a740c-4d42-49b4-9f89-2fa23280097c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Found Linear Layers in GR00T:\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj: Linear(in_features=1152, out_features=1152, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1: Linear(in_features=1152, out_features=4304, bias=True)\n",
            "  - backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2: Linear(in_features=4304, out_features=1152, bias=True)\n",
            "  - backbone.model.language_model.model.layers.0.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.0.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.0.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.0.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.0.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.0.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.0.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.1.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.1.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.1.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.1.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.1.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.1.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.1.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.2.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.2.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.2.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.2.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.2.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.2.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.2.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.3.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.3.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.3.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.3.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.3.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.3.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.3.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.4.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.4.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.4.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.4.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.4.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.4.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.4.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.5.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.5.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.5.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.5.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.5.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.5.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.5.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.6.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.6.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.6.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.6.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.6.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.6.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.6.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.7.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.7.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.7.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.7.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.7.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.7.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.7.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.8.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.8.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.8.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.8.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.8.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.8.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.8.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.9.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.9.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.9.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.9.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.9.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.9.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.9.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.10.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.10.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.10.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.10.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.10.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.10.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.10.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.11.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.11.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.11.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.11.self_attn.o_proj: Linear(in_features=2048, out_features=2048, bias=False)\n",
            "  - backbone.model.language_model.model.layers.11.mlp.gate_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.11.mlp.up_proj: Linear(in_features=2048, out_features=8192, bias=False)\n",
            "  - backbone.model.language_model.model.layers.11.mlp.down_proj: Linear(in_features=8192, out_features=2048, bias=False)\n",
            "  - backbone.model.mlp1.1: Linear(in_features=4608, out_features=2048, bias=True)\n",
            "  - backbone.model.mlp1.3: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "  - backbone.linear: Linear(in_features=2048, out_features=1536, bias=True)\n",
            "  - action_head.model.timestep_encoder.timestep_embedder.linear_1: Linear(in_features=256, out_features=1536, bias=True)\n",
            "  - action_head.model.timestep_encoder.timestep_embedder.linear_2: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.0.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.0.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.0.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.0.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.0.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.0.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.0.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.1.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.1.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.1.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.1.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.1.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.1.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.1.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.2.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.2.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.2.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.2.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.2.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.2.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.2.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.3.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.3.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.3.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.3.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.3.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.3.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.3.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.4.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.4.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.4.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.4.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.4.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.4.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.4.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.5.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.5.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.5.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.5.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.5.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.5.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.5.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.6.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.6.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.6.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.6.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.6.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.6.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.6.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.7.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.7.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.7.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.7.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.7.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.7.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.7.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.8.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.8.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.8.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.8.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.8.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.8.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.8.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.9.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.9.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.9.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.9.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.9.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.9.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.9.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.10.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.10.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.10.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.10.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.10.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.10.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.10.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.11.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.11.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.11.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.11.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.11.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.11.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.11.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.12.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.12.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.12.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.12.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.12.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.12.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.12.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.13.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.13.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.13.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.13.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.13.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.13.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.13.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.14.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.14.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.14.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.14.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.14.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.14.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.14.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.15.norm1.linear: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.transformer_blocks.15.attn1.to_q: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.15.attn1.to_k: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.15.attn1.to_v: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.15.attn1.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  - action_head.model.transformer_blocks.15.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)\n",
            "  - action_head.model.transformer_blocks.15.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)\n",
            "  - action_head.model.proj_out_1: Linear(in_features=1536, out_features=3072, bias=True)\n",
            "  - action_head.model.proj_out_2: Linear(in_features=1536, out_features=1024, bias=True)\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 15:43:56.079981: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 15:43:56.129739: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 15:43:56.129788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 15:43:56.131123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 15:43:56.138354: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 15:43:57.076896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.11it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wmu0DEm_1NEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import json\n",
        "from safetensors.torch import save_file\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# Config\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "PRUNED_MODEL_PATH = \"/content/gr00t_pruned_30\"\n",
        "SPARSITY = 0.5\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "# Prepare output directory\n",
        "os.makedirs(PRUNED_MODEL_PATH, exist_ok=True)\n",
        "\n",
        "# Load fine-tuned model using Gr00tPolicy\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "\n",
        "model = policy.model.eval()\n",
        "\n",
        "# Prune all nn.Linear layers\n",
        "total_params = 0\n",
        "total_pruned = 0\n",
        "\n",
        "print(f\"Applying {int(SPARSITY*100)} percent magnitude pruning on nn.Linear layers:\")\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        num_weights = module.weight.nelement()\n",
        "        total_params += num_weights\n",
        "        prune.l1_unstructured(module, name=\"weight\", amount=SPARSITY)\n",
        "        pruned = int(SPARSITY * num_weights)\n",
        "        total_pruned += pruned\n",
        "        print(f\"{name}: pruned {pruned} of {num_weights} weights\")\n",
        "        prune.remove(module, \"weight\")\n",
        "\n",
        "sparsity_pct = 100 * total_pruned / total_params\n",
        "print(f\"Total pruned: {total_pruned} of {total_params} weights ({sparsity_pct:.2f} percent)\")\n",
        "\n",
        "# Save pruned model in safetensors format\n",
        "save_file(model.state_dict(), f\"{PRUNED_MODEL_PATH}/model.safetensors\")\n",
        "\n",
        "# Copy config and index files\n",
        "for file in [\"config.json\", \"model.safetensors.index.json\"]:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(PRUNED_MODEL_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        os.system(f\"cp {src} {dst}\")\n",
        "\n",
        "print(f\"Pruned model saved to {PRUNED_MODEL_PATH}\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8-y7TOx2gff",
        "outputId": "9bb96ba5-7db5-4822-87b5-5cff251b88d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Applying 50 percent magnitude pruning on nn.Linear layers:\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "backbone.model.mlp1.1: pruned 4718592 of 9437184 weights\n",
            "backbone.model.mlp1.3: pruned 2097152 of 4194304 weights\n",
            "backbone.linear: pruned 1572864 of 3145728 weights\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_1: pruned 196608 of 393216 weights\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_2: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.0.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.1.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.1.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.2.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.2.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.3.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.3.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.4.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.4.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.5.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.5.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.6.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.6.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.7.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.7.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.8.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.8.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.9.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.9.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.10.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.10.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.11.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.11.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.12.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.12.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.13.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.13.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.14.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.14.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.15.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "action_head.model.transformer_blocks.15.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "action_head.model.proj_out_1: pruned 2359296 of 4718592 weights\n",
            "action_head.model.proj_out_2: pruned 786432 of 1572864 weights\n",
            "Total pruned: 885340160 of 1770680320 weights (50.00 percent)\n",
            "Pruned model saved to /content/gr00t_pruned_30\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 09:59:14.016699: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 09:59:14.066155: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 09:59:14.066203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 09:59:14.067428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 09:59:14.074870: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 09:59:15.151774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJtkTr6BTAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_FInal Magnitue pruning"
      ],
      "metadata": {
        "id": "MDSQ16lUzKJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import json\n",
        "from safetensors.torch import save_file\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "from transformers import AutoConfig\n",
        "import shutil\n",
        "\n",
        "# Config\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "PRUNED_MODEL_PATH = \"/content/gr00t_pruned_50_complete\"\n",
        "SPARSITY = 0.5\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "# Prepare output directory\n",
        "os.makedirs(PRUNED_MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(PRUNED_MODEL_PATH, \"experiment_cfg\"), exist_ok=True)\n",
        "\n",
        "# Load model via Gr00tPolicy\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "\n",
        "model = policy.model.eval()\n",
        "\n",
        "# Prune all nn.Linear layers\n",
        "total_params = 0\n",
        "total_pruned = 0\n",
        "print(f\" Applying {int(SPARSITY*100)}% L1-magnitude pruning on Linear layers:\")\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        num_weights = module.weight.nelement()\n",
        "        total_params += num_weights\n",
        "        prune.l1_unstructured(module, name=\"weight\", amount=SPARSITY)\n",
        "        pruned = int(SPARSITY * num_weights)\n",
        "        total_pruned += pruned\n",
        "        print(f\"  {name}: pruned {pruned} of {num_weights} weights\")\n",
        "        prune.remove(module, \"weight\")\n",
        "\n",
        "sparsity_pct = 100 * total_pruned / total_params\n",
        "print(f\" Total pruned: {total_pruned} of {total_params} weights ({sparsity_pct:.2f}%)\")\n",
        "\n",
        "# Save pruned model\n",
        "metadata = {\"format\": \"pt\", \"pruned\": \"true\", \"sparsity\": str(SPARSITY)}\n",
        "save_file(model.state_dict(), f\"{PRUNED_MODEL_PATH}/model.safetensors\", metadata=metadata)\n",
        "\n",
        "# Save config with pruning info\n",
        "try:\n",
        "    config = AutoConfig.from_pretrained(ORIG_MODEL_PATH)\n",
        "    config.pruned = True\n",
        "    config.sparsity_level = SPARSITY\n",
        "    config.save_pretrained(PRUNED_MODEL_PATH)\n",
        "    print(\" HF config with pruning info saved\")\n",
        "except Exception as e:\n",
        "    print(f\" Could not save AutoConfig: {e}\")\n",
        "\n",
        "# Copy all relevant files\n",
        "required_files = [\n",
        "    \"config.json\", \"special_tokens_map.json\",\n",
        "    \"tokenizer_config.json\", \"vocab.json\",\n",
        "    \"merges.txt\", \"tokenizer.json\", \"preprocessor_config.json\"\n",
        "]\n",
        "for file in required_files:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(PRUNED_MODEL_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\" Copied {file}\")\n",
        "\n",
        "# Copy experiment_cfg or fallback metadata\n",
        "exp_cfg_src = os.path.join(ORIG_MODEL_PATH, \"experiment_cfg\")\n",
        "if os.path.exists(exp_cfg_src):\n",
        "    for file in os.listdir(exp_cfg_src):\n",
        "        src = os.path.join(exp_cfg_src, file)\n",
        "        dst = os.path.join(PRUNED_MODEL_PATH, \"experiment_cfg\", file)\n",
        "        shutil.copy2(src, dst)\n",
        "    print(\" Copied experiment_cfg files\")\n",
        "else:\n",
        "    fallback_metadata = [\n",
        "        os.path.join(ORIG_MODEL_PATH, \"metadata.json\"),\n",
        "        \"/content/Isaac-GR00T/gr00t/experiment_cfg/metadata.json\"\n",
        "    ]\n",
        "    for loc in fallback_metadata:\n",
        "        if os.path.exists(loc):\n",
        "            shutil.copy2(loc, os.path.join(PRUNED_MODEL_PATH, \"experiment_cfg\", \"metadata.json\"))\n",
        "            print(\"Copied fallback metadata.json\")\n",
        "            break\n",
        "    else:\n",
        "        print(\" metadata.json not found\")\n",
        "\n",
        "# (Optional) Verify reload\n",
        "try:\n",
        "    print(\"\\\\n Verifying saved pruned model...\")\n",
        "    test_policy = Gr00tPolicy(\n",
        "        model_path=PRUNED_MODEL_PATH,\n",
        "        embodiment_tag=EMBODIMENT_TAG,\n",
        "        modality_config=modality_config,\n",
        "        modality_transform=modality_transform,\n",
        "    )\n",
        "    print(\" Verification: Pruned model reloaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\" Reload failed: {e}\")\n",
        "    print(\" Contents of pruned dir:\", os.listdir(PRUNED_MODEL_PATH))\n",
        "    print(\" experiment_cfg contents:\", os.listdir(os.path.join(PRUNED_MODEL_PATH, \"experiment_cfg\")))\n",
        "    if os.path.exists(f\"{PRUNED_MODEL_PATH}/config.json\"):\n",
        "        with open(f\"{PRUNED_MODEL_PATH}/config.json\") as f:\n",
        "            print(\" config.json:\", json.load(f))\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiK2RE15FG8k",
        "outputId": "269b00d3-1915-4948-fc6b-b739e67b1b5a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            " Applying 50% L1-magnitude pruning on Linear layers:\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj: pruned 663552 of 1327104 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2: pruned 2479104 of 4958208 weights\n",
            "  backbone.model.language_model.model.layers.0.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.0.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.0.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.0.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.0.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.0.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.0.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.1.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.1.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.1.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.1.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.1.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.1.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.1.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.2.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.2.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.2.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.2.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.2.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.2.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.2.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.3.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.3.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.3.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.3.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.3.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.3.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.3.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.4.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.4.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.4.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.4.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.4.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.4.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.4.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.5.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.5.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.5.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.5.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.5.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.5.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.5.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.6.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.6.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.6.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.6.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.6.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.6.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.6.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.7.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.7.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.7.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.7.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.7.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.7.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.7.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.8.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.8.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.8.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.8.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.8.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.8.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.8.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.9.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.9.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.9.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.9.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.9.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.9.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.9.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.10.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.10.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.10.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.10.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.10.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.10.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.10.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.11.self_attn.q_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.11.self_attn.k_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.11.self_attn.v_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.11.self_attn.o_proj: pruned 2097152 of 4194304 weights\n",
            "  backbone.model.language_model.model.layers.11.mlp.gate_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.11.mlp.up_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.language_model.model.layers.11.mlp.down_proj: pruned 8388608 of 16777216 weights\n",
            "  backbone.model.mlp1.1: pruned 4718592 of 9437184 weights\n",
            "  backbone.model.mlp1.3: pruned 2097152 of 4194304 weights\n",
            "  backbone.linear: pruned 1572864 of 3145728 weights\n",
            "  action_head.model.timestep_encoder.timestep_embedder.linear_1: pruned 196608 of 393216 weights\n",
            "  action_head.model.timestep_encoder.timestep_embedder.linear_2: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.0.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.0.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.0.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.0.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.0.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.0.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.0.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.1.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.1.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.1.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.1.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.1.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.1.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.1.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.2.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.2.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.2.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.2.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.2.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.2.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.2.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.3.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.3.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.3.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.3.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.3.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.3.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.3.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.4.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.4.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.4.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.4.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.4.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.4.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.4.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.5.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.5.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.5.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.5.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.5.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.5.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.5.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.6.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.6.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.6.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.6.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.6.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.6.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.6.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.7.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.7.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.7.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.7.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.7.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.7.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.7.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.8.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.8.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.8.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.8.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.8.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.8.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.8.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.9.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.9.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.9.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.9.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.9.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.9.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.9.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.10.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.10.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.10.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.10.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.10.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.10.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.10.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.11.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.11.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.11.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.11.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.11.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.11.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.11.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.12.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.12.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.12.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.12.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.12.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.12.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.12.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.13.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.13.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.13.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.13.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.13.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.13.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.13.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.14.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.14.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.14.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.14.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.14.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.14.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.14.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.15.norm1.linear: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.transformer_blocks.15.attn1.to_q: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.15.attn1.to_k: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.15.attn1.to_v: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.15.attn1.to_out.0: pruned 1179648 of 2359296 weights\n",
            "  action_head.model.transformer_blocks.15.ff.net.0.proj: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.transformer_blocks.15.ff.net.2: pruned 4718592 of 9437184 weights\n",
            "  action_head.model.proj_out_1: pruned 2359296 of 4718592 weights\n",
            "  action_head.model.proj_out_2: pruned 786432 of 1572864 weights\n",
            " Total pruned: 885340160 of 1770680320 weights (50.00%)\n",
            " HF config with pruning info saved\n",
            " Copied config.json\n",
            " Copied experiment_cfg files\n",
            "\n",
            " Verifying saved pruned model...\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_pruned_50_complete\n",
            "Loading pretrained dual brain from /content/gr00t_pruned_50_complete\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_pruned_50_complete\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            " Verification: Pruned model reloaded successfully\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:09:18.931388: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:09:18.981697: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:09:18.981750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:09:18.983027: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:09:18.990396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:09:20.113587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.27it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHZ3SMIMFHAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HN9NCBO8FHCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How much weight changes"
      ],
      "metadata": {
        "id": "_pjMdVHXTAXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "    import torch\n",
        "    from safetensors.torch import load_file\n",
        "    from gr00t.model.policy import Gr00tPolicy\n",
        "    from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "    # Paths\n",
        "    ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "    PRUNED_MODEL_PATH = \"/content/gr00t_pruned_50_complete\"\n",
        "    DATA_KEY = \"kuka_custom\"\n",
        "    EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "    # Load original model\n",
        "    data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "    modality_config = data_config.modality_config()\n",
        "    modality_transform = data_config.transform()\n",
        "\n",
        "    policy_fp32 = Gr00tPolicy(\n",
        "        model_path=ORIG_MODEL_PATH,\n",
        "        embodiment_tag=EMBODIMENT_TAG,\n",
        "        modality_config=modality_config,\n",
        "        modality_transform=modality_transform,\n",
        "    )\n",
        "    model_fp32 = policy_fp32.model.to(\"cpu\").eval()\n",
        "\n",
        "    # Load pruned model weights\n",
        "    pruned_state_dict = load_file(f\"{PRUNED_MODEL_PATH}/model.safetensors\")\n",
        "\n",
        "    print(\"\\\\n--- Percent change in weights (Original vs Pruned) ---\")\n",
        "    total_change = 0.0\n",
        "    counted = 0\n",
        "\n",
        "    for name, param in model_fp32.state_dict().items():\n",
        "        if name not in pruned_state_dict:\n",
        "            print(f\"[MISSING] {name}\")\n",
        "            continue\n",
        "\n",
        "        w1 = param.detach().to(torch.float32)\n",
        "        w2 = pruned_state_dict[name].detach().to(torch.float32)\n",
        "\n",
        "        if w1.shape != w2.shape:\n",
        "            print(f\"[SHAPE MISMATCH] {name}\")\n",
        "            continue\n",
        "\n",
        "        if torch.norm(w1).item() == 0:\n",
        "            continue  # avoid divide-by-zero\n",
        "\n",
        "        delta = torch.norm(w1 - w2).item()\n",
        "        base = torch.norm(w1).item()\n",
        "        percent = (delta / base) * 100\n",
        "        print(f\"{name}: {percent:.6f}% change\")\n",
        "        total_change += percent\n",
        "        counted += 1\n",
        "\n",
        "    if counted > 0:\n",
        "        avg_change = total_change / counted\n",
        "        print(f\"\\\\nAverage percent change across {counted} tensors: {avg_change:.6f}%\")\n",
        "    else:\n",
        "        print(\"No matching parameters found.\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YQ0XMEdTArv",
        "outputId": "e19beef3-5615-4869-ab99-2927930b69b3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "\n",
            "--- Percent change in weights (Original vs Pruned) ---\n",
            "backbone.model.vision_model.vision_model.embeddings.patch_embedding.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.embeddings.patch_embedding.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.embeddings.position_embedding.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight: 8.302700% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight: 12.379103% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight: 8.356377% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight: 19.184493% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.weight: 15.348797% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.weight: 21.854563% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight: 17.498837% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight: 18.271202% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight: 16.638656% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight: 22.134782% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.weight: 19.148775% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.weight: 22.233073% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight: 21.968022% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight: 23.309843% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight: 21.287508% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight: 23.961262% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.weight: 21.761665% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.weight: 22.170804% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight: 24.078987% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight: 24.872835% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight: 24.106388% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight: 25.215389% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.weight: 24.609712% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.weight: 23.835509% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight: 24.532295% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight: 25.293503% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight: 25.087386% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight: 25.357082% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.weight: 25.255174% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.weight: 24.290361% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight: 24.480223% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight: 25.437526% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight: 24.920631% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight: 25.633370% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.weight: 24.928384% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.weight: 23.905214% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight: 25.600037% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight: 25.850438% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight: 25.890514% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight: 25.926630% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.weight: 25.671650% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.weight: 24.403433% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight: 25.856400% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight: 25.940896% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight: 26.002159% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight: 25.994281% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.weight: 25.783647% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.weight: 24.441633% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight: 25.377708% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight: 25.947317% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight: 25.767436% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight: 25.984818% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.weight: 25.868150% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.weight: 24.422873% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight: 26.144007% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight: 26.134017% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight: 26.276137% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight: 26.215331% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.weight: 25.860623% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.weight: 24.531455% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight: 26.118850% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight: 26.133676% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight: 26.253829% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight: 26.225778% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.weight: 26.073537% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.weight: 24.898967% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight: 26.163262% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight: 26.286949% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight: 26.332826% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight: 26.436049% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.weight: 26.228607% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.weight: 25.388450% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight: 26.157904% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight: 26.309514% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight: 26.360263% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight: 26.434108% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.weight: 26.499480% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.weight: 26.056647% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight: 26.391824% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight: 26.328065% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight: 26.457997% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight: 26.486567% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.weight: 26.609915% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.weight: 26.377960% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight: 26.331803% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight: 26.355906% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight: 26.480104% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight: 26.404101% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.weight: 26.575069% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.weight: 26.647502% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight: 26.374225% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight: 26.492853% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight: 26.491705% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight: 26.492020% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.weight: 26.620068% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.weight: 26.619497% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight: 26.383290% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight: 26.520985% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight: 26.521255% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight: 26.671698% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.weight: 26.494025% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.weight: 26.682478% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight: 26.471156% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight: 26.539282% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight: 26.582406% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight: 26.635135% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.weight: 26.389298% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.weight: 26.588748% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight: 26.523022% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight: 26.619023% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight: 26.644121% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight: 26.598938% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.weight: 26.436690% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.weight: 26.616985% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight: 26.498845% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight: 26.569280% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight: 26.628862% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight: 26.674445% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.weight: 26.307095% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.weight: 26.547292% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight: 26.503557% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight: 26.543248% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight: 26.630286% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight: 26.668741% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.weight: 26.271523% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.weight: 26.534703% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight: 26.485638% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight: 26.606889% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight: 26.600158% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight: 26.650139% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.weight: 26.105350% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.weight: 26.351156% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight: 26.502153% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight: 26.517074% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight: 26.587589% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight: 26.597977% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.weight: 26.097883% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.weight: 26.386838% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight: 26.480833% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight: 26.453652% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight: 26.589276% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight: 26.510233% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.weight: 26.198008% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.weight: 26.405444% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.weight: 26.459102% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.weight: 26.462216% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.weight: 26.571707% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.weight: 26.579009% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.weight: 26.335460% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.weight: 26.507424% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.weight: 26.400972% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.weight: 26.292187% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.weight: 26.505624% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.weight: 26.155144% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.weight: 26.445214% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.weight: 26.545624% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.weight: 26.259085% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.weight: 25.860032% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.weight: 26.436855% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.weight: 25.607894% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm1.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.weight: 26.379088% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.weight: 26.322981% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm2.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.layer_norm2.bias: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.post_layernorm.weight: 0.000000% change\n",
            "backbone.model.vision_model.vision_model.post_layernorm.bias: 0.000000% change\n",
            "backbone.model.language_model.model.embed_tokens.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.q_proj.weight: 16.161189% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.k_proj.weight: 18.356323% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.v_proj.weight: 21.402384% change\n",
            "backbone.model.language_model.model.layers.0.self_attn.o_proj.weight: 17.113788% change\n",
            "backbone.model.language_model.model.layers.0.mlp.gate_proj.weight: 26.433573% change\n",
            "backbone.model.language_model.model.layers.0.mlp.up_proj.weight: 26.431671% change\n",
            "backbone.model.language_model.model.layers.0.mlp.down_proj.weight: 26.025027% change\n",
            "backbone.model.language_model.model.layers.0.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.0.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.q_proj.weight: 23.882447% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.k_proj.weight: 24.018974% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.v_proj.weight: 25.545304% change\n",
            "backbone.model.language_model.model.layers.1.self_attn.o_proj.weight: 25.160109% change\n",
            "backbone.model.language_model.model.layers.1.mlp.gate_proj.weight: 26.248953% change\n",
            "backbone.model.language_model.model.layers.1.mlp.up_proj.weight: 26.494525% change\n",
            "backbone.model.language_model.model.layers.1.mlp.down_proj.weight: 26.400701% change\n",
            "backbone.model.language_model.model.layers.1.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.1.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.q_proj.weight: 23.844794% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.k_proj.weight: 23.797740% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.v_proj.weight: 25.959250% change\n",
            "backbone.model.language_model.model.layers.2.self_attn.o_proj.weight: 26.073090% change\n",
            "backbone.model.language_model.model.layers.2.mlp.gate_proj.weight: 26.349827% change\n",
            "backbone.model.language_model.model.layers.2.mlp.up_proj.weight: 26.539094% change\n",
            "backbone.model.language_model.model.layers.2.mlp.down_proj.weight: 26.439798% change\n",
            "backbone.model.language_model.model.layers.2.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.2.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.q_proj.weight: 24.317942% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.k_proj.weight: 24.176033% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.v_proj.weight: 25.399525% change\n",
            "backbone.model.language_model.model.layers.3.self_attn.o_proj.weight: 25.419536% change\n",
            "backbone.model.language_model.model.layers.3.mlp.gate_proj.weight: 26.434689% change\n",
            "backbone.model.language_model.model.layers.3.mlp.up_proj.weight: 26.478321% change\n",
            "backbone.model.language_model.model.layers.3.mlp.down_proj.weight: 26.365658% change\n",
            "backbone.model.language_model.model.layers.3.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.3.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.q_proj.weight: 24.148741% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.k_proj.weight: 23.959109% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.v_proj.weight: 25.624674% change\n",
            "backbone.model.language_model.model.layers.4.self_attn.o_proj.weight: 25.696056% change\n",
            "backbone.model.language_model.model.layers.4.mlp.gate_proj.weight: 26.494219% change\n",
            "backbone.model.language_model.model.layers.4.mlp.up_proj.weight: 26.497335% change\n",
            "backbone.model.language_model.model.layers.4.mlp.down_proj.weight: 26.438787% change\n",
            "backbone.model.language_model.model.layers.4.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.4.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.q_proj.weight: 24.388294% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.k_proj.weight: 24.097951% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.v_proj.weight: 25.576240% change\n",
            "backbone.model.language_model.model.layers.5.self_attn.o_proj.weight: 25.879229% change\n",
            "backbone.model.language_model.model.layers.5.mlp.gate_proj.weight: 26.553975% change\n",
            "backbone.model.language_model.model.layers.5.mlp.up_proj.weight: 26.538941% change\n",
            "backbone.model.language_model.model.layers.5.mlp.down_proj.weight: 26.513228% change\n",
            "backbone.model.language_model.model.layers.5.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.5.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.q_proj.weight: 24.778619% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.k_proj.weight: 24.647391% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.v_proj.weight: 25.891040% change\n",
            "backbone.model.language_model.model.layers.6.self_attn.o_proj.weight: 26.042443% change\n",
            "backbone.model.language_model.model.layers.6.mlp.gate_proj.weight: 26.436025% change\n",
            "backbone.model.language_model.model.layers.6.mlp.up_proj.weight: 26.492475% change\n",
            "backbone.model.language_model.model.layers.6.mlp.down_proj.weight: 26.488357% change\n",
            "backbone.model.language_model.model.layers.6.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.6.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.q_proj.weight: 24.139007% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.k_proj.weight: 24.012095% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.v_proj.weight: 25.777163% change\n",
            "backbone.model.language_model.model.layers.7.self_attn.o_proj.weight: 25.666617% change\n",
            "backbone.model.language_model.model.layers.7.mlp.gate_proj.weight: 26.301267% change\n",
            "backbone.model.language_model.model.layers.7.mlp.up_proj.weight: 26.416960% change\n",
            "backbone.model.language_model.model.layers.7.mlp.down_proj.weight: 26.385839% change\n",
            "backbone.model.language_model.model.layers.7.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.7.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.q_proj.weight: 24.505737% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.k_proj.weight: 24.229075% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.v_proj.weight: 25.327649% change\n",
            "backbone.model.language_model.model.layers.8.self_attn.o_proj.weight: 25.262801% change\n",
            "backbone.model.language_model.model.layers.8.mlp.gate_proj.weight: 26.144967% change\n",
            "backbone.model.language_model.model.layers.8.mlp.up_proj.weight: 26.345570% change\n",
            "backbone.model.language_model.model.layers.8.mlp.down_proj.weight: 26.282358% change\n",
            "backbone.model.language_model.model.layers.8.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.8.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.q_proj.weight: 24.649665% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.k_proj.weight: 24.546665% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.v_proj.weight: 25.457396% change\n",
            "backbone.model.language_model.model.layers.9.self_attn.o_proj.weight: 25.675973% change\n",
            "backbone.model.language_model.model.layers.9.mlp.gate_proj.weight: 25.910656% change\n",
            "backbone.model.language_model.model.layers.9.mlp.up_proj.weight: 26.158789% change\n",
            "backbone.model.language_model.model.layers.9.mlp.down_proj.weight: 26.139496% change\n",
            "backbone.model.language_model.model.layers.9.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.9.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.q_proj.weight: 23.099226% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.k_proj.weight: 22.237662% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.v_proj.weight: 25.389998% change\n",
            "backbone.model.language_model.model.layers.10.self_attn.o_proj.weight: 25.556005% change\n",
            "backbone.model.language_model.model.layers.10.mlp.gate_proj.weight: 25.518461% change\n",
            "backbone.model.language_model.model.layers.10.mlp.up_proj.weight: 26.022036% change\n",
            "backbone.model.language_model.model.layers.10.mlp.down_proj.weight: 25.913538% change\n",
            "backbone.model.language_model.model.layers.10.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.10.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.q_proj.weight: 23.677388% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.k_proj.weight: 23.381781% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.v_proj.weight: 25.531991% change\n",
            "backbone.model.language_model.model.layers.11.self_attn.o_proj.weight: 25.554583% change\n",
            "backbone.model.language_model.model.layers.11.mlp.gate_proj.weight: 25.166748% change\n",
            "backbone.model.language_model.model.layers.11.mlp.up_proj.weight: 25.901434% change\n",
            "backbone.model.language_model.model.layers.11.mlp.down_proj.weight: 25.774357% change\n",
            "backbone.model.language_model.model.layers.11.input_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.layers.11.post_attention_layernorm.weight: 0.000000% change\n",
            "backbone.model.language_model.model.norm.weight: 0.000000% change\n",
            "backbone.model.mlp1.0.weight: 0.000000% change\n",
            "backbone.model.mlp1.0.bias: 0.000000% change\n",
            "backbone.model.mlp1.1.weight: 26.793440% change\n",
            "backbone.model.mlp1.1.bias: 0.000000% change\n",
            "backbone.model.mlp1.3.weight: 26.448475% change\n",
            "backbone.model.mlp1.3.bias: 0.000000% change\n",
            "backbone.linear.weight: 26.191939% change\n",
            "backbone.linear.bias: 0.000000% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_1.weight: 12.634578% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_1.bias: 0.000000% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_2.weight: 8.392745% change\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.0.norm1.linear.weight: 22.058519% change\n",
            "action_head.model.transformer_blocks.0.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_q.weight: 20.328874% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_k.weight: 25.610414% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_v.weight: 25.146557% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_out.0.weight: 21.238339% change\n",
            "action_head.model.transformer_blocks.0.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.0.ff.net.0.proj.weight: 24.742066% change\n",
            "action_head.model.transformer_blocks.0.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.0.ff.net.2.weight: 19.292539% change\n",
            "action_head.model.transformer_blocks.0.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.1.norm1.linear.weight: 23.159612% change\n",
            "action_head.model.transformer_blocks.1.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_q.weight: 22.820011% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_k.weight: 23.514515% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_v.weight: 21.769804% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_out.0.weight: 22.066247% change\n",
            "action_head.model.transformer_blocks.1.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.1.ff.net.0.proj.weight: 23.562387% change\n",
            "action_head.model.transformer_blocks.1.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.1.ff.net.2.weight: 20.991410% change\n",
            "action_head.model.transformer_blocks.1.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.2.norm1.linear.weight: 22.124105% change\n",
            "action_head.model.transformer_blocks.2.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_q.weight: 23.120977% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_k.weight: 25.957448% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_v.weight: 26.165692% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_out.0.weight: 20.919547% change\n",
            "action_head.model.transformer_blocks.2.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.2.ff.net.0.proj.weight: 23.420609% change\n",
            "action_head.model.transformer_blocks.2.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.2.ff.net.2.weight: 20.513117% change\n",
            "action_head.model.transformer_blocks.2.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.3.norm1.linear.weight: 21.178491% change\n",
            "action_head.model.transformer_blocks.3.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_q.weight: 23.494075% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_k.weight: 24.271374% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_v.weight: 19.805930% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_out.0.weight: 22.287408% change\n",
            "action_head.model.transformer_blocks.3.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.3.ff.net.0.proj.weight: 23.454928% change\n",
            "action_head.model.transformer_blocks.3.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.3.ff.net.2.weight: 20.805076% change\n",
            "action_head.model.transformer_blocks.3.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.4.norm1.linear.weight: 22.175595% change\n",
            "action_head.model.transformer_blocks.4.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_q.weight: 22.858369% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_k.weight: 26.069406% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_v.weight: 26.238144% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_out.0.weight: 20.249354% change\n",
            "action_head.model.transformer_blocks.4.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.4.ff.net.0.proj.weight: 23.005433% change\n",
            "action_head.model.transformer_blocks.4.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.4.ff.net.2.weight: 19.965660% change\n",
            "action_head.model.transformer_blocks.4.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.5.norm1.linear.weight: 21.192347% change\n",
            "action_head.model.transformer_blocks.5.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_q.weight: 23.308041% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_k.weight: 23.811100% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_v.weight: 20.605105% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_out.0.weight: 20.849097% change\n",
            "action_head.model.transformer_blocks.5.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.5.ff.net.0.proj.weight: 22.764133% change\n",
            "action_head.model.transformer_blocks.5.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.5.ff.net.2.weight: 19.600996% change\n",
            "action_head.model.transformer_blocks.5.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.6.norm1.linear.weight: 22.106303% change\n",
            "action_head.model.transformer_blocks.6.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_q.weight: 21.250513% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_k.weight: 25.987621% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_v.weight: 25.983989% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_out.0.weight: 19.835072% change\n",
            "action_head.model.transformer_blocks.6.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.6.ff.net.0.proj.weight: 22.463641% change\n",
            "action_head.model.transformer_blocks.6.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.6.ff.net.2.weight: 18.205588% change\n",
            "action_head.model.transformer_blocks.6.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.7.norm1.linear.weight: 21.016270% change\n",
            "action_head.model.transformer_blocks.7.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_q.weight: 22.673385% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_k.weight: 22.440644% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_v.weight: 21.442255% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_out.0.weight: 20.184646% change\n",
            "action_head.model.transformer_blocks.7.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.7.ff.net.0.proj.weight: 22.661614% change\n",
            "action_head.model.transformer_blocks.7.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.7.ff.net.2.weight: 18.708199% change\n",
            "action_head.model.transformer_blocks.7.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.8.norm1.linear.weight: 22.321362% change\n",
            "action_head.model.transformer_blocks.8.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_q.weight: 22.197000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_k.weight: 26.189111% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_v.weight: 25.952296% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_out.0.weight: 19.319707% change\n",
            "action_head.model.transformer_blocks.8.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.8.ff.net.0.proj.weight: 21.956264% change\n",
            "action_head.model.transformer_blocks.8.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.8.ff.net.2.weight: 17.828333% change\n",
            "action_head.model.transformer_blocks.8.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.9.norm1.linear.weight: 20.596658% change\n",
            "action_head.model.transformer_blocks.9.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_q.weight: 24.136775% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_k.weight: 23.417524% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_v.weight: 22.125771% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_out.0.weight: 19.595551% change\n",
            "action_head.model.transformer_blocks.9.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.9.ff.net.0.proj.weight: 22.653518% change\n",
            "action_head.model.transformer_blocks.9.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.9.ff.net.2.weight: 18.501340% change\n",
            "action_head.model.transformer_blocks.9.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.10.norm1.linear.weight: 22.510931% change\n",
            "action_head.model.transformer_blocks.10.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_q.weight: 22.677538% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_k.weight: 26.326837% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_v.weight: 25.988401% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_out.0.weight: 19.714411% change\n",
            "action_head.model.transformer_blocks.10.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.10.ff.net.0.proj.weight: 21.955514% change\n",
            "action_head.model.transformer_blocks.10.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.10.ff.net.2.weight: 17.677893% change\n",
            "action_head.model.transformer_blocks.10.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.11.norm1.linear.weight: 20.742737% change\n",
            "action_head.model.transformer_blocks.11.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_q.weight: 24.684779% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_k.weight: 23.147751% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_v.weight: 22.674220% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_out.0.weight: 20.314737% change\n",
            "action_head.model.transformer_blocks.11.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.11.ff.net.0.proj.weight: 22.414852% change\n",
            "action_head.model.transformer_blocks.11.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.11.ff.net.2.weight: 17.640445% change\n",
            "action_head.model.transformer_blocks.11.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.12.norm1.linear.weight: 22.297778% change\n",
            "action_head.model.transformer_blocks.12.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_q.weight: 23.188793% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_k.weight: 26.124257% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_v.weight: 25.795783% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_out.0.weight: 19.814035% change\n",
            "action_head.model.transformer_blocks.12.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.12.ff.net.0.proj.weight: 22.348652% change\n",
            "action_head.model.transformer_blocks.12.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.12.ff.net.2.weight: 17.895987% change\n",
            "action_head.model.transformer_blocks.12.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.13.norm1.linear.weight: 20.385086% change\n",
            "action_head.model.transformer_blocks.13.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_q.weight: 24.655189% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_k.weight: 23.059378% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_v.weight: 23.196482% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_out.0.weight: 20.772142% change\n",
            "action_head.model.transformer_blocks.13.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.13.ff.net.0.proj.weight: 21.797962% change\n",
            "action_head.model.transformer_blocks.13.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.13.ff.net.2.weight: 15.942562% change\n",
            "action_head.model.transformer_blocks.13.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.14.norm1.linear.weight: 21.950333% change\n",
            "action_head.model.transformer_blocks.14.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_q.weight: 23.389582% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_k.weight: 26.110044% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_v.weight: 25.708151% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_out.0.weight: 20.056600% change\n",
            "action_head.model.transformer_blocks.14.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.14.ff.net.0.proj.weight: 22.245754% change\n",
            "action_head.model.transformer_blocks.14.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.14.ff.net.2.weight: 16.947854% change\n",
            "action_head.model.transformer_blocks.14.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.15.norm1.linear.weight: 18.716522% change\n",
            "action_head.model.transformer_blocks.15.norm1.linear.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_q.weight: 24.778066% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_q.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_k.weight: 23.319501% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_k.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_v.weight: 22.608034% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_v.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_out.0.weight: 22.337651% change\n",
            "action_head.model.transformer_blocks.15.attn1.to_out.0.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.15.ff.net.0.proj.weight: 19.758439% change\n",
            "action_head.model.transformer_blocks.15.ff.net.0.proj.bias: 0.000000% change\n",
            "action_head.model.transformer_blocks.15.ff.net.2.weight: 11.046073% change\n",
            "action_head.model.transformer_blocks.15.ff.net.2.bias: 0.000000% change\n",
            "action_head.model.proj_out_1.weight: 20.009212% change\n",
            "action_head.model.proj_out_1.bias: 0.000000% change\n",
            "action_head.model.proj_out_2.weight: 22.086637% change\n",
            "action_head.model.proj_out_2.bias: 0.000000% change\n",
            "action_head.state_encoder.layer1.W: 0.000000% change\n",
            "action_head.state_encoder.layer1.b: 0.000000% change\n",
            "action_head.state_encoder.layer2.W: 0.000000% change\n",
            "action_head.state_encoder.layer2.b: 0.000000% change\n",
            "action_head.action_encoder.W1.W: 0.000000% change\n",
            "action_head.action_encoder.W1.b: 0.000000% change\n",
            "action_head.action_encoder.W2.W: 0.000000% change\n",
            "action_head.action_encoder.W2.b: 0.000000% change\n",
            "action_head.action_encoder.W3.W: 0.000000% change\n",
            "action_head.action_encoder.W3.b: 0.000000% change\n",
            "action_head.action_decoder.layer1.W: 0.000000% change\n",
            "action_head.action_decoder.layer1.b: 0.000000% change\n",
            "action_head.action_decoder.layer2.W: 0.000000% change\n",
            "action_head.action_decoder.layer2.b: 0.000000% change\n",
            "action_head.position_embedding.weight: 0.000000% change\n",
            "\n",
            "Average percent change across 802 tensors: 10.993814%\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 15:49:49.977596: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 15:49:50.028672: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 15:49:50.028721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 15:49:50.030193: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 15:49:50.038008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 15:49:50.985784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/gr00t_pruned_50_complete/ \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7JXaUGX23RY",
        "outputId": "4a7c3aa1-76a9-43d3-cb70-530a77d30bad"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:10:38.905722: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:10:38.956351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:10:38.956410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:10:38.957731: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:10:38.965318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:10:40.112985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_pruned_50_complete/\n",
            "Loading pretrained dual brain from /content/gr00t_pruned_50_complete/\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_pruned_50_complete/\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 12150.371156879592\n",
            "MSE: 12150.371156879592\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 8632.299954886521\n",
            "MSE: 8632.299954886521\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 10845.41123624515\n",
            "MSE: 10845.41123624515\n",
            "Running trajectory: 3\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 8075.975359326901\n",
            "MSE: 8075.975359326901\n",
            "Running trajectory: 4\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 10916.095328447846\n",
            "MSE: 10916.095328447846\n",
            "Running trajectory: 5\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 15459.45210671279\n",
            "MSE: 15459.45210671279\n",
            "Running trajectory: 6\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 5651.708519597133\n",
            "MSE: 5651.708519597133\n",
            "Running trajectory: 7\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 9079.69529525908\n",
            "MSE: 9079.69529525908\n",
            "Running trajectory: 8\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 12998.510590447575\n",
            "MSE: 12998.510590447575\n",
            "Running trajectory: 9\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 10232.233778763304\n",
            "MSE: 10232.233778763304\n",
            "Running trajectory: 10\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 7074.590246247085\n",
            "MSE: 7074.590246247085\n",
            "Running trajectory: 11\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 9367.872600777811\n",
            "MSE: 9367.872600777811\n",
            "Running trajectory: 12\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 13948.243420801035\n",
            "MSE: 13948.243420801035\n",
            "Running trajectory: 13\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 9557.885537268758\n",
            "MSE: 9557.885537268758\n",
            "Running trajectory: 14\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 6864.477854702239\n",
            "MSE: 6864.477854702239\n",
            "Running trajectory: 15\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 8668.863840987799\n",
            "MSE: 8668.863840987799\n",
            "Running trajectory: 16\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 5133.844607350614\n",
            "MSE: 5133.844607350614\n",
            "Running trajectory: 17\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 7877.370290351217\n",
            "MSE: 7877.370290351217\n",
            "Running trajectory: 18\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 10246.32368230353\n",
            "MSE: 10246.32368230353\n",
            "Running trajectory: 19\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 9203.79766383231\n",
            "MSE: 9203.79766383231\n",
            "Running trajectory: 20\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 5204.587708649423\n",
            "/content/Isaac-GR00T/gr00t/utils/eval.py:95: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(nrows=num_of_joints, ncols=1, figsize=(8, 4 * num_of_joints))\n",
            "MSE: 5204.587708649423\n",
            "Running trajectory: 21\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 8643.709581724132\n",
            "MSE: 8643.709581724132\n",
            "Running trajectory: 22\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 10792.521852362632\n",
            "MSE: 10792.521852362632\n",
            "Running trajectory: 23\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 12688.197669928972\n",
            "MSE: 12688.197669928972\n",
            "Running trajectory: 24\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 6703.659241647282\n",
            "MSE: 6703.659241647282\n",
            "Running trajectory: 25\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 7871.194876016334\n",
            "MSE: 7871.194876016334\n",
            "Running trajectory: 26\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 9271.018027404965\n",
            "MSE: 9271.018027404965\n",
            "Running trajectory: 27\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 11582.593050725009\n",
            "MSE: 11582.593050725009\n",
            "Running trajectory: 28\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 8741.674767404706\n",
            "MSE: 8741.674767404706\n",
            "Running trajectory: 29\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 12058.301216589398\n",
            "MSE: 12058.301216589398\n",
            "Running trajectory: 30\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 8360.944002531964\n",
            "MSE: 8360.944002531964\n",
            " Total Inference Time: 149.53 seconds\n",
            "Average MSE across all trajs: 9480.755647295906\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wWO_KHFh3pIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2OFZE7d3pSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 3 dynamic INT8 quantization -failed"
      ],
      "metadata": {
        "id": "opJlgjJG3pog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "from safetensors.torch import save_file\n",
        "import os\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# Config\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "QUANT_MODEL_PATH = \"/content/gr00t_int8_dynamic\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "os.makedirs(QUANT_MODEL_PATH, exist_ok=True)\n",
        "\n",
        "# Load model\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "model = policy.model.eval().cpu()\n",
        "\n",
        "# Apply dynamic quantization to Linear layers\n",
        "quantized_model = quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save the quantized model\n",
        "clean_state = {}\n",
        "for k, v in quantized_model.state_dict().items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        clean_state[k] = v\n",
        "    else:\n",
        "        print(f\"Skipping non-tensor key: {k} ({type(v)})\")\n",
        "\n",
        "# Now save only valid entries\n",
        "from safetensors.torch import save_file\n",
        "save_file(clean_state, f\"{QUANT_MODEL_PATH}/model.safetensors\")\n",
        "\n",
        "# Copy config and index files\n",
        "for file in [\"config.json\", \"model.safetensors.index.json\"]:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(QUANT_MODEL_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        os.system(f\"cp {src} {dst}\")\n",
        "\n",
        "print(\" Dynamically quantized GR00T saved at:\", QUANT_MODEL_PATH)\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRfJj9HH32EK",
        "outputId": "da5e06e8-cb1c-44d5-9d2a-3c16be2159fb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.0.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.1.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.2.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.3.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.4.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.5.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.6.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.7.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.8.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.9.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.10.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.q_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.q_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.k_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.k_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.v_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.v_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.o_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.self_attn.o_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.mlp.gate_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.mlp.gate_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.mlp.up_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.mlp.up_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.mlp.down_proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.language_model.model.layers.11.mlp.down_proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.mlp1.1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.mlp1.1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.model.mlp1.3._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.model.mlp1.3._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: backbone.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: backbone.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.timestep_encoder.timestep_embedder.linear_1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.timestep_encoder.timestep_embedder.linear_1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.timestep_encoder.timestep_embedder.linear_2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.timestep_encoder.timestep_embedder.linear_2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.0.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.1.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.2.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.3.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.4.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.5.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.6.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.7.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.8.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.9.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.10.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.11.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.12.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.13.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.14.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.norm1.linear._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.norm1.linear._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_q._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_q._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_k._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_k._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_v._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_v._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_out.0._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.attn1.to_out.0._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.ff.net.0.proj._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.ff.net.0.proj._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.ff.net.2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.transformer_blocks.15.ff.net.2._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.proj_out_1._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.proj_out_1._packed_params._packed_params (<class 'tuple'>)\n",
            "Skipping non-tensor key: action_head.model.proj_out_2._packed_params.dtype (<class 'torch.dtype'>)\n",
            "Skipping non-tensor key: action_head.model.proj_out_2._packed_params._packed_params (<class 'tuple'>)\n",
            " Dynamically quantized GR00T saved at: /content/gr00t_int8_dynamic\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 15:56:34.105158: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 15:56:34.155878: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 15:56:34.155925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 15:56:34.157250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 15:56:34.164575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 15:56:35.105266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "from safetensors.torch import save_file\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from transformers import AutoConfig\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# Config\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "QUANT_MODEL_PATH = \"/content/gr00t_int8_dynamic_complete\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "os.makedirs(QUANT_MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\"), exist_ok=True)\n",
        "\n",
        "# Load model\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "model = policy.model.eval().cpu()\n",
        "\n",
        "# Quantize only Linear layers\n",
        "quantized_model = quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(\" Applied dynamic INT8 quantization to Linear layers\")\n",
        "\n",
        "# Clean and save the state_dict\n",
        "clean_state = {\n",
        "    k: v for k, v in quantized_model.state_dict().items()\n",
        "    if isinstance(v, torch.Tensor)\n",
        "}\n",
        "metadata = {\"format\": \"pt\", \"quantized\": \"dynamic\", \"dtype\": \"qint8\"}\n",
        "save_file(clean_state, f\"{QUANT_MODEL_PATH}/model.safetensors\", metadata=metadata)\n",
        "print(\" Saved quantized model in safetensors format\")\n",
        "\n",
        "# Save HuggingFace-style config with quantization info\n",
        "try:\n",
        "    config = AutoConfig.from_pretrained(ORIG_MODEL_PATH)\n",
        "    config.quantized = True\n",
        "    config.quantization = \"dynamic_int8\"\n",
        "    config.save_pretrained(QUANT_MODEL_PATH)\n",
        "    print(\" Saved AutoConfig with quantization info\")\n",
        "except Exception as e:\n",
        "    print(f\" Failed to save AutoConfig: {e}\")\n",
        "\n",
        "# Copy additional required files\n",
        "required_files = [\n",
        "    \"config.json\", \"special_tokens_map.json\",\n",
        "    \"tokenizer_config.json\", \"vocab.json\",\n",
        "    \"merges.txt\", \"tokenizer.json\", \"preprocessor_config.json\"\n",
        "]\n",
        "for file in required_files:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(QUANT_MODEL_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\" Copied {file}\")\n",
        "\n",
        "# Copy experiment_cfg or fallback metadata\n",
        "exp_cfg_src = os.path.join(ORIG_MODEL_PATH, \"experiment_cfg\")\n",
        "if os.path.exists(exp_cfg_src):\n",
        "    for file in os.listdir(exp_cfg_src):\n",
        "        src = os.path.join(exp_cfg_src, file)\n",
        "        dst = os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\", file)\n",
        "        shutil.copy2(src, dst)\n",
        "    print(\" Copied experiment_cfg files\")\n",
        "else:\n",
        "    fallback_locations = [\n",
        "        os.path.join(ORIG_MODEL_PATH, \"metadata.json\"),\n",
        "        \"/content/Isaac-GR00T/gr00t/experiment_cfg/metadata.json\"\n",
        "    ]\n",
        "    for loc in fallback_locations:\n",
        "        if os.path.exists(loc):\n",
        "            shutil.copy2(loc, os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\", \"metadata.json\"))\n",
        "            print(\" Copied fallback metadata.json\")\n",
        "            break\n",
        "    else:\n",
        "        print(\" metadata.json not found\")\n",
        "\n",
        "# (Optional) Reload verification\n",
        "try:\n",
        "    print(\"\\\\n Verifying quantized model reload...\")\n",
        "    test_policy = Gr00tPolicy(\n",
        "        model_path=QUANT_MODEL_PATH,\n",
        "        embodiment_tag=EMBODIMENT_TAG,\n",
        "        modality_config=modality_config,\n",
        "        modality_transform=modality_transform,\n",
        "    )\n",
        "    print(\" Verification successful: quantized model reloaded\")\n",
        "except Exception as e:\n",
        "    print(f\" Reload failed: {e}\")\n",
        "    print(\" Folder contents:\", os.listdir(QUANT_MODEL_PATH))\n",
        "    print(\" experiment_cfg contents:\", os.listdir(os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\")))\n",
        "    if os.path.exists(f\"{QUANT_MODEL_PATH}/config.json\"):\n",
        "        with open(f\"{QUANT_MODEL_PATH}/config.json\") as f:\n",
        "            print(\" config.json:\", json.load(f))\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gClix4lNG9Dq",
        "outputId": "fc28e0a5-ea97-47b1-938b-a66bba60f6b4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            " Applied dynamic INT8 quantization to Linear layers\n",
            " Saved quantized model in safetensors format\n",
            " Saved AutoConfig with quantization info\n",
            " Copied config.json\n",
            " Copied experiment_cfg files\n",
            "\n",
            "🔍 Verifying quantized model reload...\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_int8_dynamic_complete\n",
            "Loading pretrained dual brain from /content/gr00t_int8_dynamic_complete\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_int8_dynamic_complete\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            " Verification successful: quantized model reloaded\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 15:57:52.411531: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 15:57:52.460413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 15:57:52.460463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 15:57:52.461658: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 15:57:52.468704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 15:57:53.418063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "Some weights of the model checkpoint at /content/gr00t_int8_dynamic_complete were not used when initializing GR00T_N1: ['action_head.model.proj_out_1.scale', 'action_head.model.proj_out_1.zero_point', 'action_head.model.proj_out_2.scale', 'action_head.model.proj_out_2.zero_point', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.scale', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.zero_point', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.scale', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_k.scale', 'action_head.model.transformer_blocks.0.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.0.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_q.scale', 'action_head.model.transformer_blocks.0.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_v.scale', 'action_head.model.transformer_blocks.0.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.0.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.0.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.0.ff.net.2.scale', 'action_head.model.transformer_blocks.0.ff.net.2.zero_point', 'action_head.model.transformer_blocks.0.norm1.linear.scale', 'action_head.model.transformer_blocks.0.norm1.linear.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_k.scale', 'action_head.model.transformer_blocks.1.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.1.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_q.scale', 'action_head.model.transformer_blocks.1.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_v.scale', 'action_head.model.transformer_blocks.1.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.1.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.1.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.1.ff.net.2.scale', 'action_head.model.transformer_blocks.1.ff.net.2.zero_point', 'action_head.model.transformer_blocks.1.norm1.linear.scale', 'action_head.model.transformer_blocks.1.norm1.linear.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_k.scale', 'action_head.model.transformer_blocks.10.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.10.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_q.scale', 'action_head.model.transformer_blocks.10.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_v.scale', 'action_head.model.transformer_blocks.10.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.10.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.10.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.10.ff.net.2.scale', 'action_head.model.transformer_blocks.10.ff.net.2.zero_point', 'action_head.model.transformer_blocks.10.norm1.linear.scale', 'action_head.model.transformer_blocks.10.norm1.linear.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_k.scale', 'action_head.model.transformer_blocks.11.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.11.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_q.scale', 'action_head.model.transformer_blocks.11.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_v.scale', 'action_head.model.transformer_blocks.11.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.11.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.11.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.11.ff.net.2.scale', 'action_head.model.transformer_blocks.11.ff.net.2.zero_point', 'action_head.model.transformer_blocks.11.norm1.linear.scale', 'action_head.model.transformer_blocks.11.norm1.linear.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_k.scale', 'action_head.model.transformer_blocks.12.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.12.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_q.scale', 'action_head.model.transformer_blocks.12.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_v.scale', 'action_head.model.transformer_blocks.12.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.12.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.12.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.12.ff.net.2.scale', 'action_head.model.transformer_blocks.12.ff.net.2.zero_point', 'action_head.model.transformer_blocks.12.norm1.linear.scale', 'action_head.model.transformer_blocks.12.norm1.linear.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_k.scale', 'action_head.model.transformer_blocks.13.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.13.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_q.scale', 'action_head.model.transformer_blocks.13.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_v.scale', 'action_head.model.transformer_blocks.13.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.13.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.13.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.13.ff.net.2.scale', 'action_head.model.transformer_blocks.13.ff.net.2.zero_point', 'action_head.model.transformer_blocks.13.norm1.linear.scale', 'action_head.model.transformer_blocks.13.norm1.linear.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_k.scale', 'action_head.model.transformer_blocks.14.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.14.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_q.scale', 'action_head.model.transformer_blocks.14.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_v.scale', 'action_head.model.transformer_blocks.14.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.14.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.14.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.14.ff.net.2.scale', 'action_head.model.transformer_blocks.14.ff.net.2.zero_point', 'action_head.model.transformer_blocks.14.norm1.linear.scale', 'action_head.model.transformer_blocks.14.norm1.linear.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_k.scale', 'action_head.model.transformer_blocks.15.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.15.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_q.scale', 'action_head.model.transformer_blocks.15.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_v.scale', 'action_head.model.transformer_blocks.15.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.15.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.15.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.15.ff.net.2.scale', 'action_head.model.transformer_blocks.15.ff.net.2.zero_point', 'action_head.model.transformer_blocks.15.norm1.linear.scale', 'action_head.model.transformer_blocks.15.norm1.linear.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_k.scale', 'action_head.model.transformer_blocks.2.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.2.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_q.scale', 'action_head.model.transformer_blocks.2.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_v.scale', 'action_head.model.transformer_blocks.2.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.2.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.2.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.2.ff.net.2.scale', 'action_head.model.transformer_blocks.2.ff.net.2.zero_point', 'action_head.model.transformer_blocks.2.norm1.linear.scale', 'action_head.model.transformer_blocks.2.norm1.linear.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_k.scale', 'action_head.model.transformer_blocks.3.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.3.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_q.scale', 'action_head.model.transformer_blocks.3.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_v.scale', 'action_head.model.transformer_blocks.3.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.3.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.3.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.3.ff.net.2.scale', 'action_head.model.transformer_blocks.3.ff.net.2.zero_point', 'action_head.model.transformer_blocks.3.norm1.linear.scale', 'action_head.model.transformer_blocks.3.norm1.linear.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_k.scale', 'action_head.model.transformer_blocks.4.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.4.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_q.scale', 'action_head.model.transformer_blocks.4.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_v.scale', 'action_head.model.transformer_blocks.4.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.4.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.4.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.4.ff.net.2.scale', 'action_head.model.transformer_blocks.4.ff.net.2.zero_point', 'action_head.model.transformer_blocks.4.norm1.linear.scale', 'action_head.model.transformer_blocks.4.norm1.linear.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_k.scale', 'action_head.model.transformer_blocks.5.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.5.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_q.scale', 'action_head.model.transformer_blocks.5.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_v.scale', 'action_head.model.transformer_blocks.5.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.5.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.5.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.5.ff.net.2.scale', 'action_head.model.transformer_blocks.5.ff.net.2.zero_point', 'action_head.model.transformer_blocks.5.norm1.linear.scale', 'action_head.model.transformer_blocks.5.norm1.linear.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_k.scale', 'action_head.model.transformer_blocks.6.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.6.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_q.scale', 'action_head.model.transformer_blocks.6.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_v.scale', 'action_head.model.transformer_blocks.6.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.6.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.6.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.6.ff.net.2.scale', 'action_head.model.transformer_blocks.6.ff.net.2.zero_point', 'action_head.model.transformer_blocks.6.norm1.linear.scale', 'action_head.model.transformer_blocks.6.norm1.linear.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_k.scale', 'action_head.model.transformer_blocks.7.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.7.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_q.scale', 'action_head.model.transformer_blocks.7.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_v.scale', 'action_head.model.transformer_blocks.7.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.7.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.7.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.7.ff.net.2.scale', 'action_head.model.transformer_blocks.7.ff.net.2.zero_point', 'action_head.model.transformer_blocks.7.norm1.linear.scale', 'action_head.model.transformer_blocks.7.norm1.linear.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_k.scale', 'action_head.model.transformer_blocks.8.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.8.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_q.scale', 'action_head.model.transformer_blocks.8.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_v.scale', 'action_head.model.transformer_blocks.8.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.8.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.8.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.8.ff.net.2.scale', 'action_head.model.transformer_blocks.8.ff.net.2.zero_point', 'action_head.model.transformer_blocks.8.norm1.linear.scale', 'action_head.model.transformer_blocks.8.norm1.linear.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_k.scale', 'action_head.model.transformer_blocks.9.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.9.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_q.scale', 'action_head.model.transformer_blocks.9.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_v.scale', 'action_head.model.transformer_blocks.9.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.9.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.9.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.9.ff.net.2.scale', 'action_head.model.transformer_blocks.9.ff.net.2.zero_point', 'action_head.model.transformer_blocks.9.norm1.linear.scale', 'action_head.model.transformer_blocks.9.norm1.linear.zero_point', 'backbone.linear.scale', 'backbone.linear.zero_point', 'backbone.model.language_model.model.layers.0.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.0.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.zero_point', 'backbone.model.mlp1.1.scale', 'backbone.model.mlp1.1.zero_point', 'backbone.model.mlp1.3.scale', 'backbone.model.mlp1.3.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.zero_point']\n",
            "- This IS expected if you are initializing GR00T_N1 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GR00T_N1 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GR00T_N1 were not initialized from the model checkpoint at /content/gr00t_int8_dynamic_complete and are newly initialized: ['action_head.model.proj_out_1.bias', 'action_head.model.proj_out_1.weight', 'action_head.model.proj_out_2.bias', 'action_head.model.proj_out_2.weight', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.bias', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.weight', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.bias', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.weight', 'action_head.model.transformer_blocks.0.attn1.to_k.bias', 'action_head.model.transformer_blocks.0.attn1.to_k.weight', 'action_head.model.transformer_blocks.0.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.0.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.0.attn1.to_q.bias', 'action_head.model.transformer_blocks.0.attn1.to_q.weight', 'action_head.model.transformer_blocks.0.attn1.to_v.bias', 'action_head.model.transformer_blocks.0.attn1.to_v.weight', 'action_head.model.transformer_blocks.0.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.0.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.0.ff.net.2.bias', 'action_head.model.transformer_blocks.0.ff.net.2.weight', 'action_head.model.transformer_blocks.0.norm1.linear.bias', 'action_head.model.transformer_blocks.0.norm1.linear.weight', 'action_head.model.transformer_blocks.1.attn1.to_k.bias', 'action_head.model.transformer_blocks.1.attn1.to_k.weight', 'action_head.model.transformer_blocks.1.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.1.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.1.attn1.to_q.bias', 'action_head.model.transformer_blocks.1.attn1.to_q.weight', 'action_head.model.transformer_blocks.1.attn1.to_v.bias', 'action_head.model.transformer_blocks.1.attn1.to_v.weight', 'action_head.model.transformer_blocks.1.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.1.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.1.ff.net.2.bias', 'action_head.model.transformer_blocks.1.ff.net.2.weight', 'action_head.model.transformer_blocks.1.norm1.linear.bias', 'action_head.model.transformer_blocks.1.norm1.linear.weight', 'action_head.model.transformer_blocks.10.attn1.to_k.bias', 'action_head.model.transformer_blocks.10.attn1.to_k.weight', 'action_head.model.transformer_blocks.10.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.10.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.10.attn1.to_q.bias', 'action_head.model.transformer_blocks.10.attn1.to_q.weight', 'action_head.model.transformer_blocks.10.attn1.to_v.bias', 'action_head.model.transformer_blocks.10.attn1.to_v.weight', 'action_head.model.transformer_blocks.10.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.10.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.10.ff.net.2.bias', 'action_head.model.transformer_blocks.10.ff.net.2.weight', 'action_head.model.transformer_blocks.10.norm1.linear.bias', 'action_head.model.transformer_blocks.10.norm1.linear.weight', 'action_head.model.transformer_blocks.11.attn1.to_k.bias', 'action_head.model.transformer_blocks.11.attn1.to_k.weight', 'action_head.model.transformer_blocks.11.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.11.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.11.attn1.to_q.bias', 'action_head.model.transformer_blocks.11.attn1.to_q.weight', 'action_head.model.transformer_blocks.11.attn1.to_v.bias', 'action_head.model.transformer_blocks.11.attn1.to_v.weight', 'action_head.model.transformer_blocks.11.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.11.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.11.ff.net.2.bias', 'action_head.model.transformer_blocks.11.ff.net.2.weight', 'action_head.model.transformer_blocks.11.norm1.linear.bias', 'action_head.model.transformer_blocks.11.norm1.linear.weight', 'action_head.model.transformer_blocks.12.attn1.to_k.bias', 'action_head.model.transformer_blocks.12.attn1.to_k.weight', 'action_head.model.transformer_blocks.12.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.12.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.12.attn1.to_q.bias', 'action_head.model.transformer_blocks.12.attn1.to_q.weight', 'action_head.model.transformer_blocks.12.attn1.to_v.bias', 'action_head.model.transformer_blocks.12.attn1.to_v.weight', 'action_head.model.transformer_blocks.12.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.12.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.12.ff.net.2.bias', 'action_head.model.transformer_blocks.12.ff.net.2.weight', 'action_head.model.transformer_blocks.12.norm1.linear.bias', 'action_head.model.transformer_blocks.12.norm1.linear.weight', 'action_head.model.transformer_blocks.13.attn1.to_k.bias', 'action_head.model.transformer_blocks.13.attn1.to_k.weight', 'action_head.model.transformer_blocks.13.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.13.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.13.attn1.to_q.bias', 'action_head.model.transformer_blocks.13.attn1.to_q.weight', 'action_head.model.transformer_blocks.13.attn1.to_v.bias', 'action_head.model.transformer_blocks.13.attn1.to_v.weight', 'action_head.model.transformer_blocks.13.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.13.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.13.ff.net.2.bias', 'action_head.model.transformer_blocks.13.ff.net.2.weight', 'action_head.model.transformer_blocks.13.norm1.linear.bias', 'action_head.model.transformer_blocks.13.norm1.linear.weight', 'action_head.model.transformer_blocks.14.attn1.to_k.bias', 'action_head.model.transformer_blocks.14.attn1.to_k.weight', 'action_head.model.transformer_blocks.14.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.14.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.14.attn1.to_q.bias', 'action_head.model.transformer_blocks.14.attn1.to_q.weight', 'action_head.model.transformer_blocks.14.attn1.to_v.bias', 'action_head.model.transformer_blocks.14.attn1.to_v.weight', 'action_head.model.transformer_blocks.14.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.14.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.14.ff.net.2.bias', 'action_head.model.transformer_blocks.14.ff.net.2.weight', 'action_head.model.transformer_blocks.14.norm1.linear.bias', 'action_head.model.transformer_blocks.14.norm1.linear.weight', 'action_head.model.transformer_blocks.15.attn1.to_k.bias', 'action_head.model.transformer_blocks.15.attn1.to_k.weight', 'action_head.model.transformer_blocks.15.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.15.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.15.attn1.to_q.bias', 'action_head.model.transformer_blocks.15.attn1.to_q.weight', 'action_head.model.transformer_blocks.15.attn1.to_v.bias', 'action_head.model.transformer_blocks.15.attn1.to_v.weight', 'action_head.model.transformer_blocks.15.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.15.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.15.ff.net.2.bias', 'action_head.model.transformer_blocks.15.ff.net.2.weight', 'action_head.model.transformer_blocks.15.norm1.linear.bias', 'action_head.model.transformer_blocks.15.norm1.linear.weight', 'action_head.model.transformer_blocks.2.attn1.to_k.bias', 'action_head.model.transformer_blocks.2.attn1.to_k.weight', 'action_head.model.transformer_blocks.2.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.2.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.2.attn1.to_q.bias', 'action_head.model.transformer_blocks.2.attn1.to_q.weight', 'action_head.model.transformer_blocks.2.attn1.to_v.bias', 'action_head.model.transformer_blocks.2.attn1.to_v.weight', 'action_head.model.transformer_blocks.2.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.2.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.2.ff.net.2.bias', 'action_head.model.transformer_blocks.2.ff.net.2.weight', 'action_head.model.transformer_blocks.2.norm1.linear.bias', 'action_head.model.transformer_blocks.2.norm1.linear.weight', 'action_head.model.transformer_blocks.3.attn1.to_k.bias', 'action_head.model.transformer_blocks.3.attn1.to_k.weight', 'action_head.model.transformer_blocks.3.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.3.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.3.attn1.to_q.bias', 'action_head.model.transformer_blocks.3.attn1.to_q.weight', 'action_head.model.transformer_blocks.3.attn1.to_v.bias', 'action_head.model.transformer_blocks.3.attn1.to_v.weight', 'action_head.model.transformer_blocks.3.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.3.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.3.ff.net.2.bias', 'action_head.model.transformer_blocks.3.ff.net.2.weight', 'action_head.model.transformer_blocks.3.norm1.linear.bias', 'action_head.model.transformer_blocks.3.norm1.linear.weight', 'action_head.model.transformer_blocks.4.attn1.to_k.bias', 'action_head.model.transformer_blocks.4.attn1.to_k.weight', 'action_head.model.transformer_blocks.4.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.4.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.4.attn1.to_q.bias', 'action_head.model.transformer_blocks.4.attn1.to_q.weight', 'action_head.model.transformer_blocks.4.attn1.to_v.bias', 'action_head.model.transformer_blocks.4.attn1.to_v.weight', 'action_head.model.transformer_blocks.4.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.4.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.4.ff.net.2.bias', 'action_head.model.transformer_blocks.4.ff.net.2.weight', 'action_head.model.transformer_blocks.4.norm1.linear.bias', 'action_head.model.transformer_blocks.4.norm1.linear.weight', 'action_head.model.transformer_blocks.5.attn1.to_k.bias', 'action_head.model.transformer_blocks.5.attn1.to_k.weight', 'action_head.model.transformer_blocks.5.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.5.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.5.attn1.to_q.bias', 'action_head.model.transformer_blocks.5.attn1.to_q.weight', 'action_head.model.transformer_blocks.5.attn1.to_v.bias', 'action_head.model.transformer_blocks.5.attn1.to_v.weight', 'action_head.model.transformer_blocks.5.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.5.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.5.ff.net.2.bias', 'action_head.model.transformer_blocks.5.ff.net.2.weight', 'action_head.model.transformer_blocks.5.norm1.linear.bias', 'action_head.model.transformer_blocks.5.norm1.linear.weight', 'action_head.model.transformer_blocks.6.attn1.to_k.bias', 'action_head.model.transformer_blocks.6.attn1.to_k.weight', 'action_head.model.transformer_blocks.6.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.6.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.6.attn1.to_q.bias', 'action_head.model.transformer_blocks.6.attn1.to_q.weight', 'action_head.model.transformer_blocks.6.attn1.to_v.bias', 'action_head.model.transformer_blocks.6.attn1.to_v.weight', 'action_head.model.transformer_blocks.6.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.6.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.6.ff.net.2.bias', 'action_head.model.transformer_blocks.6.ff.net.2.weight', 'action_head.model.transformer_blocks.6.norm1.linear.bias', 'action_head.model.transformer_blocks.6.norm1.linear.weight', 'action_head.model.transformer_blocks.7.attn1.to_k.bias', 'action_head.model.transformer_blocks.7.attn1.to_k.weight', 'action_head.model.transformer_blocks.7.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.7.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.7.attn1.to_q.bias', 'action_head.model.transformer_blocks.7.attn1.to_q.weight', 'action_head.model.transformer_blocks.7.attn1.to_v.bias', 'action_head.model.transformer_blocks.7.attn1.to_v.weight', 'action_head.model.transformer_blocks.7.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.7.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.7.ff.net.2.bias', 'action_head.model.transformer_blocks.7.ff.net.2.weight', 'action_head.model.transformer_blocks.7.norm1.linear.bias', 'action_head.model.transformer_blocks.7.norm1.linear.weight', 'action_head.model.transformer_blocks.8.attn1.to_k.bias', 'action_head.model.transformer_blocks.8.attn1.to_k.weight', 'action_head.model.transformer_blocks.8.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.8.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.8.attn1.to_q.bias', 'action_head.model.transformer_blocks.8.attn1.to_q.weight', 'action_head.model.transformer_blocks.8.attn1.to_v.bias', 'action_head.model.transformer_blocks.8.attn1.to_v.weight', 'action_head.model.transformer_blocks.8.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.8.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.8.ff.net.2.bias', 'action_head.model.transformer_blocks.8.ff.net.2.weight', 'action_head.model.transformer_blocks.8.norm1.linear.bias', 'action_head.model.transformer_blocks.8.norm1.linear.weight', 'action_head.model.transformer_blocks.9.attn1.to_k.bias', 'action_head.model.transformer_blocks.9.attn1.to_k.weight', 'action_head.model.transformer_blocks.9.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.9.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.9.attn1.to_q.bias', 'action_head.model.transformer_blocks.9.attn1.to_q.weight', 'action_head.model.transformer_blocks.9.attn1.to_v.bias', 'action_head.model.transformer_blocks.9.attn1.to_v.weight', 'action_head.model.transformer_blocks.9.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.9.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.9.ff.net.2.bias', 'action_head.model.transformer_blocks.9.ff.net.2.weight', 'action_head.model.transformer_blocks.9.norm1.linear.bias', 'action_head.model.transformer_blocks.9.norm1.linear.weight', 'backbone.linear.bias', 'backbone.linear.weight', 'backbone.model.language_model.model.layers.0.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.weight', 'backbone.model.mlp1.1.bias', 'backbone.model.mlp1.1.weight', 'backbone.model.mlp1.3.bias', 'backbone.model.mlp1.3.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK_ZxCXKqsjx",
        "outputId": "32759f9c-94d0-4c32-8826-6e3164a84116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How much weight changes"
      ],
      "metadata": {
        "id": "ZmkXOvtYUTE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "import copy\n",
        "\n",
        "# Config\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "# Load GR00T\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "orig_model = policy.model.eval().cpu()\n",
        "\n",
        "# Count original Linear layers\n",
        "orig_linear_count = sum(1 for m in orig_model.modules() if isinstance(m, torch.nn.Linear))\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quant_model = quantize_dynamic(copy.deepcopy(orig_model), {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Count quantized Linear layers\n",
        "quant_linear_count = sum(1 for m in quant_model.modules() if isinstance(m, torch.nn.quantized.dynamic.Linear))\n",
        "\n",
        "print(f\"Original model Linear layers: {orig_linear_count}\")\n",
        "print(f\"Quantized model Linear layers: {quant_linear_count}\")\n",
        "print(f\"Successfully quantized: {quant_linear_count} of {orig_linear_count} ({(quant_linear_count / orig_linear_count) * 100:.2f}%)\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_W-rXzvIhq",
        "outputId": "ac400055-70f9-47bf-a981-fe9d01f96513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Original model Linear layers: 365\n",
            "Quantized model Linear layers: 365\n",
            "Successfully quantized: 365 of 365 (100.00%)\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 10:02:54.255559: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 10:02:54.304996: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 10:02:54.305038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 10:02:54.306442: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 10:02:54.314070: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 10:02:55.233079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.29it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backbone quantization _ final one for int8 dynamic language only"
      ],
      "metadata": {
        "id": "74RmCBtqLNtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "from safetensors.torch import save_file\n",
        "from transformers import AutoConfig\n",
        "import os, json, shutil\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "import copy\n",
        "\n",
        "# === Config ===\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "QUANT_MODEL_PATH = \"/content/gr00t_int8_language_only\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "os.makedirs(QUANT_MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\"), exist_ok=True)\n",
        "\n",
        "# === Load FP32 GR00T model ===\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "fp32_model = policy.model.eval().cpu()\n",
        "\n",
        "# === Quantize only the language_model inside backbone ===\n",
        "if hasattr(fp32_model, \"backbone\") and hasattr(fp32_model.backbone.model, \"language_model\"):\n",
        "    language_model = fp32_model.backbone.model.language_model\n",
        "    fp32_model.backbone.model.language_model = quantize_dynamic(\n",
        "        copy.deepcopy(language_model),\n",
        "        {torch.nn.Linear},\n",
        "        dtype=torch.qint8\n",
        "    )\n",
        "    print(\" Quantized language_model only.\")\n",
        "else:\n",
        "    raise ValueError(\" Could not find language_model inside backbone.\")\n",
        "\n",
        "# === Count quantized layers ===\n",
        "total_layers = sum(1 for _ in fp32_model.modules())\n",
        "linear_layers = sum(1 for m in fp32_model.modules() if isinstance(m, torch.nn.Linear))\n",
        "quantized_linear_layers = sum(1 for m in fp32_model.modules() if isinstance(m, torch.nn.quantized.dynamic.Linear))\n",
        "quantized_layer_names = [name for name, m in fp32_model.named_modules() if isinstance(m, torch.nn.quantized.dynamic.Linear)]\n",
        "\n",
        "# === Save quantized model ===\n",
        "clean_state = {k: v for k, v in fp32_model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
        "metadata = {\"format\": \"pt\", \"quantized\": \"dynamic\", \"dtype\": \"qint8\", \"scope\": \"language_model_only\"}\n",
        "save_file(clean_state, f\"{QUANT_MODEL_PATH}/model.safetensors\", metadata=metadata)\n",
        "\n",
        "# === Save config ===\n",
        "try:\n",
        "    config = AutoConfig.from_pretrained(ORIG_MODEL_PATH)\n",
        "    config.quantized = True\n",
        "    config.quantization = \"dynamic_int8_language_only\"\n",
        "    config.save_pretrained(QUANT_MODEL_PATH)\n",
        "except Exception as e:\n",
        "    print(f\" Config saving failed: {e}\")\n",
        "\n",
        "# === Copy supporting files ===\n",
        "required_files = [\n",
        "    \"config.json\", \"special_tokens_map.json\",\n",
        "    \"tokenizer_config.json\", \"vocab.json\",\n",
        "    \"merges.txt\", \"tokenizer.json\", \"preprocessor_config.json\"\n",
        "]\n",
        "for file in required_files:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(QUANT_MODEL_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "# === Copy metadata/experiment_cfg ===\n",
        "exp_cfg_src = os.path.join(ORIG_MODEL_PATH, \"experiment_cfg\")\n",
        "if os.path.exists(exp_cfg_src):\n",
        "    for file in os.listdir(exp_cfg_src):\n",
        "        shutil.copy2(os.path.join(exp_cfg_src, file), os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\", file))\n",
        "else:\n",
        "    fallback = [\n",
        "        os.path.join(ORIG_MODEL_PATH, \"metadata.json\"),\n",
        "        \"/content/Isaac-GR00T/gr00t/experiment_cfg/metadata.json\"\n",
        "    ]\n",
        "    for loc in fallback:\n",
        "        if os.path.exists(loc):\n",
        "            shutil.copy2(loc, os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\", \"metadata.json\"))\n",
        "            break\n",
        "\n",
        "# === Save quantization report ===\n",
        "report = {\n",
        "    \"total_layers\": total_layers,\n",
        "    \"linear_layers\": linear_layers,\n",
        "    \"quantized_linear_layers\": quantized_linear_layers,\n",
        "    \"quantization_success_rate\": round(quantized_linear_layers / linear_layers * 100, 2),\n",
        "    \"quantized_layer_names\": quantized_layer_names\n",
        "}\n",
        "with open(os.path.join(QUANT_MODEL_PATH, \"quantization_report.json\"), \"w\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "# === Run forward pass with dummy observation ===\n",
        "obs = {}\n",
        "if hasattr(modality_config, \"modalities\") and hasattr(modality_config, \"get_spec\"):\n",
        "    for key in modality_config.modalities:\n",
        "        cfg = modality_config.get_spec(key)\n",
        "        shape = getattr(cfg, \"shape\", [1])\n",
        "        dtype_str = getattr(cfg, \"dtype\", \"float32\")\n",
        "        dtype = torch.float32 if \"float\" in dtype_str else torch.int32\n",
        "        obs[key] = torch.zeros(shape, dtype=dtype)\n",
        "else:\n",
        "    raise ValueError(\"modality_config must support 'modalities' and 'get_spec'\")\n",
        "\n",
        "obs_transformed = modality_transform(obs)\n",
        "with torch.no_grad():\n",
        "    try:\n",
        "        out = policy(obs_transformed)\n",
        "        if isinstance(out, torch.Tensor) and torch.isnan(out).any():\n",
        "            print(\" Output contains NaNs\")\n",
        "        else:\n",
        "            print(\" Output is numerically stable\")\n",
        "            print(\"Sample output:\", out if isinstance(out, torch.Tensor) else str(out))\n",
        "    except Exception as e:\n",
        "        print(\" Exception during forward pass:\", e)\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\\\nGR00T (language-model-only quantized) saved at:\", QUANT_MODEL_PATH)\n",
        "print(\"Quantization Report:\")\n",
        "print(json.dumps(report, indent=2))\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\\\n\", result.stdout)\n",
        "print(\"STDERR:\\\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzUxfai0NXP7",
        "outputId": "4e80c384-0992-419c-8335-7a83ed8ffc89"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\\n Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            " Quantized language_model only.\n",
            "\n",
            "STDERR:\\n /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:15:33.445258: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:15:33.496541: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:15:33.496610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:15:33.498004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:15:33.505629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:15:34.449112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 113, in <module>\n",
            "ValueError: modality_config must support 'modalities' and 'get_spec'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/gr00t_int8_language_only \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb1_ub_BJfUy",
        "outputId": "d06a448a-bfd5-47e9-b8f8-eccbbf7ea0c1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:16:40.047919: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:16:40.097281: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:16:40.097339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:16:40.098593: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:16:40.105774: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:16:41.260705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_int8_language_only\n",
            "Loading pretrained dual brain from /content/gr00t_int8_language_only\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_int8_language_only\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Some weights of the model checkpoint at /content/gr00t_int8_language_only were not used when initializing GR00T_N1: ['backbone.model.language_model.model.layers.0.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.0.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.zero_point']\n",
            "- This IS expected if you are initializing GR00T_N1 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GR00T_N1 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GR00T_N1 were not initialized from the model checkpoint at /content/gr00t_int8_language_only and are newly initialized: ['backbone.model.language_model.model.layers.0.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 3\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 4\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 5\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 6\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 7\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 8\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 9\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 10\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 11\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 12\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 13\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 14\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 15\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 16\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 17\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 18\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 19\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 20\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "/content/Isaac-GR00T/gr00t/utils/eval.py:95: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(nrows=num_of_joints, ncols=1, figsize=(8, 4 * num_of_joints))\n",
            "MSE: nan\n",
            "Running trajectory: 21\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 22\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 23\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 24\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 25\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 26\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 27\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 28\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 29\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 30\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            " Total Inference Time: 150.64 seconds\n",
            "Average MSE across all trajs: nan\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "src_dir = \"/content/gr00t_int8_language_only\"\n",
        "dst_dir = \"/content/drive/MyDrive/gr00t_models/gr00t_int8_language_only/\"\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files in the source directory\n",
        "for filename in os.listdir(src_dir):\n",
        "    src_path = os.path.join(src_dir, filename)\n",
        "    dst_path = os.path.join(dst_dir, filename)\n",
        "\n",
        "    # Move each file\n",
        "    if os.path.isfile(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "        print(f\" Moved {filename} to: {dst_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2__0FyYPOdX",
        "outputId": "712824ad-597c-42cd-8f4b-0cba449189e3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Moved model.safetensors to: /content/drive/MyDrive/gr00t_models/gr00t_int8_language_only/model.safetensors\n",
            " Moved config.json to: /content/drive/MyDrive/gr00t_models/gr00t_int8_language_only/config.json\n",
            " Moved quantization_report.json to: /content/drive/MyDrive/gr00t_models/gr00t_int8_language_only/quantization_report.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HfC0jrNcPOy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 3b)  Same int 8 but minimalistic - Dont run this i will prefer the previous one"
      ],
      "metadata": {
        "id": "RG1-pJosPO5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "from safetensors.torch import save_file\n",
        "from transformers import AutoConfig\n",
        "import os, json, shutil\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# === Config ===\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "QUANT_MODEL_PATH = \"/content/int8_minimal\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "os.makedirs(QUANT_MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\"), exist_ok=True)\n",
        "\n",
        "# === Load original GR00T model ===\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "model = policy.model.eval().cpu()\n",
        "\n",
        "# === Quantize all Linear layers ===\n",
        "quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "print(\"✓ Applied dynamic quantization to Linear layers\")\n",
        "\n",
        "# === Save quantized model ===\n",
        "clean_state = {k: v for k, v in quantized_model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
        "save_file(clean_state, f\"{QUANT_MODEL_PATH}/model.safetensors\", metadata={\"format\": \"pt\", \"quantized\": \"dynamic\", \"dtype\": \"qint8\"})\n",
        "print(\"✓ Saved model.safetensors\")\n",
        "\n",
        "# === Save HuggingFace config ===\n",
        "try:\n",
        "    config = AutoConfig.from_pretrained(ORIG_MODEL_PATH)\n",
        "    config.quantized = True\n",
        "    config.quantization = \"dynamic_int8\"\n",
        "    config.save_pretrained(QUANT_MODEL_PATH)\n",
        "    print(\"Saved config.json\")\n",
        "except Exception as e:\n",
        "    print(\" Failed to save config:\", e)\n",
        "\n",
        "# === Copy tokenizer and preprocessor files ===\n",
        "required_files = [\n",
        "    \"special_tokens_map.json\", \"tokenizer_config.json\",\n",
        "    \"vocab.json\", \"merges.txt\", \"tokenizer.json\",\n",
        "    \"preprocessor_config.json\"\n",
        "]\n",
        "for file in required_files:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(QUANT_MODEL_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"✓ Copied {file}\")\n",
        "\n",
        "# === Copy experiment_cfg or fallback metadata ===\n",
        "exp_cfg_src = os.path.join(ORIG_MODEL_PATH, \"experiment_cfg\")\n",
        "exp_cfg_dst = os.path.join(QUANT_MODEL_PATH, \"experiment_cfg\")\n",
        "\n",
        "if os.path.exists(exp_cfg_src):\n",
        "    for file in os.listdir(exp_cfg_src):\n",
        "        shutil.copy2(os.path.join(exp_cfg_src, file), os.path.join(exp_cfg_dst, file))\n",
        "    print(\"✓ Copied experiment_cfg\")\n",
        "else:\n",
        "    fallback = [\n",
        "        os.path.join(ORIG_MODEL_PATH, \"metadata.json\"),\n",
        "        \"/content/Isaac-GR00T/gr00t/experiment_cfg/metadata.json\"\n",
        "    ]\n",
        "    for loc in fallback:\n",
        "        if os.path.exists(loc):\n",
        "            shutil.copy2(loc, os.path.join(exp_cfg_dst, \"metadata.json\"))\n",
        "            print(\"✓ Copied fallback metadata.json\")\n",
        "            break\n",
        "\n",
        "# === Dummy forward pass for sanity check ===\n",
        "obs = {}\n",
        "if hasattr(modality_config, \"modalities\") and hasattr(modality_config, \"get_spec\"):\n",
        "    for key in modality_config.modalities:\n",
        "        cfg = modality_config.get_spec(key)\n",
        "        shape = getattr(cfg, \"shape\", [1])\n",
        "        dtype_str = getattr(cfg, \"dtype\", \"float32\")\n",
        "        dtype = torch.float32 if \"float\" in dtype_str else torch.int32\n",
        "        obs[key] = torch.zeros(shape, dtype=dtype)\n",
        "    obs_transformed = modality_transform(obs)\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            out = policy(obs_transformed)\n",
        "            print(\"✓ Dummy forward pass successful\")\n",
        "        except Exception as e:\n",
        "            print(\"✗ Dummy forward pass failed:\", e)\n",
        "else:\n",
        "    print(\"✗ Cannot generate dummy observation (missing modalities/get_spec)\")\n",
        "\n",
        "print(\"\\\\n=== DONE ===\")\n",
        "print(\"Quantized model saved at:\", QUANT_MODEL_PATH)\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h25vusekRfAr",
        "outputId": "06697439-ea20-4d18-df07-61afb3e35c5a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "✓ Applied dynamic quantization to Linear layers\n",
            "✓ Saved model.safetensors\n",
            "Saved config.json\n",
            "✓ Copied experiment_cfg\n",
            "✗ Cannot generate dummy observation (missing modalities/get_spec)\n",
            "\n",
            "=== DONE ===\n",
            "Quantized model saved at: /content/int8_minimal\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 16:47:33.803599: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 16:47:33.853116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 16:47:33.853169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 16:47:33.854443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 16:47:33.861650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 16:47:34.804807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/int8_minimal \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N23hztvdTD1E",
        "outputId": "e1bc709c-6726-4983-f698-ce266e9727ab"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 16:49:43.785210: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 16:49:43.835616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 16:49:43.835668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 16:49:43.836971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 16:49:43.844074: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 16:49:45.003815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/int8_minimal\n",
            "Loading pretrained dual brain from /content/int8_minimal\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/int8_minimal\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Some weights of the model checkpoint at /content/int8_minimal were not used when initializing GR00T_N1: ['action_head.model.proj_out_1.scale', 'action_head.model.proj_out_1.zero_point', 'action_head.model.proj_out_2.scale', 'action_head.model.proj_out_2.zero_point', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.scale', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.zero_point', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.scale', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_k.scale', 'action_head.model.transformer_blocks.0.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.0.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_q.scale', 'action_head.model.transformer_blocks.0.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.0.attn1.to_v.scale', 'action_head.model.transformer_blocks.0.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.0.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.0.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.0.ff.net.2.scale', 'action_head.model.transformer_blocks.0.ff.net.2.zero_point', 'action_head.model.transformer_blocks.0.norm1.linear.scale', 'action_head.model.transformer_blocks.0.norm1.linear.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_k.scale', 'action_head.model.transformer_blocks.1.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.1.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_q.scale', 'action_head.model.transformer_blocks.1.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.1.attn1.to_v.scale', 'action_head.model.transformer_blocks.1.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.1.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.1.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.1.ff.net.2.scale', 'action_head.model.transformer_blocks.1.ff.net.2.zero_point', 'action_head.model.transformer_blocks.1.norm1.linear.scale', 'action_head.model.transformer_blocks.1.norm1.linear.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_k.scale', 'action_head.model.transformer_blocks.10.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.10.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_q.scale', 'action_head.model.transformer_blocks.10.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.10.attn1.to_v.scale', 'action_head.model.transformer_blocks.10.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.10.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.10.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.10.ff.net.2.scale', 'action_head.model.transformer_blocks.10.ff.net.2.zero_point', 'action_head.model.transformer_blocks.10.norm1.linear.scale', 'action_head.model.transformer_blocks.10.norm1.linear.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_k.scale', 'action_head.model.transformer_blocks.11.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.11.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_q.scale', 'action_head.model.transformer_blocks.11.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.11.attn1.to_v.scale', 'action_head.model.transformer_blocks.11.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.11.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.11.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.11.ff.net.2.scale', 'action_head.model.transformer_blocks.11.ff.net.2.zero_point', 'action_head.model.transformer_blocks.11.norm1.linear.scale', 'action_head.model.transformer_blocks.11.norm1.linear.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_k.scale', 'action_head.model.transformer_blocks.12.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.12.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_q.scale', 'action_head.model.transformer_blocks.12.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.12.attn1.to_v.scale', 'action_head.model.transformer_blocks.12.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.12.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.12.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.12.ff.net.2.scale', 'action_head.model.transformer_blocks.12.ff.net.2.zero_point', 'action_head.model.transformer_blocks.12.norm1.linear.scale', 'action_head.model.transformer_blocks.12.norm1.linear.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_k.scale', 'action_head.model.transformer_blocks.13.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.13.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_q.scale', 'action_head.model.transformer_blocks.13.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.13.attn1.to_v.scale', 'action_head.model.transformer_blocks.13.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.13.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.13.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.13.ff.net.2.scale', 'action_head.model.transformer_blocks.13.ff.net.2.zero_point', 'action_head.model.transformer_blocks.13.norm1.linear.scale', 'action_head.model.transformer_blocks.13.norm1.linear.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_k.scale', 'action_head.model.transformer_blocks.14.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.14.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_q.scale', 'action_head.model.transformer_blocks.14.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.14.attn1.to_v.scale', 'action_head.model.transformer_blocks.14.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.14.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.14.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.14.ff.net.2.scale', 'action_head.model.transformer_blocks.14.ff.net.2.zero_point', 'action_head.model.transformer_blocks.14.norm1.linear.scale', 'action_head.model.transformer_blocks.14.norm1.linear.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_k.scale', 'action_head.model.transformer_blocks.15.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.15.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_q.scale', 'action_head.model.transformer_blocks.15.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.15.attn1.to_v.scale', 'action_head.model.transformer_blocks.15.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.15.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.15.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.15.ff.net.2.scale', 'action_head.model.transformer_blocks.15.ff.net.2.zero_point', 'action_head.model.transformer_blocks.15.norm1.linear.scale', 'action_head.model.transformer_blocks.15.norm1.linear.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_k.scale', 'action_head.model.transformer_blocks.2.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.2.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_q.scale', 'action_head.model.transformer_blocks.2.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.2.attn1.to_v.scale', 'action_head.model.transformer_blocks.2.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.2.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.2.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.2.ff.net.2.scale', 'action_head.model.transformer_blocks.2.ff.net.2.zero_point', 'action_head.model.transformer_blocks.2.norm1.linear.scale', 'action_head.model.transformer_blocks.2.norm1.linear.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_k.scale', 'action_head.model.transformer_blocks.3.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.3.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_q.scale', 'action_head.model.transformer_blocks.3.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.3.attn1.to_v.scale', 'action_head.model.transformer_blocks.3.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.3.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.3.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.3.ff.net.2.scale', 'action_head.model.transformer_blocks.3.ff.net.2.zero_point', 'action_head.model.transformer_blocks.3.norm1.linear.scale', 'action_head.model.transformer_blocks.3.norm1.linear.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_k.scale', 'action_head.model.transformer_blocks.4.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.4.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_q.scale', 'action_head.model.transformer_blocks.4.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.4.attn1.to_v.scale', 'action_head.model.transformer_blocks.4.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.4.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.4.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.4.ff.net.2.scale', 'action_head.model.transformer_blocks.4.ff.net.2.zero_point', 'action_head.model.transformer_blocks.4.norm1.linear.scale', 'action_head.model.transformer_blocks.4.norm1.linear.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_k.scale', 'action_head.model.transformer_blocks.5.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.5.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_q.scale', 'action_head.model.transformer_blocks.5.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.5.attn1.to_v.scale', 'action_head.model.transformer_blocks.5.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.5.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.5.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.5.ff.net.2.scale', 'action_head.model.transformer_blocks.5.ff.net.2.zero_point', 'action_head.model.transformer_blocks.5.norm1.linear.scale', 'action_head.model.transformer_blocks.5.norm1.linear.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_k.scale', 'action_head.model.transformer_blocks.6.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.6.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_q.scale', 'action_head.model.transformer_blocks.6.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.6.attn1.to_v.scale', 'action_head.model.transformer_blocks.6.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.6.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.6.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.6.ff.net.2.scale', 'action_head.model.transformer_blocks.6.ff.net.2.zero_point', 'action_head.model.transformer_blocks.6.norm1.linear.scale', 'action_head.model.transformer_blocks.6.norm1.linear.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_k.scale', 'action_head.model.transformer_blocks.7.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.7.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_q.scale', 'action_head.model.transformer_blocks.7.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.7.attn1.to_v.scale', 'action_head.model.transformer_blocks.7.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.7.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.7.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.7.ff.net.2.scale', 'action_head.model.transformer_blocks.7.ff.net.2.zero_point', 'action_head.model.transformer_blocks.7.norm1.linear.scale', 'action_head.model.transformer_blocks.7.norm1.linear.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_k.scale', 'action_head.model.transformer_blocks.8.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.8.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_q.scale', 'action_head.model.transformer_blocks.8.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.8.attn1.to_v.scale', 'action_head.model.transformer_blocks.8.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.8.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.8.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.8.ff.net.2.scale', 'action_head.model.transformer_blocks.8.ff.net.2.zero_point', 'action_head.model.transformer_blocks.8.norm1.linear.scale', 'action_head.model.transformer_blocks.8.norm1.linear.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_k.scale', 'action_head.model.transformer_blocks.9.attn1.to_k.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_out.0.scale', 'action_head.model.transformer_blocks.9.attn1.to_out.0.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_q.scale', 'action_head.model.transformer_blocks.9.attn1.to_q.zero_point', 'action_head.model.transformer_blocks.9.attn1.to_v.scale', 'action_head.model.transformer_blocks.9.attn1.to_v.zero_point', 'action_head.model.transformer_blocks.9.ff.net.0.proj.scale', 'action_head.model.transformer_blocks.9.ff.net.0.proj.zero_point', 'action_head.model.transformer_blocks.9.ff.net.2.scale', 'action_head.model.transformer_blocks.9.ff.net.2.zero_point', 'action_head.model.transformer_blocks.9.norm1.linear.scale', 'action_head.model.transformer_blocks.9.norm1.linear.zero_point', 'backbone.linear.scale', 'backbone.linear.zero_point', 'backbone.model.language_model.model.layers.0.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.0.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.0.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.1.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.1.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.10.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.10.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.11.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.11.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.2.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.2.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.3.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.3.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.4.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.4.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.5.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.5.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.6.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.6.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.7.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.7.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.8.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.8.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.down_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.down_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.zero_point', 'backbone.model.language_model.model.layers.9.mlp.up_proj.scale', 'backbone.model.language_model.model.layers.9.mlp.up_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.zero_point', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.scale', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.zero_point', 'backbone.model.mlp1.1.scale', 'backbone.model.mlp1.1.zero_point', 'backbone.model.mlp1.3.scale', 'backbone.model.mlp1.3.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.zero_point', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.scale', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.zero_point']\n",
            "- This IS expected if you are initializing GR00T_N1 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GR00T_N1 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GR00T_N1 were not initialized from the model checkpoint at /content/int8_minimal and are newly initialized: ['action_head.model.proj_out_1.bias', 'action_head.model.proj_out_1.weight', 'action_head.model.proj_out_2.bias', 'action_head.model.proj_out_2.weight', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.bias', 'action_head.model.timestep_encoder.timestep_embedder.linear_1.weight', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.bias', 'action_head.model.timestep_encoder.timestep_embedder.linear_2.weight', 'action_head.model.transformer_blocks.0.attn1.to_k.bias', 'action_head.model.transformer_blocks.0.attn1.to_k.weight', 'action_head.model.transformer_blocks.0.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.0.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.0.attn1.to_q.bias', 'action_head.model.transformer_blocks.0.attn1.to_q.weight', 'action_head.model.transformer_blocks.0.attn1.to_v.bias', 'action_head.model.transformer_blocks.0.attn1.to_v.weight', 'action_head.model.transformer_blocks.0.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.0.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.0.ff.net.2.bias', 'action_head.model.transformer_blocks.0.ff.net.2.weight', 'action_head.model.transformer_blocks.0.norm1.linear.bias', 'action_head.model.transformer_blocks.0.norm1.linear.weight', 'action_head.model.transformer_blocks.1.attn1.to_k.bias', 'action_head.model.transformer_blocks.1.attn1.to_k.weight', 'action_head.model.transformer_blocks.1.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.1.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.1.attn1.to_q.bias', 'action_head.model.transformer_blocks.1.attn1.to_q.weight', 'action_head.model.transformer_blocks.1.attn1.to_v.bias', 'action_head.model.transformer_blocks.1.attn1.to_v.weight', 'action_head.model.transformer_blocks.1.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.1.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.1.ff.net.2.bias', 'action_head.model.transformer_blocks.1.ff.net.2.weight', 'action_head.model.transformer_blocks.1.norm1.linear.bias', 'action_head.model.transformer_blocks.1.norm1.linear.weight', 'action_head.model.transformer_blocks.10.attn1.to_k.bias', 'action_head.model.transformer_blocks.10.attn1.to_k.weight', 'action_head.model.transformer_blocks.10.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.10.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.10.attn1.to_q.bias', 'action_head.model.transformer_blocks.10.attn1.to_q.weight', 'action_head.model.transformer_blocks.10.attn1.to_v.bias', 'action_head.model.transformer_blocks.10.attn1.to_v.weight', 'action_head.model.transformer_blocks.10.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.10.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.10.ff.net.2.bias', 'action_head.model.transformer_blocks.10.ff.net.2.weight', 'action_head.model.transformer_blocks.10.norm1.linear.bias', 'action_head.model.transformer_blocks.10.norm1.linear.weight', 'action_head.model.transformer_blocks.11.attn1.to_k.bias', 'action_head.model.transformer_blocks.11.attn1.to_k.weight', 'action_head.model.transformer_blocks.11.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.11.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.11.attn1.to_q.bias', 'action_head.model.transformer_blocks.11.attn1.to_q.weight', 'action_head.model.transformer_blocks.11.attn1.to_v.bias', 'action_head.model.transformer_blocks.11.attn1.to_v.weight', 'action_head.model.transformer_blocks.11.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.11.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.11.ff.net.2.bias', 'action_head.model.transformer_blocks.11.ff.net.2.weight', 'action_head.model.transformer_blocks.11.norm1.linear.bias', 'action_head.model.transformer_blocks.11.norm1.linear.weight', 'action_head.model.transformer_blocks.12.attn1.to_k.bias', 'action_head.model.transformer_blocks.12.attn1.to_k.weight', 'action_head.model.transformer_blocks.12.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.12.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.12.attn1.to_q.bias', 'action_head.model.transformer_blocks.12.attn1.to_q.weight', 'action_head.model.transformer_blocks.12.attn1.to_v.bias', 'action_head.model.transformer_blocks.12.attn1.to_v.weight', 'action_head.model.transformer_blocks.12.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.12.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.12.ff.net.2.bias', 'action_head.model.transformer_blocks.12.ff.net.2.weight', 'action_head.model.transformer_blocks.12.norm1.linear.bias', 'action_head.model.transformer_blocks.12.norm1.linear.weight', 'action_head.model.transformer_blocks.13.attn1.to_k.bias', 'action_head.model.transformer_blocks.13.attn1.to_k.weight', 'action_head.model.transformer_blocks.13.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.13.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.13.attn1.to_q.bias', 'action_head.model.transformer_blocks.13.attn1.to_q.weight', 'action_head.model.transformer_blocks.13.attn1.to_v.bias', 'action_head.model.transformer_blocks.13.attn1.to_v.weight', 'action_head.model.transformer_blocks.13.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.13.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.13.ff.net.2.bias', 'action_head.model.transformer_blocks.13.ff.net.2.weight', 'action_head.model.transformer_blocks.13.norm1.linear.bias', 'action_head.model.transformer_blocks.13.norm1.linear.weight', 'action_head.model.transformer_blocks.14.attn1.to_k.bias', 'action_head.model.transformer_blocks.14.attn1.to_k.weight', 'action_head.model.transformer_blocks.14.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.14.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.14.attn1.to_q.bias', 'action_head.model.transformer_blocks.14.attn1.to_q.weight', 'action_head.model.transformer_blocks.14.attn1.to_v.bias', 'action_head.model.transformer_blocks.14.attn1.to_v.weight', 'action_head.model.transformer_blocks.14.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.14.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.14.ff.net.2.bias', 'action_head.model.transformer_blocks.14.ff.net.2.weight', 'action_head.model.transformer_blocks.14.norm1.linear.bias', 'action_head.model.transformer_blocks.14.norm1.linear.weight', 'action_head.model.transformer_blocks.15.attn1.to_k.bias', 'action_head.model.transformer_blocks.15.attn1.to_k.weight', 'action_head.model.transformer_blocks.15.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.15.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.15.attn1.to_q.bias', 'action_head.model.transformer_blocks.15.attn1.to_q.weight', 'action_head.model.transformer_blocks.15.attn1.to_v.bias', 'action_head.model.transformer_blocks.15.attn1.to_v.weight', 'action_head.model.transformer_blocks.15.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.15.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.15.ff.net.2.bias', 'action_head.model.transformer_blocks.15.ff.net.2.weight', 'action_head.model.transformer_blocks.15.norm1.linear.bias', 'action_head.model.transformer_blocks.15.norm1.linear.weight', 'action_head.model.transformer_blocks.2.attn1.to_k.bias', 'action_head.model.transformer_blocks.2.attn1.to_k.weight', 'action_head.model.transformer_blocks.2.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.2.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.2.attn1.to_q.bias', 'action_head.model.transformer_blocks.2.attn1.to_q.weight', 'action_head.model.transformer_blocks.2.attn1.to_v.bias', 'action_head.model.transformer_blocks.2.attn1.to_v.weight', 'action_head.model.transformer_blocks.2.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.2.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.2.ff.net.2.bias', 'action_head.model.transformer_blocks.2.ff.net.2.weight', 'action_head.model.transformer_blocks.2.norm1.linear.bias', 'action_head.model.transformer_blocks.2.norm1.linear.weight', 'action_head.model.transformer_blocks.3.attn1.to_k.bias', 'action_head.model.transformer_blocks.3.attn1.to_k.weight', 'action_head.model.transformer_blocks.3.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.3.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.3.attn1.to_q.bias', 'action_head.model.transformer_blocks.3.attn1.to_q.weight', 'action_head.model.transformer_blocks.3.attn1.to_v.bias', 'action_head.model.transformer_blocks.3.attn1.to_v.weight', 'action_head.model.transformer_blocks.3.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.3.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.3.ff.net.2.bias', 'action_head.model.transformer_blocks.3.ff.net.2.weight', 'action_head.model.transformer_blocks.3.norm1.linear.bias', 'action_head.model.transformer_blocks.3.norm1.linear.weight', 'action_head.model.transformer_blocks.4.attn1.to_k.bias', 'action_head.model.transformer_blocks.4.attn1.to_k.weight', 'action_head.model.transformer_blocks.4.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.4.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.4.attn1.to_q.bias', 'action_head.model.transformer_blocks.4.attn1.to_q.weight', 'action_head.model.transformer_blocks.4.attn1.to_v.bias', 'action_head.model.transformer_blocks.4.attn1.to_v.weight', 'action_head.model.transformer_blocks.4.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.4.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.4.ff.net.2.bias', 'action_head.model.transformer_blocks.4.ff.net.2.weight', 'action_head.model.transformer_blocks.4.norm1.linear.bias', 'action_head.model.transformer_blocks.4.norm1.linear.weight', 'action_head.model.transformer_blocks.5.attn1.to_k.bias', 'action_head.model.transformer_blocks.5.attn1.to_k.weight', 'action_head.model.transformer_blocks.5.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.5.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.5.attn1.to_q.bias', 'action_head.model.transformer_blocks.5.attn1.to_q.weight', 'action_head.model.transformer_blocks.5.attn1.to_v.bias', 'action_head.model.transformer_blocks.5.attn1.to_v.weight', 'action_head.model.transformer_blocks.5.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.5.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.5.ff.net.2.bias', 'action_head.model.transformer_blocks.5.ff.net.2.weight', 'action_head.model.transformer_blocks.5.norm1.linear.bias', 'action_head.model.transformer_blocks.5.norm1.linear.weight', 'action_head.model.transformer_blocks.6.attn1.to_k.bias', 'action_head.model.transformer_blocks.6.attn1.to_k.weight', 'action_head.model.transformer_blocks.6.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.6.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.6.attn1.to_q.bias', 'action_head.model.transformer_blocks.6.attn1.to_q.weight', 'action_head.model.transformer_blocks.6.attn1.to_v.bias', 'action_head.model.transformer_blocks.6.attn1.to_v.weight', 'action_head.model.transformer_blocks.6.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.6.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.6.ff.net.2.bias', 'action_head.model.transformer_blocks.6.ff.net.2.weight', 'action_head.model.transformer_blocks.6.norm1.linear.bias', 'action_head.model.transformer_blocks.6.norm1.linear.weight', 'action_head.model.transformer_blocks.7.attn1.to_k.bias', 'action_head.model.transformer_blocks.7.attn1.to_k.weight', 'action_head.model.transformer_blocks.7.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.7.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.7.attn1.to_q.bias', 'action_head.model.transformer_blocks.7.attn1.to_q.weight', 'action_head.model.transformer_blocks.7.attn1.to_v.bias', 'action_head.model.transformer_blocks.7.attn1.to_v.weight', 'action_head.model.transformer_blocks.7.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.7.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.7.ff.net.2.bias', 'action_head.model.transformer_blocks.7.ff.net.2.weight', 'action_head.model.transformer_blocks.7.norm1.linear.bias', 'action_head.model.transformer_blocks.7.norm1.linear.weight', 'action_head.model.transformer_blocks.8.attn1.to_k.bias', 'action_head.model.transformer_blocks.8.attn1.to_k.weight', 'action_head.model.transformer_blocks.8.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.8.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.8.attn1.to_q.bias', 'action_head.model.transformer_blocks.8.attn1.to_q.weight', 'action_head.model.transformer_blocks.8.attn1.to_v.bias', 'action_head.model.transformer_blocks.8.attn1.to_v.weight', 'action_head.model.transformer_blocks.8.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.8.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.8.ff.net.2.bias', 'action_head.model.transformer_blocks.8.ff.net.2.weight', 'action_head.model.transformer_blocks.8.norm1.linear.bias', 'action_head.model.transformer_blocks.8.norm1.linear.weight', 'action_head.model.transformer_blocks.9.attn1.to_k.bias', 'action_head.model.transformer_blocks.9.attn1.to_k.weight', 'action_head.model.transformer_blocks.9.attn1.to_out.0.bias', 'action_head.model.transformer_blocks.9.attn1.to_out.0.weight', 'action_head.model.transformer_blocks.9.attn1.to_q.bias', 'action_head.model.transformer_blocks.9.attn1.to_q.weight', 'action_head.model.transformer_blocks.9.attn1.to_v.bias', 'action_head.model.transformer_blocks.9.attn1.to_v.weight', 'action_head.model.transformer_blocks.9.ff.net.0.proj.bias', 'action_head.model.transformer_blocks.9.ff.net.0.proj.weight', 'action_head.model.transformer_blocks.9.ff.net.2.bias', 'action_head.model.transformer_blocks.9.ff.net.2.weight', 'action_head.model.transformer_blocks.9.norm1.linear.bias', 'action_head.model.transformer_blocks.9.norm1.linear.weight', 'backbone.linear.bias', 'backbone.linear.weight', 'backbone.model.language_model.model.layers.0.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.weight', 'backbone.model.mlp1.1.bias', 'backbone.model.mlp1.1.weight', 'backbone.model.mlp1.3.bias', 'backbone.model.mlp1.3.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/eval_policy.py\", line 86, in <module>\n",
            "    mse = calc_mse_for_single_trajectory(\n",
            "  File \"/content/Isaac-GR00T/gr00t/utils/eval.py\", line 51, in calc_mse_for_single_trajectory\n",
            "    data_point = dataset.get_step_data(traj_id, step_count)\n",
            "  File \"/content/Isaac-GR00T/gr00t/data/dataset.py\", line 522, in get_step_data\n",
            "    data[key] = self.get_data_by_modality(trajectory_id, modality, key, base_index)\n",
            "  File \"/content/Isaac-GR00T/gr00t/data/dataset.py\", line 782, in get_data_by_modality\n",
            "    return self.get_video(trajectory_id, key, base_index)\n",
            "  File \"/content/Isaac-GR00T/gr00t/data/dataset.py\", line 652, in get_video\n",
            "    return get_frames_by_timestamps(\n",
            "  File \"/content/Isaac-GR00T/gr00t/utils/video.py\", line 64, in get_frames_by_timestamps\n",
            "    vr = decord.VideoReader(video_path, **video_backend_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/decord/video_reader.py\", line 54, in __init__\n",
            "    self._handle = _CAPI_VideoReaderGetVideoReader(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/decord/_ffi/_ctypes/function.py\", line 173, in __call__\n",
            "    check_call(_LIB.DECORDFuncCall(\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1DH-h7PN6H3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method4 Structured Pruning"
      ],
      "metadata": {
        "id": "eOlmsyG46IER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import json\n",
        "from safetensors.torch import save_file\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# Config\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "STRUCT_PRUNED_PATH = \"/content/gr00t_structured_30\"\n",
        "SPARSITY = 0.3\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "os.makedirs(STRUCT_PRUNED_PATH, exist_ok=True)\n",
        "\n",
        "# Load model\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "model = policy.model.eval()\n",
        "\n",
        "# Apply structured pruning\n",
        "total_params = 0\n",
        "total_pruned = 0\n",
        "\n",
        "print(f\"Applying {int(SPARSITY*100)}% structured (L2) pruning on nn.Linear layers:\")\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        num_weights = module.weight.nelement()\n",
        "        total_params += num_weights\n",
        "        prune.ln_structured(module, name=\"weight\", amount=SPARSITY, n=2, dim=0)\n",
        "        pruned = int(SPARSITY * num_weights)\n",
        "        total_pruned += pruned\n",
        "        print(f\"{name}: pruned {pruned} of {num_weights} weights\")\n",
        "        prune.remove(module, \"weight\")\n",
        "\n",
        "sparsity_pct = 100 * total_pruned / total_params\n",
        "print(f\"Total pruned: {total_pruned} of {total_params} weights ({sparsity_pct:.2f}%)\")\n",
        "\n",
        "# Save model\n",
        "save_file(model.state_dict(), f\"{STRUCT_PRUNED_PATH}/model.safetensors\")\n",
        "\n",
        "# Copy config and index\n",
        "for file in [\"config.json\", \"model.safetensors.index.json\"]:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(STRUCT_PRUNED_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        os.system(f\"cp {src} {dst}\")\n",
        "\n",
        "print(f\"Structured pruned model saved to {STRUCT_PRUNED_PATH}\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X80CXeTc6lai",
        "outputId": "34b48bb9-e63b-4e81-8c33-cde8ea3f2b5e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Applying 30% structured (L2) pruning on nn.Linear layers:\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.mlp1.1: pruned 2831155 of 9437184 weights\n",
            "backbone.model.mlp1.3: pruned 1258291 of 4194304 weights\n",
            "backbone.linear: pruned 943718 of 3145728 weights\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_1: pruned 117964 of 393216 weights\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_2: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.0.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.1.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.1.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.2.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.2.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.3.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.3.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.4.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.4.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.5.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.5.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.6.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.6.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.7.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.7.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.8.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.8.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.9.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.9.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.10.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.10.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.11.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.11.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.12.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.12.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.13.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.13.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.14.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.14.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.15.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.15.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.proj_out_1: pruned 1415577 of 4718592 weights\n",
            "action_head.model.proj_out_2: pruned 471859 of 1572864 weights\n",
            "Total pruned: 531203944 of 1770680320 weights (30.00%)\n",
            "Structured pruned model saved to /content/gr00t_structured_30\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:25:49.540644: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:25:49.590913: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:25:49.590969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:25:49.592252: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:25:49.599618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:25:50.710422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Final run this"
      ],
      "metadata": {
        "id": "_kK6Zje43N1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from safetensors.torch import save_file\n",
        "from transformers import AutoConfig\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# === Config ===\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "STRUCT_PRUNED_PATH = \"/content/gr00t_structured_30\"\n",
        "SPARSITY = 0.3\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "os.makedirs(STRUCT_PRUNED_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(STRUCT_PRUNED_PATH, \"experiment_cfg\"), exist_ok=True)\n",
        "\n",
        "# === Load model ===\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "model = policy.model.eval()\n",
        "\n",
        "# === Apply structured pruning ===\n",
        "total_params = 0\n",
        "total_pruned = 0\n",
        "\n",
        "print(f\"Applying {int(SPARSITY*100)}% structured (L2) pruning on nn.Linear layers:\")\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        num_weights = module.weight.nelement()\n",
        "        total_params += num_weights\n",
        "        prune.ln_structured(module, name=\"weight\", amount=SPARSITY, n=2, dim=0)\n",
        "        pruned = int(SPARSITY * num_weights)\n",
        "        total_pruned += pruned\n",
        "        print(f\"{name}: pruned {pruned} of {num_weights} weights\")\n",
        "        prune.remove(module, \"weight\")\n",
        "\n",
        "sparsity_pct = 100 * total_pruned / total_params\n",
        "print(f\"✓ Total pruned: {total_pruned} of {total_params} weights ({sparsity_pct:.2f}%)\")\n",
        "\n",
        "# === Save pruned model ===\n",
        "save_file(model.state_dict(), f\"{STRUCT_PRUNED_PATH}/model.safetensors\", metadata={\"format\": \"pt\", \"pruned\": \"structured\", \"sparsity\": str(SPARSITY)})\n",
        "print(\"✓ Saved model.safetensors\")\n",
        "\n",
        "# === Save AutoConfig ===\n",
        "try:\n",
        "    config = AutoConfig.from_pretrained(ORIG_MODEL_PATH)\n",
        "    config.pruned = True\n",
        "    config.pruning_type = \"structured_l2\"\n",
        "    config.pruning_sparsity = SPARSITY\n",
        "    config.save_pretrained(STRUCT_PRUNED_PATH)\n",
        "    print(\"✓ Saved config.json\")\n",
        "except Exception as e:\n",
        "    print(\"✗ Failed to save config:\", e)\n",
        "\n",
        "# === Copy tokenizer and preprocessor files ===\n",
        "required_files = [\n",
        "    \"special_tokens_map.json\", \"tokenizer_config.json\",\n",
        "    \"vocab.json\", \"merges.txt\", \"tokenizer.json\",\n",
        "    \"preprocessor_config.json\"\n",
        "]\n",
        "for file in required_files:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(STRUCT_PRUNED_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"✓ Copied {file}\")\n",
        "\n",
        "# === Copy experiment_cfg or fallback metadata ===\n",
        "exp_cfg_src = os.path.join(ORIG_MODEL_PATH, \"experiment_cfg\")\n",
        "exp_cfg_dst = os.path.join(STRUCT_PRUNED_PATH, \"experiment_cfg\")\n",
        "\n",
        "if os.path.exists(exp_cfg_src):\n",
        "    for file in os.listdir(exp_cfg_src):\n",
        "        shutil.copy2(os.path.join(exp_cfg_src, file), os.path.join(exp_cfg_dst, file))\n",
        "    print(\"✓ Copied experiment_cfg\")\n",
        "else:\n",
        "    fallback = [\n",
        "        os.path.join(ORIG_MODEL_PATH, \"metadata.json\"),\n",
        "        \"/content/Isaac-GR00T/gr00t/experiment_cfg/metadata.json\"\n",
        "    ]\n",
        "    for loc in fallback:\n",
        "        if os.path.exists(loc):\n",
        "            shutil.copy2(loc, os.path.join(exp_cfg_dst, \"metadata.json\"))\n",
        "            print(\"✓ Copied fallback metadata.json\")\n",
        "            break\n",
        "\n",
        "print(\"\\\\n=== DONE ===\")\n",
        "print(\"Structured pruned model saved to:\", STRUCT_PRUNED_PATH)\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jZggQ8LTm14",
        "outputId": "302d2b2c-39d9-400e-9b65-c5bba7cb26a0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Applying 30% structured (L2) pruning on nn.Linear layers:\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.0.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.1.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.2.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.3.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.4.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.5.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.6.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.7.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.8.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.9.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.10.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.11.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.12.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.13.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.14.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.15.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.16.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.17.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.18.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.19.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.20.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.21.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.22.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.23.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.24.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.25.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.k_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.v_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.q_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.self_attn.out_proj: pruned 398131 of 1327104 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc1: pruned 1487462 of 4958208 weights\n",
            "backbone.model.vision_model.vision_model.encoder.layers.26.mlp.fc2: pruned 1487462 of 4958208 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.0.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.1.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.2.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.3.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.4.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.5.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.6.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.7.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.8.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.9.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.10.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.q_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.k_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.v_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.self_attn.o_proj: pruned 1258291 of 4194304 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.gate_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.up_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.language_model.model.layers.11.mlp.down_proj: pruned 5033164 of 16777216 weights\n",
            "backbone.model.mlp1.1: pruned 2831155 of 9437184 weights\n",
            "backbone.model.mlp1.3: pruned 1258291 of 4194304 weights\n",
            "backbone.linear: pruned 943718 of 3145728 weights\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_1: pruned 117964 of 393216 weights\n",
            "action_head.model.timestep_encoder.timestep_embedder.linear_2: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.0.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.0.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.1.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.1.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.1.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.2.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.2.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.2.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.3.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.3.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.3.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.4.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.4.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.4.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.5.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.5.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.5.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.6.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.6.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.6.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.7.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.7.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.7.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.8.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.8.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.8.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.9.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.9.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.9.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.10.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.10.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.10.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.11.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.11.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.11.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.12.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.12.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.12.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.13.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.13.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.13.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.14.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.14.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.14.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.15.norm1.linear: pruned 1415577 of 4718592 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_q: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_k: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_v: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.attn1.to_out.0: pruned 707788 of 2359296 weights\n",
            "action_head.model.transformer_blocks.15.ff.net.0.proj: pruned 2831155 of 9437184 weights\n",
            "action_head.model.transformer_blocks.15.ff.net.2: pruned 2831155 of 9437184 weights\n",
            "action_head.model.proj_out_1: pruned 1415577 of 4718592 weights\n",
            "action_head.model.proj_out_2: pruned 471859 of 1572864 weights\n",
            "✓ Total pruned: 531203944 of 1770680320 weights (30.00%)\n",
            "✓ Saved model.safetensors\n",
            "✓ Saved config.json\n",
            "✓ Copied experiment_cfg\n",
            "\n",
            "=== DONE ===\n",
            "Structured pruned model saved to: /content/gr00t_structured_30\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:26:41.671527: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:26:41.720186: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:26:41.720233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:26:41.721447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:26:41.728402: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:26:42.674359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/gr00t_structured_30 \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2h7s0zRURY9",
        "outputId": "1c99380b-8ea8-44ad-a52d-1a676329526b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:27:39.533541: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:27:39.582776: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:27:39.582834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:27:39.584061: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:27:39.591098: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:27:40.720151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_structured_30\n",
            "Loading pretrained dual brain from /content/gr00t_structured_30\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_structured_30\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 99029.57835382073\n",
            "MSE: 99029.57835382073\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 90806.91916046475\n",
            "MSE: 90806.91916046475\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 115798.72951530124\n",
            "MSE: 115798.72951530124\n",
            "Running trajectory: 3\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 101242.78156282968\n",
            "MSE: 101242.78156282968\n",
            "Running trajectory: 4\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 89543.04245745401\n",
            "MSE: 89543.04245745401\n",
            "Running trajectory: 5\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 143847.7759252525\n",
            "MSE: 143847.7759252525\n",
            "Running trajectory: 6\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 82275.62140231948\n",
            "MSE: 82275.62140231948\n",
            "Running trajectory: 7\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 96326.13323217652\n",
            "MSE: 96326.13323217652\n",
            "Running trajectory: 8\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 118881.91098609427\n",
            "MSE: 118881.91098609427\n",
            "Running trajectory: 9\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 111261.98344688963\n",
            "MSE: 111261.98344688963\n",
            "Running trajectory: 10\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 115601.972828248\n",
            "MSE: 115601.972828248\n",
            "Running trajectory: 11\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 111977.5697309726\n",
            "MSE: 111977.5697309726\n",
            "Running trajectory: 12\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 101104.49635960686\n",
            "MSE: 101104.49635960686\n",
            "Running trajectory: 13\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 92100.78889278354\n",
            "MSE: 92100.78889278354\n",
            "Running trajectory: 14\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 81065.23739458346\n",
            "MSE: 81065.23739458346\n",
            "Running trajectory: 15\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 87481.57336776477\n",
            "MSE: 87481.57336776477\n",
            "Running trajectory: 16\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 111062.76557025006\n",
            "MSE: 111062.76557025006\n",
            "Running trajectory: 17\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 108479.68585184205\n",
            "MSE: 108479.68585184205\n",
            "Running trajectory: 18\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 93166.18861696917\n",
            "MSE: 93166.18861696917\n",
            "Running trajectory: 19\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 97136.01960564672\n",
            "MSE: 97136.01960564672\n",
            "Running trajectory: 20\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 91229.00137195068\n",
            "/content/Isaac-GR00T/gr00t/utils/eval.py:95: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(nrows=num_of_joints, ncols=1, figsize=(8, 4 * num_of_joints))\n",
            "MSE: 91229.00137195068\n",
            "Running trajectory: 21\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 107815.29355888868\n",
            "MSE: 107815.29355888868\n",
            "Running trajectory: 22\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 119045.96201457869\n",
            "MSE: 119045.96201457869\n",
            "Running trajectory: 23\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 116814.38756670417\n",
            "MSE: 116814.38756670417\n",
            "Running trajectory: 24\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 92894.2105596621\n",
            "MSE: 92894.2105596621\n",
            "Running trajectory: 25\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 96212.21296674303\n",
            "MSE: 96212.21296674303\n",
            "Running trajectory: 26\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 81514.43416719513\n",
            "MSE: 81514.43416719513\n",
            "Running trajectory: 27\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 95500.44950069487\n",
            "MSE: 95500.44950069487\n",
            "Running trajectory: 28\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 105076.15706055284\n",
            "MSE: 105076.15706055284\n",
            "Running trajectory: 29\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 109593.77826176715\n",
            "MSE: 109593.77826176715\n",
            "Running trajectory: 30\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 87762.59509267325\n",
            "MSE: 87762.59509267325\n",
            " Total Inference Time: 147.24 seconds\n",
            "Average MSE across all trajs: 101666.10504460259\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How much the weights change"
      ],
      "metadata": {
        "id": "uRQ4lVa1mGTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "import copy\n",
        "from safetensors.torch import load_file\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# Config\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "STRUCT_PRUNED_WEIGHTS = \"/content/gr00t_structured_30/model.safetensors\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "\n",
        "# Load config and model\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "\n",
        "orig_model = policy.model.eval().cpu()\n",
        "orig_state = orig_model.state_dict()\n",
        "\n",
        "# Clone and load pruned weights\n",
        "pruned_model = copy.deepcopy(orig_model)\n",
        "pruned_state = load_file(STRUCT_PRUNED_WEIGHTS)\n",
        "pruned_model.load_state_dict(pruned_state)\n",
        "pruned_model = pruned_model.eval()\n",
        "\n",
        "# Compare all float-like tensors\n",
        "total_diff = 0.0\n",
        "total_count = 0\n",
        "changed = 0\n",
        "\n",
        "print(\"Comparing parameter differences:\")\n",
        "\n",
        "for key in orig_state:\n",
        "    if key in pruned_state and torch.is_floating_point(orig_state[key]):\n",
        "        o = orig_state[key].flatten().to(torch.float32)\n",
        "        p = pruned_state[key].flatten().to(torch.float32)\n",
        "        if o.shape != p.shape:\n",
        "            print(f\"Shape mismatch for {key}, skipping.\")\n",
        "            continue\n",
        "        diff = (o - p).abs()\n",
        "        total_diff += diff.sum().item()\n",
        "        total_count += diff.numel()\n",
        "        changed += (diff > 1e-6).sum().item()\n",
        "\n",
        "if total_count == 0:\n",
        "    print(\" No comparable floating-point parameters found.\")\n",
        "else:\n",
        "    print(f\"Mean absolute parameter difference: {total_diff / total_count:.6f}\")\n",
        "    print(f\"Changed weights: {changed} of {total_count} ({(changed / total_count) * 100:.2f}%)\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izSJECSNmgY6",
        "outputId": "46a222b5-3473-495c-fa3e-625974290695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Comparing parameter differences:\n",
            "Mean absolute parameter difference: 0.014374\n",
            "Changed weights: 531249465 of 2190016752 (24.26%)\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 10:06:37.322332: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 10:06:37.373225: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 10:06:37.373283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 10:06:37.374577: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 10:06:37.382350: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 10:06:38.345749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.29it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method5 LORA Replace layers with low-rank adapters\n"
      ],
      "metadata": {
        "id": "4OWwM6k067Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2N4XeSZN7Bnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os, shutil\n",
        "from safetensors.torch import save_file\n",
        "from transformers import AutoConfig\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "# === LoRA Layer ===\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, r=4, alpha=1.0, dropout=0.0, bias=True):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        if r > 0:\n",
        "            self.lora_A = nn.Linear(in_features, r, bias=False)\n",
        "            self.lora_B = nn.Linear(r, out_features, bias=False)\n",
        "            self.scaling = alpha / r\n",
        "        else:\n",
        "            self.lora_A = None\n",
        "            self.lora_B = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        if self.r > 0:\n",
        "            lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
        "            out += lora_out\n",
        "        return out\n",
        "\n",
        "# === Safe Injection ===\n",
        "def inject_lora(model, target_submodule=\"backbone.model.language_model\", r=4, alpha=1.0):\n",
        "    submodule = model\n",
        "    for attr in target_submodule.split('.'):\n",
        "        submodule = getattr(submodule, attr)\n",
        "\n",
        "    def replace_linear_with_lora(module):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, LoRALinear):\n",
        "                continue\n",
        "            if isinstance(child, nn.Linear):\n",
        "                lora_layer = LoRALinear(child.in_features, child.out_features, r=r, alpha=alpha, bias=child.bias is not None)\n",
        "                lora_layer.linear.weight.data.copy_(child.weight.data)\n",
        "                if child.bias is not None:\n",
        "                    lora_layer.linear.bias.data.copy_(child.bias.data)\n",
        "                setattr(module, name, lora_layer)\n",
        "            else:\n",
        "                replace_linear_with_lora(child)\n",
        "\n",
        "    replace_linear_with_lora(submodule)\n",
        "    print(f\"✓ Injected LoRA (r={r}, alpha={alpha}) into {target_submodule}\")\n",
        "\n",
        "# === Configs ===\n",
        "ORIG_MODEL_PATH = \"/content/gr00t_finetuned_kuka\"\n",
        "LORA_SAVE_PATH = \"/content/gr00t_lora_injected\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "os.makedirs(LORA_SAVE_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(LORA_SAVE_PATH, \"experiment_cfg\"), exist_ok=True)\n",
        "\n",
        "# === Load GR00T ===\n",
        "data_config = DATA_CONFIG_MAP[DATA_KEY]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=ORIG_MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "model = policy.model\n",
        "\n",
        "# === Inject & Freeze ===\n",
        "inject_lora(model, r=8, alpha=16)\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = \"lora_\" in name\n",
        "\n",
        "# === Save model.safetensors ===\n",
        "state_dict = {k: v for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
        "save_file(\n",
        "    state_dict,\n",
        "    os.path.join(LORA_SAVE_PATH, \"model.safetensors\"),\n",
        "    metadata={\"format\": \"pt\", \"lora\": \"true\"}\n",
        ")\n",
        "print(\"✓ Saved LoRA-injected model\")\n",
        "\n",
        "# === Save config ===\n",
        "try:\n",
        "    config = AutoConfig.from_pretrained(ORIG_MODEL_PATH)\n",
        "    config.lora = True\n",
        "    config.lora_rank = 8\n",
        "    config.lora_alpha = 16\n",
        "    config.save_pretrained(LORA_SAVE_PATH)\n",
        "    print(\"✓ Saved config.json\")\n",
        "except Exception as e:\n",
        "    print(\"✗ Config save failed:\", e)\n",
        "\n",
        "# === Copy tokenizer and other required files ===\n",
        "extra_files = [\n",
        "    \"special_tokens_map.json\", \"tokenizer_config.json\", \"vocab.json\",\n",
        "    \"merges.txt\", \"tokenizer.json\", \"preprocessor_config.json\"\n",
        "]\n",
        "for file in extra_files:\n",
        "    src = os.path.join(ORIG_MODEL_PATH, file)\n",
        "    dst = os.path.join(LORA_SAVE_PATH, file)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"✓ Copied {file}\")\n",
        "\n",
        "# === Copy experiment_cfg ===\n",
        "exp_src = os.path.join(ORIG_MODEL_PATH, \"experiment_cfg\")\n",
        "exp_dst = os.path.join(LORA_SAVE_PATH, \"experiment_cfg\")\n",
        "if os.path.exists(exp_src):\n",
        "    for file in os.listdir(exp_src):\n",
        "        shutil.copy2(os.path.join(exp_src, file), os.path.join(exp_dst, file))\n",
        "    print(\"✓ Copied experiment_cfg\")\n",
        "\n",
        "# === Print summary ===\n",
        "print(\"\\\\n✓ LoRA model saved at:\", LORA_SAVE_PATH)\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXP2nq0w7B7n",
        "outputId": "a172819c-b1dd-48ff-8eed-88d83e7635e0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Loading pretrained dual brain from /content/gr00t_finetuned_kuka\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_finetuned_kuka\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "✓ Injected LoRA (r=8, alpha=16) into backbone.model.language_model\n",
            "✓ Saved LoRA-injected model\n",
            "✓ Saved config.json\n",
            "✓ Copied experiment_cfg\n",
            "\n",
            "✓ LoRA model saved at: /content/gr00t_lora_injected\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:31:41.794219: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:31:41.844402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:31:41.844458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:31:41.845734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:31:41.853228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:31:42.781143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How much weight Changes"
      ],
      "metadata": {
        "id": "qfh8T6x6oDcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/gr00t_lora_injected \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqOFwLcEYcf_",
        "outputId": "1b27f478-3785-4d48-c451-b25af46f3977"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 19:33:38.756953: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-18 19:33:38.806628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 19:33:38.806683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 19:33:38.807952: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 19:33:38.815258: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-18 19:33:39.955349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_lora_injected\n",
            "Loading pretrained dual brain from /content/gr00t_lora_injected\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_lora_injected\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Some weights of the model checkpoint at /content/gr00t_lora_injected were not used when initializing GR00T_N1: ['backbone.model.language_model.model.layers.0.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.0.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.0.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.0.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.0.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.0.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.1.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.1.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.1.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.1.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.1.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.1.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.10.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.10.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.10.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.10.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.10.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.10.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.11.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.11.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.11.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.11.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.11.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.11.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.2.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.2.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.2.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.2.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.2.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.2.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.3.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.3.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.3.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.3.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.3.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.3.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.4.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.4.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.4.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.4.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.4.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.4.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.5.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.5.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.5.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.5.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.5.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.5.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.6.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.6.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.6.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.6.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.6.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.6.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.7.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.7.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.7.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.7.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.7.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.7.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.8.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.8.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.8.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.8.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.8.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.8.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.lora_B.weight', 'backbone.model.language_model.model.layers.9.mlp.down_proj.linear.weight', 'backbone.model.language_model.model.layers.9.mlp.down_proj.lora_A.weight', 'backbone.model.language_model.model.layers.9.mlp.down_proj.lora_B.weight', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.linear.weight', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.lora_A.weight', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.lora_B.weight', 'backbone.model.language_model.model.layers.9.mlp.up_proj.linear.weight', 'backbone.model.language_model.model.layers.9.mlp.up_proj.lora_A.weight', 'backbone.model.language_model.model.layers.9.mlp.up_proj.lora_B.weight', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.linear.weight', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.lora_A.weight', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.lora_B.weight', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.linear.weight', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.lora_A.weight', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.lora_B.weight', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.linear.weight', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.lora_A.weight', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.lora_B.weight', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.linear.weight', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.lora_A.weight', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.lora_B.weight']\n",
            "- This IS expected if you are initializing GR00T_N1 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GR00T_N1 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GR00T_N1 were not initialized from the model checkpoint at /content/gr00t_lora_injected and are newly initialized: ['backbone.model.language_model.model.layers.0.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.0.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.0.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.1.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.1.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.10.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.10.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.11.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.11.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.2.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.2.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.3.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.3.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.4.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.4.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.5.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.5.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.6.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.6.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.7.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.7.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.8.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.8.self_attn.v_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.down_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.gate_proj.weight', 'backbone.model.language_model.model.layers.9.mlp.up_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.k_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.o_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.q_proj.weight', 'backbone.model.language_model.model.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 3\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 4\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 5\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 6\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 7\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 8\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 9\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 10\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 11\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 12\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 13\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 14\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 15\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 16\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 17\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 18\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 19\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 20\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "/content/Isaac-GR00T/gr00t/utils/eval.py:95: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(nrows=num_of_joints, ncols=1, figsize=(8, 4 * num_of_joints))\n",
            "MSE: nan\n",
            "Running trajectory: 21\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 22\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 23\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 24\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 25\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 26\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 27\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 28\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 29\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            "Running trajectory: 30\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: nan\n",
            "MSE: nan\n",
            " Total Inference Time: 148.03 seconds\n",
            "Average MSE across all trajs: nan\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source and destination directories\n",
        "src_dir = \"/content/gr00t_lora_injected/\"\n",
        "dst_dir = \"/content/drive/MyDrive/gr00t_models/gr00t_lora_injected/\"\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files in the source directory\n",
        "for filename in os.listdir(src_dir):\n",
        "    src_path = os.path.join(src_dir, filename)\n",
        "    dst_path = os.path.join(dst_dir, filename)\n",
        "\n",
        "    # Move each file\n",
        "    if os.path.isfile(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "        print(f\" Moved {filename} to: {dst_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYoID1_v5oQx",
        "outputId": "a1f6aeae-226d-4574-e7b1-510e94a0df6b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Moved model.safetensors to: /content/drive/MyDrive/gr00t_models/gr00t_lora_injected/model.safetensors\n",
            " Moved config.json to: /content/drive/MyDrive/gr00t_models/gr00t_lora_injected/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HwO9NfQpD7w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source file in Colab\n",
        "src = \"/content/gr00t_lora_injected/model.safetensors\"\n",
        "\n",
        "# Destination in Google Drive\n",
        "dst = \"/content/drive/MyDrive/gr00t_models/gr00t_lora_injected/model.safetensors\"\n",
        "\n",
        "# Ensure the destination folder exists\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "\n",
        "# Move the file\n",
        "shutil.move(src, dst)\n",
        "\n",
        "print(f\" Moved model.safetensors to: {dst}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzMJfgtJcEEI",
        "outputId": "77070f10-8e01-4b3f-a1fc-b2e1449507f2"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Moved model.safetensors to: /content/drive/MyDrive/gr00t_models/gr00t_lora_injected/model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source file in Colab\n",
        "src = \"/content/gr00t_int8_language_only/model.safetensors\"\n",
        "\n",
        "\n",
        "\n",
        "# Destination in Google Drive\n",
        "dst = \"/content/drive/MyDrive/gr00t_models/gr00t_int8_language_only/model.safetensors\"\n",
        "\n",
        "# Ensure the destination folder exists\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "\n",
        "# Move the file\n",
        "shutil.move(src, dst)\n",
        "\n",
        "print(f\" Moved model.safetensors to: {dst}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NvFWHe-fDPW",
        "outputId": "f4096d69-c299-45e0-b3cc-5911dd8831a0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Moved model.safetensors to: /content/drive/MyDrive/gr00t_models/gr00t_int8_language_only/model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "27CcrN5Q8BE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6 Offline Distillation"
      ],
      "metadata": {
        "id": "KkX7g_r28BSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "olfrom pathlib import Path\n",
        "\n",
        "\n",
        "target_file_path = Path(\"/content/eval_policy.py\")\n",
        "\n",
        "if not target_file_path.exists():\n",
        "    raise FileNotFoundError(f\"File not found: {target_file_path}\")\n",
        "\n",
        "code = target_file_path.read_text()\n",
        "\n",
        "start_token = \"for traj_id in range(args.trajs):\"\n",
        "end_token = \"print(\\\"Average MSE across all trajs:\\\"\"\n",
        "\n",
        "if start_token not in code or end_token not in code:\n",
        "    raise ValueError(\"Could not locate loop boundaries in file.\")\n",
        "\n",
        "# New loop that saves predicted actions\n",
        "patch_block = \"\"\"\n",
        "save_dir = '/content/seepage'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "all_mse = []\n",
        "for traj_id in range(args.trajs):\n",
        "    print(\"Running trajectory:\", traj_id)\n",
        "\n",
        "    # === Save predicted actions ===\n",
        "    pred_actions = []\n",
        "    for step in range(args.steps):\n",
        "        step_tensor = {\n",
        "    k: torch.from_numpy(v).cuda().half() if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.floating)\n",
        "    else torch.from_numpy(v).cuda()\n",
        "    for k, v in step_data.items() if isinstance(v, np.ndarray)\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = policy.model(step_tensor)\n",
        "            if isinstance(out, dict) and \"actions\" in out:\n",
        "                pred_actions.append(out[\"actions\"].cpu().numpy())\n",
        "\n",
        "    if pred_actions:\n",
        "        actions = np.stack(pred_actions, axis=0)\n",
        "        np.savez(os.path.join(save_dir, f\"actions_traj_{traj_id}.npz\"), actions=actions)\n",
        "        print(f\"Saved: actions_traj_{traj_id}.npz\")\n",
        "\n",
        "    # === MSE and Plot ===\n",
        "    mse = calc_mse_for_single_trajectory(\n",
        "        policy,\n",
        "        dataset,\n",
        "        traj_id,\n",
        "        modality_keys=args.modality_keys,\n",
        "        steps=args.steps,\n",
        "        action_horizon=args.action_horizon,\n",
        "        plot=args.plot,\n",
        "    )\n",
        "    print(\"MSE:\", mse)\n",
        "    all_mse.append(mse)\n",
        "\n",
        "    if args.plot:\n",
        "        plt.savefig(os.path.join(save_dir, f\"trajectory_{traj_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "print(\"Average MSE across all trajs:\", np.mean(all_mse))\n",
        "print(\"Teacher predicted actions saved to:\", save_dir)\n",
        "\"\"\"\n",
        "\n",
        "# Replace old loop with new one\n",
        "pre = code.split(start_token)[0]\n",
        "post = code.split(end_token)[-1]\n",
        "patched_code = pre + patch_block + \"\\n\" + post\n",
        "\n",
        "# Save back\n",
        "target_file_path.write_text(patched_code)\n",
        "\n",
        "print(f\"Patched {target_file_path.name} with action-saving logic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "9wpUCO_t-I0S",
        "outputId": "a5a84b90-2a81-4288-f73e-b4d300a7575b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-931946082272>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-931946082272>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    olfrom pathlib import Path\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py \\\n",
        "  --model_path /content/gr00t_finetuned_kuka \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --plot\n"
      ],
      "metadata": {
        "id": "Gv2ADpb9--yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from safetensors.torch import save_file\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "from gr00t.data.dataset import LeRobotSingleDataset\n",
        "from gr00t.model.gr00t_n1 import GR00T_N1, GR00T_N1Config\n",
        "\n",
        "DATASET_PATH = \"/content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset\"\n",
        "ACTIONS_PATH = \"/content/seepage\"\n",
        "OUTPUT_PATH = \"/content/gr00t_student_offline\"\n",
        "DATA_KEY = \"kuka_custom\"\n",
        "EMBODIMENT_TAG = \"new_embodiment\"\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 3\n",
        "LR = 1e-4\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "class OfflineDistillDataset(Dataset):\n",
        "    def __init__(self, dataset_path, actions_path, data_key, embodiment_tag):\n",
        "        self.data_config = DATA_CONFIG_MAP[data_key]\n",
        "        self.dataset = LeRobotSingleDataset(\n",
        "            dataset_path=dataset_path,\n",
        "            modality_configs=self.data_config.modality_config(),\n",
        "            transforms=self.data_config.transform(),\n",
        "            embodiment_tag=embodiment_tag\n",
        "        )\n",
        "        self.actions_path = actions_path\n",
        "        self.traj_ids = sorted([\n",
        "            int(f.split(\"_\")[-1].split(\".\")[0])\n",
        "            for f in os.listdir(actions_path) if f.startswith(\"actions_traj_\")\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.traj_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        traj_id = self.traj_ids[idx]\n",
        "        actions = np.load(os.path.join(self.actions_path, f\"actions_traj_{traj_id}.npz\"))[\"actions\"]\n",
        "        step_data = self.dataset.get_trajectory(traj_id)\n",
        "        return step_data, actions\n",
        "\n",
        "dataset = OfflineDistillDataset(DATASET_PATH, ACTIONS_PATH, DATA_KEY, EMBODIMENT_TAG)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "teacher_cfg = dataset.dataset.policy_config\n",
        "student_cfg = GR00T_N1Config(\n",
        "    action_dim=teacher_cfg.action_dim,\n",
        "    action_head_cfg=teacher_cfg.action_head_cfg,\n",
        "    action_horizon=teacher_cfg.action_horizon,\n",
        "    backbone_cfg=teacher_cfg.backbone_cfg,\n",
        "    hidden_size=teacher_cfg.hidden_size // 2,\n",
        ")\n",
        "student = GR00T_N1(student_cfg).cuda().train().half()\n",
        "\n",
        "opt = torch.optim.Adam(student.parameters(), lr=LR)\n",
        "\n",
        "print(\"Starting offline distillation training\")\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for step_data, teacher_actions in dataloader:\n",
        "        pixel_values = step_data[\"pixel_values\"].squeeze(0).cuda().half()\n",
        "        input_ids = step_data[\"input_ids\"].squeeze(0).cuda()\n",
        "        attention_mask = step_data[\"attention_mask\"].squeeze(0).cuda()\n",
        "        gt_actions = torch.tensor(teacher_actions).squeeze(0).cuda().half()\n",
        "\n",
        "        x = {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        }\n",
        "\n",
        "        student_out = student(x)[\"actions\"]\n",
        "        loss = F.mse_loss(student_out, gt_actions)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, MSE Loss: {avg:.6f}\")\n",
        "\n",
        "save_file(student.state_dict(), os.path.join(OUTPUT_PATH, \"model.safetensors\"))\n",
        "print(f\"Student model saved to {OUTPUT_PATH}\")\n",
        "\"\"\")\n",
        "\n",
        "# Run with python3 (not python3.10)\n",
        "result = subprocess.run([\"python3\", \"-c\", code], capture_output=True, text=True)\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n",
        "\n"
      ],
      "metadata": {
        "id": "QJmnqdTB-VHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d0acQU5ND84s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mUnOpL0VD87w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lMjKb_6ID8_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8sywHiwND9Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recovery Fine tuning"
      ],
      "metadata": {
        "id": "_nab1GpfD9FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "\n",
        "data_config = DATA_CONFIG_MAP[\"kuka_custom\"]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=\"/content/gr00t_pruned_50_complete\",\n",
        "    embodiment_tag=\"new_embodiment\",\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        ")\n",
        "\n",
        "model = policy.model\n",
        "\n",
        "print(\"Loaded GR00T pruned model successfully.\")\n",
        "print(f\"Model class: {model.__class__.__name__}\")\n",
        "\"\"\")\n",
        "\n",
        "result = subprocess.run([\"python3.10\", \"-c\", code], capture_output=True, text=True)\n",
        "\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OVaQoQiFI06",
        "outputId": "4044792a-0ec6-4349-e5d1-e8769d82e600"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_pruned_50_complete\n",
            "Loading pretrained dual brain from /content/gr00t_pruned_50_complete\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_pruned_50_complete\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Loaded GR00T pruned model successfully.\n",
            "Model class: GR00T_N1\n",
            "\n",
            "STDERR:\n",
            " /usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 20:45:37.566701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 20:45:37.566761: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 20:45:37.568095: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 20:45:38.519049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/gr00t_finetune.py \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --output_dir /content/gr00t_recovered_finetuned \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --base_model_path /content/gr00t_pruned_50_complete \\\n",
        "  --batch_size 4 \\\n",
        "  --max_steps 5000 \\\n",
        "  --save_steps 2500 \\\n",
        "  --no-tune_llm \\\n",
        "  --tune_visual \\\n",
        "  --tune_projector \\\n",
        "  --tune_diffusion_model \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --video_backend decord \\\n",
        "  --report_to tensorboard \\\n",
        "  --num_gpus 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ9xj-MzI-xB",
        "outputId": "b2b76a9d-7c7c-47b0-e6fc-c1ac65b2fd1e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 20:46:04.543205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 20:46:04.543256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 20:46:04.544482: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 20:46:05.684068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "==================================================\n",
            "GR00T FINE-TUNING CONFIGURATION:\n",
            "==================================================\n",
            "dataset_path: /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset\n",
            "output_dir: /content/gr00t_recovered_finetuned\n",
            "data_config: kuka_custom\n",
            "batch_size: 4\n",
            "max_steps: 5000\n",
            "num_gpus: 1\n",
            "save_steps: 2500\n",
            "base_model_path: /content/gr00t_pruned_50_complete\n",
            "tune_llm: False\n",
            "tune_visual: True\n",
            "tune_projector: True\n",
            "tune_diffusion_model: True\n",
            "resume: False\n",
            "learning_rate: 0.0001\n",
            "weight_decay: 1e-05\n",
            "warmup_ratio: 0.05\n",
            "lora_rank: 0\n",
            "lora_alpha: 16\n",
            "lora_dropout: 0.1\n",
            "dataloader_num_workers: 8\n",
            "report_to: tensorboard\n",
            "embodiment_tag: new_embodiment\n",
            "video_backend: decord\n",
            "==================================================\n",
            "\n",
            "Using 1 GPUs\n",
            "Initialized dataset Full_Kuka_Dataset with EmbodimentTag.NEW_EMBODIMENT\n",
            "Loading pretrained dual brain from /content/gr00t_pruned_50_complete\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_pruned_50_complete\n",
            "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in SiglipVisionModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "/content/Isaac-GR00T/gr00t/model/action_head/cross_attention_dit.py:249: FutureWarning: Accessing config attribute `output_dim` directly via 'DiT' object attribute is deprecated. Please access 'output_dim' over 'DiT's config object instead, e.g. 'unet.config.output_dim'.\n",
            "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Run name: /content/gr00t_recovered_finetuned\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "train dataloader length: 7018\n",
            "train dataset length: 28072\n",
            "GPU memory before training: 8.159651279449463 GB\n",
            "TensorBoard logs will be saved to: /content/gr00t_recovered_finetuned/runs\n",
            "  0% 0/5000 [00:00<?, ?it/s]The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "{'loss': 0.0566, 'grad_norm': 1.128785490989685, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
            "{'loss': 0.0673, 'grad_norm': 1.1194803714752197, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
            "{'loss': 0.0384, 'grad_norm': 0.2890309691429138, 'learning_rate': 1.2e-05, 'epoch': 0.0}\n",
            "{'loss': 0.0199, 'grad_norm': 0.2305658906698227, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}\n",
            "{'loss': 0.0379, 'grad_norm': 1.0469893217086792, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.0281, 'grad_norm': 0.535857617855072, 'learning_rate': 2.4e-05, 'epoch': 0.01}\n",
            "{'loss': 0.0167, 'grad_norm': 0.6343327164649963, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}\n",
            "{'loss': 0.0229, 'grad_norm': 0.5250380635261536, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}\n",
            "{'loss': 0.0177, 'grad_norm': 0.4372001886367798, 'learning_rate': 3.6e-05, 'epoch': 0.01}\n",
            "{'loss': 0.0281, 'grad_norm': 0.4696393311023712, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
            "{'loss': 0.0273, 'grad_norm': 0.21068011224269867, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0343, 'grad_norm': 0.41740456223487854, 'learning_rate': 4.8e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0411, 'grad_norm': 0.7868494987487793, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0413, 'grad_norm': 0.7226679921150208, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0245, 'grad_norm': 0.39310386776924133, 'learning_rate': 6e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0296, 'grad_norm': 0.6737079620361328, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0339, 'grad_norm': 0.21512621641159058, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.02}\n",
            "{'loss': 0.0443, 'grad_norm': 0.42122602462768555, 'learning_rate': 7.2e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0269, 'grad_norm': 0.57246994972229, 'learning_rate': 7.6e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0364, 'grad_norm': 0.6350848078727722, 'learning_rate': 8e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0432, 'grad_norm': 0.5700230002403259, 'learning_rate': 8.4e-05, 'epoch': 0.03}\n",
            "{'loss': 0.041, 'grad_norm': 0.4463905394077301, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0493, 'grad_norm': 0.5493106245994568, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0483, 'grad_norm': 0.5487852692604065, 'learning_rate': 9.6e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0522, 'grad_norm': 0.517928421497345, 'learning_rate': 0.0001, 'epoch': 0.04}\n",
            "{'loss': 0.0312, 'grad_norm': 1.0599762201309204, 'learning_rate': 9.999890641901125e-05, 'epoch': 0.04}\n",
            "{'loss': 0.0622, 'grad_norm': 0.5583426356315613, 'learning_rate': 9.99956257238817e-05, 'epoch': 0.04}\n",
            "{'loss': 0.0529, 'grad_norm': 0.5922247171401978, 'learning_rate': 9.999015805811965e-05, 'epoch': 0.04}\n",
            "{'loss': 0.0463, 'grad_norm': 0.41127917170524597, 'learning_rate': 9.998250366089848e-05, 'epoch': 0.04}\n",
            "{'loss': 0.0575, 'grad_norm': 0.6044924259185791, 'learning_rate': 9.997266286704631e-05, 'epoch': 0.04}\n",
            "{'loss': 0.0562, 'grad_norm': 0.5326669812202454, 'learning_rate': 9.996063610703137e-05, 'epoch': 0.04}\n",
            "{'loss': 0.0462, 'grad_norm': 0.4721403121948242, 'learning_rate': 9.994642390694308e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0516, 'grad_norm': 0.3839823603630066, 'learning_rate': 9.993002688846913e-05, 'epoch': 0.05}\n",
            "{'loss': 0.031, 'grad_norm': 0.34792086482048035, 'learning_rate': 9.991144576886823e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0446, 'grad_norm': 0.7285334467887878, 'learning_rate': 9.989068136093873e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0194, 'grad_norm': 0.3460688591003418, 'learning_rate': 9.986773457298311e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0418, 'grad_norm': 0.7128819227218628, 'learning_rate': 9.984260640876821e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0635, 'grad_norm': 0.518873929977417, 'learning_rate': 9.981529796748134e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0524, 'grad_norm': 0.46643146872520447, 'learning_rate': 9.97858104436822e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0687, 'grad_norm': 0.39115896821022034, 'learning_rate': 9.975414512725057e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0369, 'grad_norm': 0.7417157888412476, 'learning_rate': 9.972030340333001e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0709, 'grad_norm': 0.7478544116020203, 'learning_rate': 9.968428675226714e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0329, 'grad_norm': 0.6911685466766357, 'learning_rate': 9.964609674954696e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0584, 'grad_norm': 1.0225341320037842, 'learning_rate': 9.96057350657239e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0305, 'grad_norm': 0.4818757176399231, 'learning_rate': 9.956320346634876e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0431, 'grad_norm': 0.5269176959991455, 'learning_rate': 9.95185038118915e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0378, 'grad_norm': 0.7675641775131226, 'learning_rate': 9.94716380576598e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0636, 'grad_norm': 0.8119481801986694, 'learning_rate': 9.942260825371358e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0395, 'grad_norm': 0.6289470195770264, 'learning_rate': 9.937141654477528e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0338, 'grad_norm': 0.3166036605834961, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0346, 'grad_norm': 0.2949894964694977, 'learning_rate': 9.926255646355804e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0161, 'grad_norm': 0.3814481198787689, 'learning_rate': 9.92048928531717e-05, 'epoch': 0.07}\n",
            "{'loss': 0.0588, 'grad_norm': 0.4492241144180298, 'learning_rate': 9.914507686137019e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0275, 'grad_norm': 0.3792129456996918, 'learning_rate': 9.90831111046988e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0397, 'grad_norm': 0.3961276113986969, 'learning_rate': 9.901899829374047e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0425, 'grad_norm': 0.2490721344947815, 'learning_rate': 9.895274123299723e-05, 'epoch': 0.08}\n",
            "{'loss': 0.036, 'grad_norm': 0.8719230890274048, 'learning_rate': 9.888434282076758e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0441, 'grad_norm': 0.6734160780906677, 'learning_rate': 9.881380604901964e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0508, 'grad_norm': 0.6934139728546143, 'learning_rate': 9.87411340032603e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0367, 'grad_norm': 0.6469799876213074, 'learning_rate': 9.86663298624003e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0548, 'grad_norm': 0.6362919807434082, 'learning_rate': 9.858939689861506e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0258, 'grad_norm': 0.7071079611778259, 'learning_rate': 9.851033847720166e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0288, 'grad_norm': 0.4947265088558197, 'learning_rate': 9.842915805643155e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0397, 'grad_norm': 0.2474195957183838, 'learning_rate': 9.834585918739936e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0477, 'grad_norm': 0.4104402959346771, 'learning_rate': 9.826044551386744e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0708, 'grad_norm': 0.6013357043266296, 'learning_rate': 9.817292077210659e-05, 'epoch': 0.09}\n",
            "{'loss': 0.0328, 'grad_norm': 0.45694682002067566, 'learning_rate': 9.808328879073251e-05, 'epoch': 0.1}\n",
            "{'loss': 0.041, 'grad_norm': 0.5913037061691284, 'learning_rate': 9.799155349053851e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0208, 'grad_norm': 0.36365413665771484, 'learning_rate': 9.789771888432375e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0598, 'grad_norm': 0.3377276659011841, 'learning_rate': 9.780178907671789e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0393, 'grad_norm': 0.500757098197937, 'learning_rate': 9.77037682640015e-05, 'epoch': 0.1}\n",
            "{'loss': 0.018, 'grad_norm': 0.22156165540218353, 'learning_rate': 9.760366073392246e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0436, 'grad_norm': 0.7493307590484619, 'learning_rate': 9.750147086550844e-05, 'epoch': 0.1}\n",
            "{'loss': 0.0543, 'grad_norm': 0.33223816752433777, 'learning_rate': 9.739720312887535e-05, 'epoch': 0.11}\n",
            "{'loss': 0.053, 'grad_norm': 0.5688533186912537, 'learning_rate': 9.729086208503174e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0385, 'grad_norm': 0.8824422359466553, 'learning_rate': 9.718245238567939e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0272, 'grad_norm': 0.5633466839790344, 'learning_rate': 9.707197877300974e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0483, 'grad_norm': 1.049838662147522, 'learning_rate': 9.695944607949649e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0256, 'grad_norm': 0.4048340320587158, 'learning_rate': 9.684485922768422e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0543, 'grad_norm': 0.7562808394432068, 'learning_rate': 9.672822322997305e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0415, 'grad_norm': 0.6585128903388977, 'learning_rate': 9.660954318839933e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0384, 'grad_norm': 0.4620371460914612, 'learning_rate': 9.648882429441257e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0289, 'grad_norm': 0.5404986143112183, 'learning_rate': 9.636607182864827e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0254, 'grad_norm': 0.47828224301338196, 'learning_rate': 9.624129116069694e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0518, 'grad_norm': 0.811813235282898, 'learning_rate': 9.611448774886924e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0381, 'grad_norm': 0.6008068919181824, 'learning_rate': 9.598566713995718e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0518, 'grad_norm': 0.42440858483314514, 'learning_rate': 9.58548349689915e-05, 'epoch': 0.12}\n",
            "{'loss': 0.0345, 'grad_norm': 0.6461933255195618, 'learning_rate': 9.572199695899522e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0355, 'grad_norm': 0.5679421424865723, 'learning_rate': 9.558715892073323e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0392, 'grad_norm': 0.650520384311676, 'learning_rate': 9.545032675245813e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0349, 'grad_norm': 0.297756165266037, 'learning_rate': 9.531150643965223e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0383, 'grad_norm': 0.5607098340988159, 'learning_rate': 9.517070405476575e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0305, 'grad_norm': 0.37859785556793213, 'learning_rate': 9.502792575695112e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0311, 'grad_norm': 0.3796374499797821, 'learning_rate': 9.488317779179361e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0345, 'grad_norm': 0.4701034128665924, 'learning_rate': 9.473646649103818e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0316, 'grad_norm': 0.4171515703201294, 'learning_rate': 9.458779827231237e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0356, 'grad_norm': 0.28263741731643677, 'learning_rate': 9.443717963884569e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0448, 'grad_norm': 0.8028872609138489, 'learning_rate': 9.428461717918511e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0523, 'grad_norm': 1.0048218965530396, 'learning_rate': 9.413011756690685e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0294, 'grad_norm': 0.6502152681350708, 'learning_rate': 9.397368756032445e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0305, 'grad_norm': 0.6674329042434692, 'learning_rate': 9.381533400219318e-05, 'epoch': 0.14}\n",
            "{'loss': 0.0265, 'grad_norm': 0.460300087928772, 'learning_rate': 9.365506381941066e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0407, 'grad_norm': 0.874735951423645, 'learning_rate': 9.349288402271388e-05, 'epoch': 0.15}\n",
            "{'loss': 0.042, 'grad_norm': 0.7219331860542297, 'learning_rate': 9.332880170637252e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0347, 'grad_norm': 0.9595798254013062, 'learning_rate': 9.316282404787871e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0377, 'grad_norm': 0.6741781234741211, 'learning_rate': 9.299495830763286e-05, 'epoch': 0.15}\n",
            "{'loss': 0.027, 'grad_norm': 0.4312041401863098, 'learning_rate': 9.282521182862629e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0184, 'grad_norm': 0.22261781990528107, 'learning_rate': 9.265359203611987e-05, 'epoch': 0.15}\n",
            "{'loss': 0.0376, 'grad_norm': 0.5724735260009766, 'learning_rate': 9.248010643731935e-05, 'epoch': 0.16}\n",
            "{'loss': 0.045, 'grad_norm': 0.2733806371688843, 'learning_rate': 9.230476262104677e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0369, 'grad_norm': 0.3411385416984558, 'learning_rate': 9.212756825740873e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0307, 'grad_norm': 0.8529811501502991, 'learning_rate': 9.194853109746074e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0255, 'grad_norm': 0.28842514753341675, 'learning_rate': 9.176765897286813e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0237, 'grad_norm': 0.2685678005218506, 'learning_rate': 9.158495979556358e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0273, 'grad_norm': 0.47121384739875793, 'learning_rate': 9.140044155740101e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0377, 'grad_norm': 0.5843690633773804, 'learning_rate': 9.121411232980588e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0441, 'grad_norm': 0.4557766914367676, 'learning_rate': 9.102598026342222e-05, 'epoch': 0.17}\n",
            "{'loss': 0.035, 'grad_norm': 0.6667288541793823, 'learning_rate': 9.083605358775612e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0367, 'grad_norm': 0.27658864855766296, 'learning_rate': 9.064434061081562e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0381, 'grad_norm': 0.3640891909599304, 'learning_rate': 9.045084971874738e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0409, 'grad_norm': 0.3298492729663849, 'learning_rate': 9.025558937546988e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0345, 'grad_norm': 0.48405832052230835, 'learning_rate': 9.005856812230304e-05, 'epoch': 0.17}\n",
            "{'loss': 0.0466, 'grad_norm': 0.33730149269104004, 'learning_rate': 8.98597945775948e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0296, 'grad_norm': 0.3426072597503662, 'learning_rate': 8.965927743634391e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0391, 'grad_norm': 0.17531034350395203, 'learning_rate': 8.945702546981969e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0357, 'grad_norm': 0.590785801410675, 'learning_rate': 8.92530475251784e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0402, 'grad_norm': 0.7060582637786865, 'learning_rate': 8.90473525250761e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0417, 'grad_norm': 0.6762171387672424, 'learning_rate': 8.883994946727849e-05, 'epoch': 0.18}\n",
            "{'loss': 0.0413, 'grad_norm': 1.1512620449066162, 'learning_rate': 8.863084742426719e-05, 'epoch': 0.18}\n",
            "{'loss': 0.032, 'grad_norm': 0.6733593940734863, 'learning_rate': 8.842005554284296e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0346, 'grad_norm': 0.6774088740348816, 'learning_rate': 8.820758304372557e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0456, 'grad_norm': 0.7156211733818054, 'learning_rate': 8.799343922115044e-05, 'epoch': 0.19}\n",
            "{'loss': 0.025, 'grad_norm': 0.31506913900375366, 'learning_rate': 8.77776334424621e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0453, 'grad_norm': 1.3116531372070312, 'learning_rate': 8.756017514770443e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0293, 'grad_norm': 0.32261723279953003, 'learning_rate': 8.73410738492077e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0488, 'grad_norm': 0.47789546847343445, 'learning_rate': 8.71203391311725e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0307, 'grad_norm': 0.31241750717163086, 'learning_rate': 8.689798064925049e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0272, 'grad_norm': 0.38106217980384827, 'learning_rate': 8.6674008130122e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0321, 'grad_norm': 0.6408520936965942, 'learning_rate': 8.644843137107059e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0459, 'grad_norm': 0.3852924704551697, 'learning_rate': 8.622126023955446e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0282, 'grad_norm': 0.3121860921382904, 'learning_rate': 8.599250467277483e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0347, 'grad_norm': 0.43970024585723877, 'learning_rate': 8.576217467724128e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0293, 'grad_norm': 0.755355179309845, 'learning_rate': 8.553028032833397e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0357, 'grad_norm': 0.22102198004722595, 'learning_rate': 8.529683176986295e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0258, 'grad_norm': 0.35247695446014404, 'learning_rate': 8.506183921362443e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0615, 'grad_norm': 1.410597801208496, 'learning_rate': 8.482531293895412e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0285, 'grad_norm': 0.6141176223754883, 'learning_rate': 8.458726329227747e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0224, 'grad_norm': 0.17507700622081757, 'learning_rate': 8.434770068665723e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0272, 'grad_norm': 0.5140872597694397, 'learning_rate': 8.410663560133784e-05, 'epoch': 0.21}\n",
            "{'loss': 0.021, 'grad_norm': 0.23558643460273743, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0255, 'grad_norm': 0.719936192035675, 'learning_rate': 8.362004023673474e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0235, 'grad_norm': 0.46291133761405945, 'learning_rate': 8.337453124270863e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0265, 'grad_norm': 0.27494826912879944, 'learning_rate': 8.31275623385675e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0292, 'grad_norm': 0.20143455266952515, 'learning_rate': 8.287914432753123e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0431, 'grad_norm': 0.5777794122695923, 'learning_rate': 8.262928807620843e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0393, 'grad_norm': 0.2944813072681427, 'learning_rate': 8.237800451412095e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0527, 'grad_norm': 0.4309273660182953, 'learning_rate': 8.212530463322583e-05, 'epoch': 0.22}\n",
            "{'loss': 0.0238, 'grad_norm': 0.2693815231323242, 'learning_rate': 8.18711994874345e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0303, 'grad_norm': 0.6385652422904968, 'learning_rate': 8.161570019212921e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0248, 'grad_norm': 0.35206112265586853, 'learning_rate': 8.135881792367686e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0222, 'grad_norm': 0.36084264516830444, 'learning_rate': 8.110056391894005e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0265, 'grad_norm': 0.43154776096343994, 'learning_rate': 8.084094947478556e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0368, 'grad_norm': 0.3411639332771301, 'learning_rate': 8.057998594759022e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0343, 'grad_norm': 0.8891053795814514, 'learning_rate': 8.031768475274413e-05, 'epoch': 0.23}\n",
            "{'loss': 0.0401, 'grad_norm': 0.7903939485549927, 'learning_rate': 8.005405736415126e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0226, 'grad_norm': 0.3196123242378235, 'learning_rate': 7.978911531372765e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0298, 'grad_norm': 0.2925596833229065, 'learning_rate': 7.952287019089685e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0322, 'grad_norm': 0.48564672470092773, 'learning_rate': 7.925533364208309e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0186, 'grad_norm': 0.28172412514686584, 'learning_rate': 7.898651737020166e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0296, 'grad_norm': 0.28261762857437134, 'learning_rate': 7.871643313414718e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0196, 'grad_norm': 0.24523934721946716, 'learning_rate': 7.844509274827907e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0599, 'grad_norm': 0.4748106598854065, 'learning_rate': 7.817250808190483e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0917, 'grad_norm': 0.6871548295021057, 'learning_rate': 7.789869105876083e-05, 'epoch': 0.25}\n",
            "{'loss': 0.035, 'grad_norm': 0.649092435836792, 'learning_rate': 7.762365365649067e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0246, 'grad_norm': 0.22171813249588013, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.25}\n",
            "{'loss': 0.031, 'grad_norm': 0.48786044120788574, 'learning_rate': 7.70699658915369e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0285, 'grad_norm': 0.5340459942817688, 'learning_rate': 7.679133974894983e-05, 'epoch': 0.25}\n",
            "{'loss': 0.029, 'grad_norm': 0.22177298367023468, 'learning_rate': 7.651154166637025e-05, 'epoch': 0.25}\n",
            "{'loss': 0.0201, 'grad_norm': 0.247237890958786, 'learning_rate': 7.623058388307269e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0261, 'grad_norm': 0.40993475914001465, 'learning_rate': 7.594847868906076e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0303, 'grad_norm': 0.48892757296562195, 'learning_rate': 7.566523842452958e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0241, 'grad_norm': 0.22415748238563538, 'learning_rate': 7.538087547932585e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0289, 'grad_norm': 0.596916139125824, 'learning_rate': 7.509540229240601e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0345, 'grad_norm': 0.5662222504615784, 'learning_rate': 7.480883135129211e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0289, 'grad_norm': 0.5426652431488037, 'learning_rate': 7.452117519152542e-05, 'epoch': 0.26}\n",
            "{'loss': 0.0403, 'grad_norm': 1.2393839359283447, 'learning_rate': 7.423244639611826e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0449, 'grad_norm': 0.43303295969963074, 'learning_rate': 7.394265759500348e-05, 'epoch': 0.27}\n",
            "{'loss': 0.018, 'grad_norm': 0.27701711654663086, 'learning_rate': 7.365182146448205e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0314, 'grad_norm': 0.5257946848869324, 'learning_rate': 7.335995072666848e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0343, 'grad_norm': 0.3349822461605072, 'learning_rate': 7.30670581489344e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0415, 'grad_norm': 0.6725331544876099, 'learning_rate': 7.277315654334997e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0343, 'grad_norm': 0.5471007227897644, 'learning_rate': 7.247825876612353e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0349, 'grad_norm': 0.21348942816257477, 'learning_rate': 7.218237771703921e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0263, 'grad_norm': 0.7367433905601501, 'learning_rate': 7.188552633889259e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0245, 'grad_norm': 0.3767470121383667, 'learning_rate': 7.158771761692464e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0204, 'grad_norm': 0.2515780031681061, 'learning_rate': 7.128896457825364e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0294, 'grad_norm': 0.6084213852882385, 'learning_rate': 7.09892802913053e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0203, 'grad_norm': 0.3867761790752411, 'learning_rate': 7.068867786524116e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0438, 'grad_norm': 0.52139812707901, 'learning_rate': 7.038717044938519e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0263, 'grad_norm': 0.6147079467773438, 'learning_rate': 7.008477123264848e-05, 'epoch': 0.28}\n",
            "{'loss': 0.0257, 'grad_norm': 0.6671499609947205, 'learning_rate': 6.978149344295242e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0249, 'grad_norm': 0.5522366762161255, 'learning_rate': 6.947735034665002e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0317, 'grad_norm': 0.6985076665878296, 'learning_rate': 6.917235524794558e-05, 'epoch': 0.29}\n",
            "{'loss': 0.036, 'grad_norm': 0.5744683146476746, 'learning_rate': 6.886652148831279e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0307, 'grad_norm': 0.7654780149459839, 'learning_rate': 6.855986244591104e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0288, 'grad_norm': 0.38896992802619934, 'learning_rate': 6.825239153500029e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0236, 'grad_norm': 0.376792848110199, 'learning_rate': 6.794412220535426e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0235, 'grad_norm': 0.5245242118835449, 'learning_rate': 6.763506794167208e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0281, 'grad_norm': 0.9308373928070068, 'learning_rate': 6.732524226298841e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0258, 'grad_norm': 0.29473602771759033, 'learning_rate': 6.701465872208216e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0227, 'grad_norm': 0.5089647173881531, 'learning_rate': 6.670333090488356e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0289, 'grad_norm': 0.39076679944992065, 'learning_rate': 6.639127242987988e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0337, 'grad_norm': 0.30175772309303284, 'learning_rate': 6.607849694751977e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0394, 'grad_norm': 0.25136691331863403, 'learning_rate': 6.576501813961609e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0269, 'grad_norm': 0.4302510917186737, 'learning_rate': 6.545084971874738e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0432, 'grad_norm': 0.7010806798934937, 'learning_rate': 6.513600542765817e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0408, 'grad_norm': 0.351310133934021, 'learning_rate': 6.48204990386577e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0253, 'grad_norm': 0.5257596969604492, 'learning_rate': 6.450434435301751e-05, 'epoch': 0.31}\n",
            "{'loss': 0.031, 'grad_norm': 0.36386996507644653, 'learning_rate': 6.418755520036775e-05, 'epoch': 0.31}\n",
            "{'loss': 0.021, 'grad_norm': 0.48272213339805603, 'learning_rate': 6.387014543809223e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0738, 'grad_norm': 0.6045370101928711, 'learning_rate': 6.355212895072223e-05, 'epoch': 0.31}\n",
            "{'loss': 0.0319, 'grad_norm': 0.3411273956298828, 'learning_rate': 6.323351964932908e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0485, 'grad_norm': 0.4047580361366272, 'learning_rate': 6.291433147091583e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0318, 'grad_norm': 0.3766031861305237, 'learning_rate': 6.259457837780742e-05, 'epoch': 0.32}\n",
            "{'loss': 0.032, 'grad_norm': 0.47106146812438965, 'learning_rate': 6.227427435703997e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0307, 'grad_norm': 0.39596647024154663, 'learning_rate': 6.195343341974899e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0414, 'grad_norm': 0.5624241828918457, 'learning_rate': 6.163206960055651e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0478, 'grad_norm': 0.520587146282196, 'learning_rate': 6.131019695695702e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0301, 'grad_norm': 0.6479803323745728, 'learning_rate': 6.0987829568702656e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0439, 'grad_norm': 0.39529651403427124, 'learning_rate': 6.066498153718735e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0272, 'grad_norm': 0.39974406361579895, 'learning_rate': 6.034166698482984e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0305, 'grad_norm': 0.29038283228874207, 'learning_rate': 6.001790005445607e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0238, 'grad_norm': 0.6843763589859009, 'learning_rate': 5.969369490868042e-05, 'epoch': 0.33}\n",
            "{'loss': 0.025, 'grad_norm': 0.5514174699783325, 'learning_rate': 5.9369065729286245e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0283, 'grad_norm': 0.45514631271362305, 'learning_rate': 5.90440267166055e-05, 'epoch': 0.33}\n",
            "{'loss': 0.0237, 'grad_norm': 0.4461638331413269, 'learning_rate': 5.871859208889759e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0284, 'grad_norm': 0.5986854434013367, 'learning_rate': 5.8392776081727385e-05, 'epoch': 0.34}\n",
            "{'loss': 0.025, 'grad_norm': 0.5370329022407532, 'learning_rate': 5.8066592947342555e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0301, 'grad_norm': 0.4677866995334625, 'learning_rate': 5.7740056954050084e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0196, 'grad_norm': 0.2966081202030182, 'learning_rate': 5.74131823855921e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0441, 'grad_norm': 0.2877251207828522, 'learning_rate': 5.7085983540521216e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0277, 'grad_norm': 0.6754294633865356, 'learning_rate': 5.675847473157485e-05, 'epoch': 0.34}\n",
            "{'loss': 0.0269, 'grad_norm': 0.27527254819869995, 'learning_rate': 5.6430670285049314e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0341, 'grad_norm': 0.794159471988678, 'learning_rate': 5.6102584540173006e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0369, 'grad_norm': 0.5574747920036316, 'learning_rate': 5.577423184847932e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0206, 'grad_norm': 0.5612127184867859, 'learning_rate': 5.544562657317863e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0242, 'grad_norm': 0.33513545989990234, 'learning_rate': 5.511678308853026e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0252, 'grad_norm': 0.3891351521015167, 'learning_rate': 5.478771577921351e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0328, 'grad_norm': 0.18039993941783905, 'learning_rate': 5.445843903969854e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0266, 'grad_norm': 0.5279309153556824, 'learning_rate': 5.4128967273616625e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0304, 'grad_norm': 0.25616559386253357, 'learning_rate': 5.379931489313016e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0333, 'grad_norm': 0.29972484707832336, 'learning_rate': 5.3469496318302204e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0342, 'grad_norm': 0.8925487995147705, 'learning_rate': 5.313952597646568e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0303, 'grad_norm': 0.44030439853668213, 'learning_rate': 5.280941830159227e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0316, 'grad_norm': 0.35092058777809143, 'learning_rate': 5.247918773366112e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0231, 'grad_norm': 0.6712638139724731, 'learning_rate': 5.214884871802703e-05, 'epoch': 0.36}\n",
            "{'loss': 0.0327, 'grad_norm': 0.34313544631004333, 'learning_rate': 5.1818415704788725e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0165, 'grad_norm': 0.6354212760925293, 'learning_rate': 5.148790314815663e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0226, 'grad_norm': 0.3873981833457947, 'learning_rate': 5.1157325505820694e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0186, 'grad_norm': 0.24842457473278046, 'learning_rate': 5.0826697238317935e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0223, 'grad_norm': 1.043185830116272, 'learning_rate': 5.0496032808399815e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0161, 'grad_norm': 0.4288707375526428, 'learning_rate': 5.016534668039976e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0213, 'grad_norm': 0.20031018555164337, 'learning_rate': 4.9834653319600246e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0236, 'grad_norm': 0.30158671736717224, 'learning_rate': 4.950396719160018e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0321, 'grad_norm': 0.2705470621585846, 'learning_rate': 4.917330276168208e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0163, 'grad_norm': 0.2543568015098572, 'learning_rate': 4.884267449417931e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0153, 'grad_norm': 0.26738476753234863, 'learning_rate': 4.851209685184338e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0257, 'grad_norm': 0.20371919870376587, 'learning_rate': 4.818158429521129e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0211, 'grad_norm': 0.23828330636024475, 'learning_rate': 4.785115128197298e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0246, 'grad_norm': 0.13894270360469818, 'learning_rate': 4.7520812266338885e-05, 'epoch': 0.38}\n",
            "{'loss': 0.0166, 'grad_norm': 0.7099860906600952, 'learning_rate': 4.7190581698407725e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0281, 'grad_norm': 0.5634819269180298, 'learning_rate': 4.6860474023534335e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0168, 'grad_norm': 0.2325936108827591, 'learning_rate': 4.65305036816978e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0185, 'grad_norm': 0.33377671241760254, 'learning_rate': 4.620068510686985e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0238, 'grad_norm': 0.3013489544391632, 'learning_rate': 4.5871032726383386e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0255, 'grad_norm': 0.5196808576583862, 'learning_rate': 4.554156096030149e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0155, 'grad_norm': 0.22326427698135376, 'learning_rate': 4.5212284220786494e-05, 'epoch': 0.39}\n",
            "{'loss': 0.0184, 'grad_norm': 0.603134274482727, 'learning_rate': 4.488321691146975e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0165, 'grad_norm': 0.18310551345348358, 'learning_rate': 4.4554373426821374e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0187, 'grad_norm': 0.21990540623664856, 'learning_rate': 4.4225768151520694e-05, 'epoch': 0.4}\n",
            "{'loss': 0.027, 'grad_norm': 0.34051650762557983, 'learning_rate': 4.3897415459827e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0294, 'grad_norm': 0.7391195297241211, 'learning_rate': 4.3569329714950704e-05, 'epoch': 0.4}\n",
            "{'loss': 0.013, 'grad_norm': 0.1296246200799942, 'learning_rate': 4.324152526842517e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0371, 'grad_norm': 0.5469731092453003, 'learning_rate': 4.291401645947879e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0199, 'grad_norm': 0.9097554087638855, 'learning_rate': 4.2586817614407895e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0151, 'grad_norm': 0.34389039874076843, 'learning_rate': 4.2259943045949934e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0165, 'grad_norm': 0.4317546784877777, 'learning_rate': 4.1933407052657456e-05, 'epoch': 0.41}\n",
            "{'loss': 0.023, 'grad_norm': 0.4753378927707672, 'learning_rate': 4.160722391827262e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0311, 'grad_norm': 0.254341721534729, 'learning_rate': 4.1281407911102425e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0372, 'grad_norm': 0.3390559256076813, 'learning_rate': 4.095597328339452e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0383, 'grad_norm': 0.5731344819068909, 'learning_rate': 4.063093427071376e-05, 'epoch': 0.41}\n",
            "{'loss': 0.0139, 'grad_norm': 0.11850430816411972, 'learning_rate': 4.0306305091319595e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0255, 'grad_norm': 0.5006719827651978, 'learning_rate': 3.9982099945543945e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0218, 'grad_norm': 0.41032931208610535, 'learning_rate': 3.965833301517017e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0265, 'grad_norm': 0.23898600041866302, 'learning_rate': 3.933501846281267e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0147, 'grad_norm': 0.4433373510837555, 'learning_rate': 3.901217043129735e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0158, 'grad_norm': 0.3535097539424896, 'learning_rate': 3.8689803043043e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0256, 'grad_norm': 0.47721847891807556, 'learning_rate': 3.836793039944349e-05, 'epoch': 0.42}\n",
            "{'loss': 0.0289, 'grad_norm': 0.22639694809913635, 'learning_rate': 3.8046566580251e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0277, 'grad_norm': 0.2935684323310852, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0181, 'grad_norm': 0.3732638359069824, 'learning_rate': 3.74054216221926e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0238, 'grad_norm': 0.26901599764823914, 'learning_rate': 3.7085668529084184e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0195, 'grad_norm': 0.5003439784049988, 'learning_rate': 3.676648035067093e-05, 'epoch': 0.43}\n",
            "{'loss': 0.026, 'grad_norm': 0.41428786516189575, 'learning_rate': 3.6447871049277796e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0116, 'grad_norm': 0.4614727795124054, 'learning_rate': 3.612985456190778e-05, 'epoch': 0.43}\n",
            "{'loss': 0.025, 'grad_norm': 0.28828784823417664, 'learning_rate': 3.581244479963225e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0158, 'grad_norm': 0.3473781943321228, 'learning_rate': 3.5495655646982505e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0171, 'grad_norm': 0.40249884128570557, 'learning_rate': 3.517950096134232e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0286, 'grad_norm': 0.374467134475708, 'learning_rate': 3.4863994572341843e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0195, 'grad_norm': 0.24207919836044312, 'learning_rate': 3.4549150281252636e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0235, 'grad_norm': 0.6199809312820435, 'learning_rate': 3.423498186038393e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0149, 'grad_norm': 0.2586860656738281, 'learning_rate': 3.392150305248024e-05, 'epoch': 0.44}\n",
            "{'loss': 0.0195, 'grad_norm': 0.3349500298500061, 'learning_rate': 3.360872757012011e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0153, 'grad_norm': 0.3716855049133301, 'learning_rate': 3.329666909511645e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0147, 'grad_norm': 0.3396827280521393, 'learning_rate': 3.298534127791785e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0334, 'grad_norm': 0.17684803903102875, 'learning_rate': 3.267475773701161e-05, 'epoch': 0.45}\n",
            "{'loss': 0.017, 'grad_norm': 0.23439326882362366, 'learning_rate': 3.236493205832795e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0206, 'grad_norm': 0.3746092915534973, 'learning_rate': 3.205587779464576e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0283, 'grad_norm': 0.38893967866897583, 'learning_rate': 3.1747608464999725e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0183, 'grad_norm': 0.2860335409641266, 'learning_rate': 3.144013755408895e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0162, 'grad_norm': 0.51277756690979, 'learning_rate': 3.113347851168721e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0145, 'grad_norm': 0.4431398808956146, 'learning_rate': 3.082764475205442e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0247, 'grad_norm': 0.3576700687408447, 'learning_rate': 3.052264965335e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0122, 'grad_norm': 0.26254135370254517, 'learning_rate': 3.0218506557047598e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0307, 'grad_norm': 0.35758015513420105, 'learning_rate': 2.991522876735154e-05, 'epoch': 0.46}\n",
            "{'loss': 0.0505, 'grad_norm': 0.4743243455886841, 'learning_rate': 2.9612829550614836e-05, 'epoch': 0.46}\n",
            "{'loss': 0.021, 'grad_norm': 0.16167695820331573, 'learning_rate': 2.931132213475884e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0273, 'grad_norm': 0.26341983675956726, 'learning_rate': 2.9010719708694722e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0295, 'grad_norm': 0.4624570310115814, 'learning_rate': 2.8711035421746367e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0266, 'grad_norm': 0.3511357605457306, 'learning_rate': 2.8412282383075363e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0196, 'grad_norm': 0.39537709951400757, 'learning_rate': 2.811447366110741e-05, 'epoch': 0.47}\n",
            "{'loss': 0.016, 'grad_norm': 0.3323928415775299, 'learning_rate': 2.7817622282960815e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0223, 'grad_norm': 0.33130088448524475, 'learning_rate': 2.7521741233876496e-05, 'epoch': 0.47}\n",
            "{'loss': 0.0198, 'grad_norm': 0.25545546412467957, 'learning_rate': 2.7226843456650037e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0295, 'grad_norm': 0.13207504153251648, 'learning_rate': 2.693294185106562e-05, 'epoch': 0.48}\n",
            "{'loss': 0.011, 'grad_norm': 0.13450884819030762, 'learning_rate': 2.6640049273331515e-05, 'epoch': 0.48}\n",
            "{'loss': 0.008, 'grad_norm': 0.21298056840896606, 'learning_rate': 2.6348178535517966e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0178, 'grad_norm': 0.3269748389720917, 'learning_rate': 2.6057342404996522e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0153, 'grad_norm': 0.24248212575912476, 'learning_rate': 2.5767553603881767e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0249, 'grad_norm': 0.18530836701393127, 'learning_rate': 2.547882480847461e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0203, 'grad_norm': 0.3459658920764923, 'learning_rate': 2.5191168648707887e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0287, 'grad_norm': 0.2052689492702484, 'learning_rate': 2.490459770759398e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0241, 'grad_norm': 0.2983453869819641, 'learning_rate': 2.4619124520674146e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0274, 'grad_norm': 0.5232238173484802, 'learning_rate': 2.433476157547044e-05, 'epoch': 0.49}\n",
            "{'loss': 0.027, 'grad_norm': 0.5273234248161316, 'learning_rate': 2.405152131093926e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0153, 'grad_norm': 0.36187490820884705, 'learning_rate': 2.3769416116927335e-05, 'epoch': 0.49}\n",
            "{'loss': 0.019, 'grad_norm': 0.2405499815940857, 'learning_rate': 2.3488458333629777e-05, 'epoch': 0.49}\n",
            "{'loss': 0.0158, 'grad_norm': 0.17196886241436005, 'learning_rate': 2.3208660251050158e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0246, 'grad_norm': 1.0894302129745483, 'learning_rate': 2.29300341084631e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0166, 'grad_norm': 0.3640495836734772, 'learning_rate': 2.2652592093878666e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0238, 'grad_norm': 0.5345442295074463, 'learning_rate': 2.237634634350934e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0203, 'grad_norm': 0.5282937288284302, 'learning_rate': 2.2101308941239203e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0115, 'grad_norm': 0.3099782168865204, 'learning_rate': 2.182749191809518e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0304, 'grad_norm': 0.485614538192749, 'learning_rate': 2.1554907251720945e-05, 'epoch': 0.5}\n",
            "{'loss': 0.0357, 'grad_norm': 0.5249229669570923, 'learning_rate': 2.128356686585282e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0366, 'grad_norm': 0.3707239031791687, 'learning_rate': 2.1013482629798333e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0373, 'grad_norm': 0.7319945693016052, 'learning_rate': 2.0744666357916925e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0228, 'grad_norm': 0.6920149326324463, 'learning_rate': 2.0477129809103147e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0226, 'grad_norm': 0.35244300961494446, 'learning_rate': 2.0210884686272368e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0129, 'grad_norm': 0.30261924862861633, 'learning_rate': 1.9945942635848748e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0312, 'grad_norm': 0.6173079609870911, 'learning_rate': 1.9682315247255894e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0187, 'grad_norm': 0.16331589221954346, 'learning_rate': 1.942001405240979e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0202, 'grad_norm': 0.6183362007141113, 'learning_rate': 1.9159050525214452e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0278, 'grad_norm': 0.38717079162597656, 'learning_rate': 1.8899436081059975e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0189, 'grad_norm': 0.37867283821105957, 'learning_rate': 1.8641182076323148e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0431, 'grad_norm': 0.5827931761741638, 'learning_rate': 1.838429980787081e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0234, 'grad_norm': 0.3046736717224121, 'learning_rate': 1.8128800512565513e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0109, 'grad_norm': 0.5718783736228943, 'learning_rate': 1.787469536677419e-05, 'epoch': 0.52}\n",
            "{'loss': 0.0267, 'grad_norm': 0.5277161598205566, 'learning_rate': 1.7621995485879062e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0229, 'grad_norm': 0.22192144393920898, 'learning_rate': 1.7370711923791567e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0181, 'grad_norm': 0.4073824882507324, 'learning_rate': 1.712085567246878e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0297, 'grad_norm': 0.6126202344894409, 'learning_rate': 1.6872437661432517e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0203, 'grad_norm': 0.219778373837471, 'learning_rate': 1.662546875729138e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0186, 'grad_norm': 0.35304537415504456, 'learning_rate': 1.637995976326527e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0124, 'grad_norm': 0.26816385984420776, 'learning_rate': 1.6135921418712956e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0209, 'grad_norm': 0.5859585404396057, 'learning_rate': 1.5893364398662176e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0281, 'grad_norm': 0.27187836170196533, 'learning_rate': 1.5652299313342773e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0242, 'grad_norm': 0.6184488534927368, 'learning_rate': 1.5412736707722537e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0198, 'grad_norm': 0.2706862986087799, 'learning_rate': 1.517468706104589e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0253, 'grad_norm': 0.31621429324150085, 'learning_rate': 1.4938160786375572e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0173, 'grad_norm': 0.20305299758911133, 'learning_rate': 1.470316823013707e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0297, 'grad_norm': 0.3207492232322693, 'learning_rate': 1.4469719671666043e-05, 'epoch': 0.54}\n",
            "{'loss': 0.0283, 'grad_norm': 0.5847471356391907, 'learning_rate': 1.4237825322758736e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0203, 'grad_norm': 0.15099067986011505, 'learning_rate': 1.4007495327225162e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0148, 'grad_norm': 0.10051504522562027, 'learning_rate': 1.3778739760445552e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0157, 'grad_norm': 0.21084730327129364, 'learning_rate': 1.3551568628929434e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0243, 'grad_norm': 0.2375740110874176, 'learning_rate': 1.3325991869878013e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0196, 'grad_norm': 0.6233916282653809, 'learning_rate': 1.3102019350749528e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0175, 'grad_norm': 0.18230986595153809, 'learning_rate': 1.2879660868827508e-05, 'epoch': 0.55}\n",
            "{'loss': 0.0147, 'grad_norm': 0.38803377747535706, 'learning_rate': 1.2658926150792322e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0121, 'grad_norm': 0.3993435502052307, 'learning_rate': 1.243982485229559e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0225, 'grad_norm': 1.1958503723144531, 'learning_rate': 1.2222366557537911e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0112, 'grad_norm': 0.19191104173660278, 'learning_rate': 1.2006560778849578e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0103, 'grad_norm': 0.3692326545715332, 'learning_rate': 1.1792416956274444e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0226, 'grad_norm': 0.3116147518157959, 'learning_rate': 1.157994445715706e-05, 'epoch': 0.56}\n",
            "{'loss': 0.024, 'grad_norm': 0.15690626204013824, 'learning_rate': 1.1369152575732822e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0178, 'grad_norm': 0.5521041750907898, 'learning_rate': 1.1160050532721528e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0144, 'grad_norm': 0.6134569644927979, 'learning_rate': 1.095264747492391e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0263, 'grad_norm': 0.34133824706077576, 'learning_rate': 1.0746952474821614e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0156, 'grad_norm': 0.15272268652915955, 'learning_rate': 1.0542974530180327e-05, 'epoch': 0.57}\n",
            "{'loss': 0.02, 'grad_norm': 0.36750903725624084, 'learning_rate': 1.0340722563656107e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0305, 'grad_norm': 0.13072945177555084, 'learning_rate': 1.0140205422405214e-05, 'epoch': 0.57}\n",
            "{'loss': 0.0145, 'grad_norm': 0.13838903605937958, 'learning_rate': 9.941431877696955e-06, 'epoch': 0.57}\n",
            "{'loss': 0.0347, 'grad_norm': 0.2935420572757721, 'learning_rate': 9.744410624530148e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0128, 'grad_norm': 0.1497795581817627, 'learning_rate': 9.549150281252633e-06, 'epoch': 0.58}\n",
            "{'loss': 0.029, 'grad_norm': 0.13382412493228912, 'learning_rate': 9.355659389184396e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0198, 'grad_norm': 0.3926634192466736, 'learning_rate': 9.163946412243896e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0334, 'grad_norm': 0.5171880722045898, 'learning_rate': 8.974019736577777e-06, 'epoch': 0.58}\n",
            "{'loss': 0.019, 'grad_norm': 0.5006824135780334, 'learning_rate': 8.785887670194138e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0323, 'grad_norm': 0.6581718325614929, 'learning_rate': 8.599558442598998e-06, 'epoch': 0.58}\n",
            "{'loss': 0.0169, 'grad_norm': 0.478680819272995, 'learning_rate': 8.415040204436426e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0134, 'grad_norm': 0.15400730073451996, 'learning_rate': 8.232341027131885e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0136, 'grad_norm': 0.2645317018032074, 'learning_rate': 8.051468902539272e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0169, 'grad_norm': 0.3719949722290039, 'learning_rate': 7.872431742591268e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0217, 'grad_norm': 0.12324504554271698, 'learning_rate': 7.695237378953223e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0173, 'grad_norm': 0.5821826457977295, 'learning_rate': 7.519893562680663e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0153, 'grad_norm': 0.3137139081954956, 'learning_rate': 7.3464079638801365e-06, 'epoch': 0.59}\n",
            "{'loss': 0.0275, 'grad_norm': 0.4341557025909424, 'learning_rate': 7.174788171373731e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0146, 'grad_norm': 0.4669596254825592, 'learning_rate': 7.005041692367154e-06, 'epoch': 0.6}\n",
            "{'loss': 0.022, 'grad_norm': 0.38140183687210083, 'learning_rate': 6.837175952121306e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0222, 'grad_norm': 0.2427193969488144, 'learning_rate': 6.671198293627479e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0372, 'grad_norm': 0.23705066740512848, 'learning_rate': 6.5071159772861436e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0172, 'grad_norm': 0.5069279074668884, 'learning_rate': 6.344936180589351e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0103, 'grad_norm': 0.13942350447177887, 'learning_rate': 6.184665997806832e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0187, 'grad_norm': 0.07110580801963806, 'learning_rate': 6.026312439675552e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0162, 'grad_norm': 0.20308741927146912, 'learning_rate': 5.869882433093155e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0149, 'grad_norm': 0.14467564225196838, 'learning_rate': 5.715382820814885e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0091, 'grad_norm': 0.24895994365215302, 'learning_rate': 5.562820361154314e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0272, 'grad_norm': 0.4006349742412567, 'learning_rate': 5.412201727687644e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0156, 'grad_norm': 0.2517448663711548, 'learning_rate': 5.263533508961827e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0247, 'grad_norm': 0.1404338777065277, 'learning_rate': 5.116822208206396e-06, 'epoch': 0.61}\n",
            "{'loss': 0.0242, 'grad_norm': 0.488162636756897, 'learning_rate': 4.972074243048897e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0145, 'grad_norm': 0.23903702199459076, 'learning_rate': 4.829295945234258e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0139, 'grad_norm': 0.1169155165553093, 'learning_rate': 4.688493560347773e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0151, 'grad_norm': 0.3235781192779541, 'learning_rate': 4.549673247541875e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0226, 'grad_norm': 0.2287001758813858, 'learning_rate': 4.412841079266777e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0202, 'grad_norm': 0.36608514189720154, 'learning_rate': 4.27800304100478e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0166, 'grad_norm': 0.3172740340232849, 'learning_rate': 4.145165031008508e-06, 'epoch': 0.62}\n",
            "{'loss': 0.0266, 'grad_norm': 0.38134875893592834, 'learning_rate': 4.01433286004283e-06, 'epoch': 0.63}\n",
            "{'loss': 0.018, 'grad_norm': 0.07668819278478622, 'learning_rate': 3.885512251130763e-06, 'epoch': 0.63}\n",
            "{'loss': 0.045, 'grad_norm': 0.4798314571380615, 'learning_rate': 3.75870883930306e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0121, 'grad_norm': 0.1510574072599411, 'learning_rate': 3.6339281713517303e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0237, 'grad_norm': 0.14776523411273956, 'learning_rate': 3.511175705587433e-06, 'epoch': 0.63}\n",
            "{'loss': 0.022, 'grad_norm': 0.44962215423583984, 'learning_rate': 3.390456811600673e-06, 'epoch': 0.63}\n",
            "{'loss': 0.013, 'grad_norm': 0.31735455989837646, 'learning_rate': 3.271776770026963e-06, 'epoch': 0.63}\n",
            "{'loss': 0.0212, 'grad_norm': 0.14385835826396942, 'learning_rate': 3.155140772315773e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0115, 'grad_norm': 0.3679725229740143, 'learning_rate': 3.040553920503503e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0101, 'grad_norm': 0.4942588806152344, 'learning_rate': 2.928021226990263e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0112, 'grad_norm': 0.24157874286174774, 'learning_rate': 2.817547614320615e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0326, 'grad_norm': 0.5831852555274963, 'learning_rate': 2.7091379149682685e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0196, 'grad_norm': 0.2513081729412079, 'learning_rate': 2.602796871124663e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0097, 'grad_norm': 0.15452350676059723, 'learning_rate': 2.4985291344915674e-06, 'epoch': 0.64}\n",
            "{'loss': 0.0199, 'grad_norm': 0.299945205450058, 'learning_rate': 2.3963392660775575e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0135, 'grad_norm': 0.4779902994632721, 'learning_rate': 2.296231735998511e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0098, 'grad_norm': 0.07393565773963928, 'learning_rate': 2.1982109232821178e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0241, 'grad_norm': 0.33781173825263977, 'learning_rate': 2.102281115676258e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0203, 'grad_norm': 0.25174155831336975, 'learning_rate': 2.008446509461498e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0236, 'grad_norm': 0.759065568447113, 'learning_rate': 1.91671120926748e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0127, 'grad_norm': 0.3754681646823883, 'learning_rate': 1.8270792278934302e-06, 'epoch': 0.65}\n",
            "{'loss': 0.0156, 'grad_norm': 0.43712660670280457, 'learning_rate': 1.7395544861325718e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0257, 'grad_norm': 0.5231953263282776, 'learning_rate': 1.6541408126006463e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0179, 'grad_norm': 0.301906943321228, 'learning_rate': 1.5708419435684462e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0118, 'grad_norm': 0.2168475091457367, 'learning_rate': 1.4896615227983468e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0133, 'grad_norm': 0.17164844274520874, 'learning_rate': 1.4106031013849496e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0284, 'grad_norm': 0.3235485255718231, 'learning_rate': 1.333670137599713e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0231, 'grad_norm': 0.3031786382198334, 'learning_rate': 1.2588659967397e-06, 'epoch': 0.66}\n",
            "{'loss': 0.0374, 'grad_norm': 0.7104330062866211, 'learning_rate': 1.1861939509803687e-06, 'epoch': 0.67}\n",
            "{'loss': 0.016, 'grad_norm': 0.2020801156759262, 'learning_rate': 1.1156571792324211e-06, 'epoch': 0.67}\n",
            "{'loss': 0.0176, 'grad_norm': 0.42757681012153625, 'learning_rate': 1.0472587670027678e-06, 'epoch': 0.67}\n",
            "{'loss': 0.0157, 'grad_norm': 0.11538434773683548, 'learning_rate': 9.810017062595322e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0143, 'grad_norm': 0.287175714969635, 'learning_rate': 9.168888953011989e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0225, 'grad_norm': 1.0689688920974731, 'learning_rate': 8.549231386298151e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0257, 'grad_norm': 0.41424688696861267, 'learning_rate': 7.951071468283167e-07, 'epoch': 0.67}\n",
            "{'loss': 0.0229, 'grad_norm': 0.2553465664386749, 'learning_rate': 7.374435364419674e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0256, 'grad_norm': 0.5482161045074463, 'learning_rate': 6.819348298638839e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0152, 'grad_norm': 0.11880373954772949, 'learning_rate': 6.285834552247128e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0237, 'grad_norm': 0.12715405225753784, 'learning_rate': 5.773917462864264e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0144, 'grad_norm': 0.18960794806480408, 'learning_rate': 5.283619423401998e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0293, 'grad_norm': 0.5735313296318054, 'learning_rate': 4.814961881085045e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0198, 'grad_norm': 0.346845418214798, 'learning_rate': 4.367965336512403e-07, 'epoch': 0.68}\n",
            "{'loss': 0.0148, 'grad_norm': 0.5084758400917053, 'learning_rate': 3.9426493427611177e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0262, 'grad_norm': 0.516304612159729, 'learning_rate': 3.5390325045304706e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0623, 'grad_norm': 0.4048095941543579, 'learning_rate': 3.157132477328628e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0254, 'grad_norm': 0.39243170619010925, 'learning_rate': 2.796965966699927e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0145, 'grad_norm': 0.6746180653572083, 'learning_rate': 2.458548727494292e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0136, 'grad_norm': 0.5022639632225037, 'learning_rate': 2.1418955631781202e-07, 'epoch': 0.69}\n",
            "{'loss': 0.0259, 'grad_norm': 0.24458688497543335, 'learning_rate': 1.847020325186577e-07, 'epoch': 0.69}\n",
            "{'loss': 0.02, 'grad_norm': 0.3322196304798126, 'learning_rate': 1.5739359123178587e-07, 'epoch': 0.7}\n",
            "{'loss': 0.0147, 'grad_norm': 0.13066765666007996, 'learning_rate': 1.3226542701689215e-07, 'epoch': 0.7}\n",
            "{'loss': 0.0179, 'grad_norm': 0.1604219675064087, 'learning_rate': 1.0931863906127327e-07, 'epoch': 0.7}\n",
            "{'loss': 0.0268, 'grad_norm': 0.31382104754447937, 'learning_rate': 8.855423113177664e-08, 'epoch': 0.7}\n",
            "{'loss': 0.0326, 'grad_norm': 0.18972839415073395, 'learning_rate': 6.997311153086883e-08, 'epoch': 0.7}\n",
            "{'loss': 0.0233, 'grad_norm': 0.6745316386222839, 'learning_rate': 5.3576093056922906e-08, 'epoch': 0.7}\n",
            "{'loss': 0.0232, 'grad_norm': 1.0063142776489258, 'learning_rate': 3.936389296864129e-08, 'epoch': 0.7}\n",
            "{'loss': 0.0218, 'grad_norm': 0.398796945810318, 'learning_rate': 2.7337132953697554e-08, 'epoch': 0.71}\n",
            "{'loss': 0.0414, 'grad_norm': 1.8266445398330688, 'learning_rate': 1.749633910153592e-08, 'epoch': 0.71}\n",
            "{'loss': 0.0179, 'grad_norm': 0.866723358631134, 'learning_rate': 9.841941880361916e-09, 'epoch': 0.71}\n",
            "{'loss': 0.0327, 'grad_norm': 0.5512344241142273, 'learning_rate': 4.3742761183018784e-09, 'epoch': 0.71}\n",
            "{'loss': 0.0221, 'grad_norm': 0.2397661805152893, 'learning_rate': 1.0935809887702154e-09, 'epoch': 0.71}\n",
            "{'loss': 0.0278, 'grad_norm': 0.22333407402038574, 'learning_rate': 0.0, 'epoch': 0.71}\n",
            "{'train_runtime': 1789.6838, 'train_samples_per_second': 11.175, 'train_steps_per_second': 2.794, 'train_loss': 0.028757989095151426, 'epoch': 0.71}\n",
            "100% 5000/5000 [29:49<00:00,  2.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/eval_policy.py --plot \\\n",
        "  --model_path /content/gr00t_recovered_finetuned \\\n",
        "  --dataset_path /content/Kuka_Kr60ha_DataLogs/Full_Kuka_Dataset \\\n",
        "  --data_config kuka_custom \\\n",
        "  --embodiment_tag new_embodiment \\\n",
        "  --modality_keys single_arm \\\n",
        "  --steps 150 \\\n",
        "  --trajs 31 \\\n",
        "  --action_horizon 16 \\\n",
        "  --video_backend decord \\\n",
        "  --plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRFg0qZCMzMO",
        "outputId": "499e80c4-10b3-45c7-ab48-3403096f9611"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.7 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "2025-05-18 21:17:09.265137: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-18 21:17:09.265188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-18 21:17:09.266437: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-18 21:17:10.551974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_recovered_finetuned\n",
            "Loading pretrained dual brain from /content/gr00t_recovered_finetuned\n",
            "Tune backbone vision tower: True\n",
            "Tune backbone LLM: False\n",
            "Tune action head projector: True\n",
            "Tune action head DiT: True\n",
            "Model not found or avail in the huggingface hub. Loading from local path: /content/gr00t_recovered_finetuned\n",
            "Total number of DiT parameters:  537803776\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.35it/s]\n",
            "Tune action head projector: True\n",
            "Tune action head diffusion model: True\n",
            "Set action denoising steps to 4\n",
            "{'video': ModalityConfig(delta_indices=[0], modality_keys=['video.webcam']), 'state': ModalityConfig(delta_indices=[0], modality_keys=['state.single_arm']), 'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['action.single_arm'])}\n",
            "Initialized dataset Full_Kuka_Dataset with new_embodiment\n",
            "28072\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "video.webcam (1, 480, 640, 3)\n",
            "state.single_arm (1, 7)\n",
            "action.single_arm (16, 7)\n",
            "Total trajectories: 31\n",
            "All trajectories: [ 762  822  839  982  971 1163  960 1083 1030 1009  983 1086  588  598\n",
            "  864  690  908  899  906  900  954  944 1028 1008  984  960  635  640\n",
            "  831 1028 1017]\n",
            "Running on all trajs with modality keys: ['single_arm']\n",
            "Running trajectory: 0\n",
            "inferencing at step:  0\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3389.8631603295794\n",
            "MSE: 3389.8631603295794\n",
            "Running trajectory: 1\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2357.2585846862025\n",
            "MSE: 2357.2585846862025\n",
            "Running trajectory: 2\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 938.7491310222573\n",
            "MSE: 938.7491310222573\n",
            "Running trajectory: 3\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 741.6515894516132\n",
            "MSE: 741.6515894516132\n",
            "Running trajectory: 4\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2065.4269650023175\n",
            "MSE: 2065.4269650023175\n",
            "Running trajectory: 5\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1658.450443257749\n",
            "MSE: 1658.450443257749\n",
            "Running trajectory: 6\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2438.8353609504156\n",
            "MSE: 2438.8353609504156\n",
            "Running trajectory: 7\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1451.5295289062267\n",
            "MSE: 1451.5295289062267\n",
            "Running trajectory: 8\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 951.0477065405131\n",
            "MSE: 951.0477065405131\n",
            "Running trajectory: 9\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1956.457581462449\n",
            "MSE: 1956.457581462449\n",
            "Running trajectory: 10\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1384.408714772503\n",
            "MSE: 1384.408714772503\n",
            "Running trajectory: 11\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 764.5959356769157\n",
            "MSE: 764.5959356769157\n",
            "Running trajectory: 12\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3667.1794680606527\n",
            "MSE: 3667.1794680606527\n",
            "Running trajectory: 13\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1443.1898679212004\n",
            "MSE: 1443.1898679212004\n",
            "Running trajectory: 14\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1878.8182718928672\n",
            "MSE: 1878.8182718928672\n",
            "Running trajectory: 15\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1442.1873550691032\n",
            "MSE: 1442.1873550691032\n",
            "Running trajectory: 16\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 810.9743182644463\n",
            "MSE: 810.9743182644463\n",
            "Running trajectory: 17\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1478.7624439391482\n",
            "MSE: 1478.7624439391482\n",
            "Running trajectory: 18\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2043.9622715100447\n",
            "MSE: 2043.9622715100447\n",
            "Running trajectory: 19\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1134.957817583403\n",
            "MSE: 1134.957817583403\n",
            "Running trajectory: 20\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 578.6705150314383\n",
            "/content/Isaac-GR00T/gr00t/utils/eval.py:95: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(nrows=num_of_joints, ncols=1, figsize=(8, 4 * num_of_joints))\n",
            "MSE: 578.6705150314383\n",
            "Running trajectory: 21\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 705.0823154343201\n",
            "MSE: 705.0823154343201\n",
            "Running trajectory: 22\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1592.1200242930997\n",
            "MSE: 1592.1200242930997\n",
            "Running trajectory: 23\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 917.0947247173732\n",
            "MSE: 917.0947247173732\n",
            "Running trajectory: 24\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 3327.70423624506\n",
            "MSE: 3327.70423624506\n",
            "Running trajectory: 25\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 513.3127415127825\n",
            "MSE: 513.3127415127825\n",
            "Running trajectory: 26\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1133.1641606855783\n",
            "MSE: 1133.1641606855783\n",
            "Running trajectory: 27\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1588.6826780375843\n",
            "MSE: 1588.6826780375843\n",
            "Running trajectory: 28\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1177.0485951824403\n",
            "MSE: 1177.0485951824403\n",
            "Running trajectory: 29\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 2168.5656534449763\n",
            "MSE: 2168.5656534449763\n",
            "Running trajectory: 30\n",
            "inferencing at step:  0\n",
            "inferencing at step:  16\n",
            "inferencing at step:  32\n",
            "inferencing at step:  48\n",
            "inferencing at step:  64\n",
            "inferencing at step:  80\n",
            "inferencing at step:  96\n",
            "inferencing at step:  112\n",
            "inferencing at step:  128\n",
            "inferencing at step:  144\n",
            "Unnormalized Action MSE across single traj: 1025.3832167418118\n",
            "MSE: 1025.3832167418118\n",
            " Total Inference Time: 149.55 seconds\n",
            "Average MSE across all trajs: 1571.778560568583\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oPtlkrNeRRvw"
      }
    }
  ]
}